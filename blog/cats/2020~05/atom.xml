<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: 2020~05 | kk Blog —— 通用基础]]></title>
  <link href="http://abcdxyzk.github.io/blog/cats/2020~05/atom.xml" rel="self"/>
  <link href="http://abcdxyzk.github.io/"/>
  <updated>2024-05-16T17:25:48+08:00</updated>
  <id>http://abcdxyzk.github.io/</id>
  <author>
    <name><![CDATA[kk]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[MPTCP 使用]]></title>
    <link href="http://abcdxyzk.github.io/blog/2020/05/28/kernel-mptcp/"/>
    <updated>2020-05-28T17:08:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2020/05/28/kernel-mptcp</id>
    <content type="html"><![CDATA[<h3>编译</h3>

<p><a href="https://github.com/abcdxyzk/mptcp-v0.95">https://github.com/abcdxyzk/mptcp-v0.95</a></p>

<p><a href="https://github.com/abcdxyzk/ubuntu-mptcp-v0.95">https://github.com/abcdxyzk/ubuntu-mptcp-v0.95</a></p>

<p>.config 配置</p>

<pre><code>    diff --git a/.config b/.config
    index cf146c1c..f5be3370 100644
    --- a/.config
    +++ b/.config
    @@ -1054,6 +1054,11 @@ CONFIG_TCP_CONG_ILLINOIS=m
     CONFIG_TCP_CONG_DCTCP=m
     CONFIG_TCP_CONG_CDG=m
     CONFIG_TCP_CONG_BBR=m
    +CONFIG_TCP_CONG_LIA=m
    +CONFIG_TCP_CONG_OLIA=m
    +CONFIG_TCP_CONG_WVEGAS=m
    +CONFIG_TCP_CONG_BALIA=m
    +# CONFIG_TCP_CONG_MCTCPDESYNC is not set
     CONFIG_DEFAULT_CUBIC=y
     # CONFIG_DEFAULT_RENO is not set
     CONFIG_DEFAULT_TCP_CONG="cubic"
    @@ -1090,6 +1095,21 @@ CONFIG_IPV6_PIMSM_V2=y
     CONFIG_IPV6_SEG6_LWTUNNEL=y
     CONFIG_IPV6_SEG6_HMAC=y
     CONFIG_NETLABEL=y
    +CONFIG_MPTCP=y
    +CONFIG_MPTCP_PM_ADVANCED=y
    +CONFIG_MPTCP_FULLMESH=y
    +CONFIG_MPTCP_NDIFFPORTS=m
    +CONFIG_MPTCP_BINDER=m
    +# CONFIG_MPTCP_NETLINK is not set
    +CONFIG_DEFAULT_FULLMESH=y
    +# CONFIG_DEFAULT_DUMMY is not set
    +CONFIG_DEFAULT_MPTCP_PM="fullmesh"
    +CONFIG_MPTCP_SCHED_ADVANCED=y
    +# CONFIG_MPTCP_BLEST is not set
    +CONFIG_MPTCP_ROUNDROBIN=m
    +CONFIG_MPTCP_REDUNDANT=m
    +CONFIG_DEFAULT_SCHEDULER=y
    +CONFIG_DEFAULT_MPTCP_SCHED="default"
     CONFIG_NETWORK_SECMARK=y
     CONFIG_NET_PTP_CLASSIFY=y
     CONFIG_NETWORK_PHY_TIMESTAMPING=y
</code></pre>

<p><a href="https://www.cnblogs.com/fenglt/p/8570343.html">https://www.cnblogs.com/fenglt/p/8570343.html</a></p>

<h3>路由配置</h3>

<p>enp0s9: 192.168.2.5  gw 192.168.2.4</p>

<p>enp0s10: 192.168.3.5 gw 192.168.3.4</p>

<pre><code>    ip rule add table 1 from 192.168.2.5
    ip route add 192.168.2.0/24 dev enp0s9 scope link table 1
    ip route add default via 192.168.2.4 dev enp0s9 table 1

    ip rule add table 2 from 192.168.3.5
    ip route add 192.168.3.0/24 dev enp0s10 scope link table 2
    ip route add default via 192.168.3.4 dev enp0s10 table 2
</code></pre>

<h3>fullmesh, ndiffports, binder, netlink</h3>

<h4>工具</h4>

<p>sudo apt-get install bison flex</p>

<p><a href="https://github.com/abcdxyzk/iproute-mptcp">https://github.com/abcdxyzk/iproute-mptcp</a></p>

<p>./configure</p>

<p>make</p>

<p>./ip/ip link set dev enp0s3 multipath off/on/backup</p>

<p>off命令是在MPTCP层面上的，并不是完全关闭该接口，而是控制MPTCP不去试图使用该网卡，换言之，当路由表指向该接口时，该接口还是会被使用的。</p>

<p>backup命令就是将该接口设置为backup模式，并且会通过PRIO option通知对方，两边会标记low_prio、rcv_low_prio。但目前所有pm都没有用到low_prio。</p>

<hr />

<p><a href="https://www.cnblogs.com/zhuting/p/5828988.html">https://www.cnblogs.com/zhuting/p/5828988.html</a></p>

<p><a href="http://multipath-tcp.org/pmwiki.php/Users/ConfigureMPTCP">http://multipath-tcp.org/pmwiki.php/Users/ConfigureMPTCP</a></p>

<h3>参数</h3>

<h4>net.mptcp.mptcp_enabled</h4>

<p>顾名思义，该变量控制MPTCP开关，实现MPTCP与传统TCP之间的切换。变量值为0或1（默认为1）。</p>

<h4>net.mptcp.mptcp_checksum</h4>

<p>该变量控制MPTCP传输层中数据序列号校验和（DSS-checksum）的开关，DSS-checksum主要和传输的可靠性相关，只要通信对端中有一端开启，就会执行。变量值为0或1（默认为1）。</p>

<h4>net.mptcp.mptcp_syn_retries</h4>

<p>设置SYN的重传次数。SYN里包含了MP_CAPABLE-option字段，通过该控制变量，SYN将不会包含MP_CAPABLE-option字段，这是为了处理会丢弃含有未知TCP选项的SYN的网络中间件。变量默认值为3。</p>

<h4>net.mptcp.mptcp_debug</h4>

<p>调试MPTCP，控制是否打印debug报告文件。</p>

<h4>net.mptcp.mptcp_path_manager</h4>

<p>MPTCP路径管理，有四个不同的配置值，分别是 default/fullmesh/ndiffports/binder。default/ndiffports/fullmesh分别选择单路、多路或者全路进行传输。其中单路是指跟传统TCP状态一样还是用单一的TCP子流进行传输，多路是当前所有TCP子流中用户选择x条子流数进行传输，全路是指将当前所有可用的TCP子流应用到网络传输中。而binder参考了文献 Binder: a system to aggregate multiple internet gateways in community networks。</p>

<p>fix: default=fullmesh</p>

<h4>net.mptcp.mptcp_scheduler</h4>

<p>MPTCP子流调度策略，有default/roundrobin两个选项。default优先选择RTT较低的子流直到拥塞窗口满，roundrobin采用轮询策略。</p>

<hr />

<p><a href="https://www.cnblogs.com/lxgeek/p/4187164.html">https://www.cnblogs.com/lxgeek/p/4187164.html</a></p>

<h3>MPTCP 理解</h3>

<p>MPTCP允许在一条TCP链路中建立多个子通道。当一条通道按照三次握手的方式建立起来后，可以按照三次握手的
方式建立其他的子通道，这些通道以三次握手建立连接和四次握手解除连接。这些通道都会绑定于MPTCP session，
发送端的数据可以选择其中一条通道进行传输。</p>

<p>MPTCP的设计遵守以下两个原则：</p>

<p>1.应用程序的兼容性，应用程序只要可以运行在TCP环境下，就可以在没有任何修改的情况下，运行于MPTCP环境。</p>

<p>2.网络的兼容性，MPTCP兼容其他协议。</p>

<p>MPTCP在协议栈中的位置如下所示：</p>

<p><img src="/images/kernel/20200528-1.jpg" alt="" /></p>

<h4>建立连接过程</h4>

<p><img src="/images/kernel/20200528-2.jpg" alt="" /></p>

<p>如上图所示：MPTCP的第一个子通道的建立遵守TCP的三次握手，唯一的区别是每次发送的
报文段需要添加MP_CAPABLE的的TCP选项和一个安全用途的key。而下图是建立其他的子通道：</p>

<p><img src="/images/kernel/20200528-3.jpg" alt="" /></p>

<p>如上图所示：第二条子通道的建立依然遵守TCP的三次握手，而TCP选项换成了MP_JOIN。
而token是基于key的一个hash值，rand为一个随机数，而HMAC是基于rand的一个hash值。</p>

<h4>数据的发送和接收</h4>

<p>MPTCP可以选择多条子通道中任意一条来发送数据。MPTCP如果使用传统的TCP的方式
来发送数据，将会出现一部分包在一条子通道，而另一部分包在另外一条子通道。这样的话，防火墙等
中间设备将会收到TCP的序号跳跃的包，因此将会发生丢包等异常情况。为了解决这个问题，MPTCP通过
增加DSN(data sequence number)来管理包的发送，DSN统计总的报文段序号，而每个子通道中的
序号始终是连续。</p>

<p>MPTCP的接收包过程分为两个阶段：第一、每个子通道依据自身序号来重组报文段；第二、MPTCP
的控制模块依据DSN对所有子通道的报文段进行重组。</p>

<h4>拥塞控制</h4>

<p>MPTCP中拥塞控制的设计需遵守以下两个原则：</p>

<p>第一：MPTCP和传统TCP应该拥有相同的吞吐量，而不是MPTCP中每一条子通道和传统TCP具有相同的吞吐量。</p>

<p>第二：MPTCP在选择子通道的时候应该选择拥塞情况更好的子通道。</p>

<h4>MPTCP的实现</h4>

<p>MPTCP的实现主要分为三部分：</p>

<p>master subsocket</p>

<p>Multi-path control bock(mpcb)</p>

<p>slave subsocket</p>

<p><img src="/images/kernel/20200528-4.jpg" alt="" /></p>

<p>master subsock是一个标准的sock结构体用于TCP通信。mpcb提供开启或关闭子通道、
选择发送数据的子通道以及重组报文段的功能。slave subsocket对应用程序并不可见，他们
都是被mpcb管理并用于发送数据。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[reuseport使用]]></title>
    <link href="http://abcdxyzk.github.io/blog/2020/05/25/kernel-reuseport-history/"/>
    <updated>2020-05-25T17:07:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2020/05/25/kernel-reuseport-history</id>
    <content type="html"><![CDATA[<p><a href="http://www.it165.net/os/html/201605/17066.html">http://www.it165.net/os/html/201605/17066.html</a></p>

<h3>Q&amp;A</h3>

<h4>Q1：什么是reuseport？</h4>

<p>A1：reuseport是一种套接字复用机制，它允许你将多个套接字bind在同一个IP地址/端口对上，这样一来，就可以建立多个服务来接受到同一个端口的连接。</p>

<h4>Q2：当来了一个连接时，系统怎么决定到底是哪个套接字来处理它？</h4>

<p>A2：对于不同的内核，处理机制是不一样的，总的说来，reuseport分为两种模式，即热备份模式和负载均衡模式，在早期的内核版本中，即便是加入对reuseport选项的支持，也仅仅为热备份模式，而在3.9内核之后，则全部改为了负载均衡模式，两种模式没有共存</p>

<h4>Q3：什么是热备份模式和负载均衡模式呢？</h4>

<p>A3：
热备份模式：即你创建了N个reuseport的套接字，然而工作的只有一个，其它的作为备份，只有当前一个套接字不再可用的时候，才会由后一个来取代，其投入工作的顺序取决于实现。</p>

<p>负载均衡模式：即你创建的所有N个reuseport的套接字均可以同时工作，当连接到来的时候，系统会取一个套接字来处理它。这样就可以达到负载均衡的目的，降低某一个服务的压力。</p>

<h4>Q4：到底怎么取套接字呢？</h4>

<p>A4：这个对于热备份模式和负载均衡模式是不同的。</p>

<p>热备份模式：一般而言，会将所有的reuseport同一个IP地址/端口的套接字挂在一个链表上，取第一个即可，如果该套接字挂了，它会被从链表删除，然后第二个便会成为第一个。</p>

<p>负载均衡模式：和热备份模式一样，所有reuseport同一个IP地址/端口的套接字会挂在一个链表上，你也可以认为是一个数组，这样会更加方便，当有连接到来时，用数据包的源IP/源端口作为一个HASH函数的输入，将结果对reuseport套接字数量取模，得到一个索引，该索引指示的数组位置对应的套接字便是工作套接字。</p>

<h3>关于REUSEPORT的实现</h3>

<p>Linux 4.5/4.6所谓的对reuseport的优化主要体现在查询速度上，在优化前，在HASH冲突链表上遍历所有的套接字之后才能知道到底取哪个(基于一种冒泡的score打分机制，不完成一轮冒泡遍历，不能确定谁的score最高)，之所以如此低效是因为内核将reuseport的所有套接字和其它套接字混合在了一起，查找是平坦的，正常的做法应该是将它们分为一个组，进行分层查找，先找到这个组(这个很容易)，然后再在组中找具体的套接字。Linux 4.5针对UDP做了上述优化，而Linux 4.6则将这个优化引入到了TCP。</p>

<p>设想系统中一共有10000个套接字被HASH到同一个冲突链表，其中9950个是reuseport的同一组套接字，如果按照老的算法，需要遍历10000个套接字，如果使用基于分组的算法，最多只需要遍历51个套接字即可，找到那个组之后，一步HASH就可以找到目标套接字的索引！</p>

<h4>Linux 4.5之前的reuseport查找实现(4.3内核)</h4>

<p>以下是未优化前的Linux 4.3内核的实现，可见是多么地不直观。它采用了遍历HASH冲突链表的方式进行reuseport套接字的精确定位：</p>

<pre><code>    result = NULL;
    badness = 0;
    udp_portaddr_for_each_entry_rcu(sk, node, &amp;hslot2-&gt;head) {
        score = compute_score2(sk, net, saddr, sport, daddr, hnum, dif);
        if (score &gt; badness) { // 冒泡排序
            // 找到了更加合适的socket，需要重新hash
            result = sk;
            badness = score;
            reuseport = sk-&gt;sk_reuseport;
            if (reuseport) {
                hash = udp_ehashfn(net, daddr, hnum, saddr, sport);
                matches = 1;
            }
        } else if (score == badness &amp;&amp; reuseport) { // reuseport套接字散列定位
            // 找到了同样reuseport的socket，进行定位
            matches++;
            if (reciprocal_scale(hash, matches) == 0)
                result = sk;
            hash = next_pseudo_random32(hash);
        }
    }
</code></pre>

<p>之所以要遍历是因为所有的reuseport套接字和其它的套接字都被平坦地插入到同一个表中，事先并不知道有多少组reuseport套接字以及每一组中有多少个套接字</p>

<h4>Linux 4.5(针对UDP)/4.6(针对TCP)的reuseport查找实现</h4>

<p>我们来看看在4.5和4.6内核中对于reuseport的查找增加了一些什么神奇的新东西：
<code>
    result = NULL;
    badness = 0;
    udp_portaddr_for_each_entry_rcu(sk, node, &amp;hslot2-&gt;head) {
        score = compute_score2(sk, net, saddr, sport, daddr, hnum, dif);
        if (score &gt; badness) {
            // 在reuseport情形下，意味着找到了更加合适的socket组，需要重新hash
            result = sk;
            badness = score;
            reuseport = sk-&gt;sk_reuseport;
            if (reuseport) {
                hash = udp_ehashfn(net, daddr, hnum, saddr, sport);
                if (select_ok) {
                    struct sock *sk2;
                    // 找到了一个组，接着进行组内hash。
                    sk2 = reuseport_select_sock(sk, hash, skb, sizeof(struct udphdr));
                    if (sk2) {
                        result = sk2;
                        select_ok = false;
                        goto found;
                    }
                }
                matches = 1;
            }
        } else if (score == badness &amp;&amp; reuseport) {
            // 这个else if分支的期待是，在分层查找不适用的时候，寻找更加匹配的reuseport组，注意4.5/4.6以后直接寻找的是一个reuseport组。
            // 在某种意义上，这回退到了4.5之前的算法。
            matches++;
            if (reciprocal_scale(hash, matches) == 0)
                result = sk;
            hash = next_pseudo_random32(hash);
        }
    }
</code></p>

<p>我们着重看一下reuseport_select_sock，这个函数是第二层组内查找的关键，其实不应该叫做查找，而应该叫做定位更加合适：
```
    struct sock <em>reuseport_select_sock(struct sock </em>sk, u32 hash, struct sk_buff <em>skb, int hdr_len)
    {
        &hellip;
        prog = rcu_dereference(reuse->prog);
        socks = READ_ONCE(reuse->num_socks);
        if (likely(socks)) {
            /</em> paired with smp_wmb() in reuseport_add_sock() */
            smp_rmb();</p>

<pre><code>        if (prog &amp;&amp; skb) // 可以用BPF来从用户态注入自己的定位逻辑，更好实现基于策略的负载均衡
            sk2 = run_bpf(reuse, socks, prog, skb, hdr_len);
        else
            // reciprocal_scale简单地将结果限制在了[0,socks)这个区间内
            sk2 = reuse-&gt;socks[reciprocal_scale(hash, socks)];
    }
    ...
}
</code></pre>

<p>```</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[reuseport使用]]></title>
    <link href="http://abcdxyzk.github.io/blog/2020/05/25/kernel-reuseport/"/>
    <updated>2020-05-25T16:57:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2020/05/25/kernel-reuseport</id>
    <content type="html"><![CDATA[<p><a href="https://www.cnblogs.com/Anker/p/7076537.html">https://www.cnblogs.com/Anker/p/7076537.html</a></p>

<h4>SO_REUSEPORT解决了什么问题</h4>

<p>SO_REUSEPORT支持多个进程或者线程绑定到同一端口，提高服务器程序的性能，解决的问题：</p>

<p>允许多个套接字 bind()/listen() 同一个TCP/UDP端口<br/>
  每一个线程拥有自己的服务器套接字<br/>
  在服务器套接字上没有了锁的竞争</p>

<p>内核层面实现负载均衡</p>

<p>安全层面，监听同一个端口的套接字只能位于同一个用户下面</p>

<p>其核心的实现主要有三点：</p>

<p>  扩展 socket option，增加 SO_REUSEPORT 选项，用来设置 reuseport。</p>

<p>  修改 bind 系统调用实现，以便支持可以绑定到相同的 IP 和端口</p>

<p>  修改处理新建连接的实现，查找 listener 的时候，能够支持在监听相同 IP 和端口的多个 sock 之间均衡选择。</p>

<p>有了SO_RESUEPORT后，每个进程可以自己创建socket、bind、listen、accept相同的地址和端口，各自是独立平等的。让多进程监听同一个端口，各个进程中accept socket fd不一样，有新连接建立时，内核只会唤醒一个进程来accept，并且保证唤醒的均衡性。</p>

<h4>可优化 ？？</h4>

<p>在 <code>___inet_lookup_listener</code> -> compute_score() 中，每个cpu只选特定的sk，这样能减少锁竞争和cache吗 ？？？</p>

<h4>代码</h4>

<pre><code>    #define _GNU_SOURCE

    #include &lt;stdio.h&gt;
    #include &lt;unistd.h&gt;
    #include &lt;sys/types.h&gt;
    #include &lt;sys/socket.h&gt;
    #include &lt;netinet/in.h&gt;
    #include &lt;arpa/inet.h&gt;
    #include &lt;assert.h&gt;
    #include &lt;sys/wait.h&gt;
    #include &lt;string.h&gt;
    #include &lt;errno.h&gt;
    #include &lt;stdlib.h&gt;
    #include &lt;fcntl.h&gt;


    #include &lt;sched.h&gt;
    #include &lt;pthread.h&gt;

    #include &lt;netinet/tcp.h&gt;

    #define IP      "192.168.3.6"
    #define PORT        80
    #define WORKER      8
    #define MAXLINE     4096


    int worker(int i)
    {
        struct sockaddr_in address;
        bzero(&amp;address, sizeof(address));
        address.sin_family = AF_INET;
        inet_pton( AF_INET, IP, &amp;address.sin_addr);
        address.sin_port = htons(PORT);

        pid_t pid = getpid();
        unsigned cc = 0;
        cpu_set_t mask;
        CPU_ZERO(&amp;mask);
        CPU_SET(i, &amp;mask);

        printf("pid=%d %d\n", pid, i);
        if (sched_getaffinity(pid, sizeof(mask), &amp;mask) &lt; 0) {
            printf("sched_getaffinity err\n");
        }

        int listenfd = socket(PF_INET, SOCK_STREAM, 0);
        assert(listenfd &gt;= 0);

        int val =1;
        /*set SO_REUSEPORT*/
        if (setsockopt(listenfd, SOL_SOCKET, SO_REUSEPORT, &amp;val, sizeof(val)) &lt; 0) {
            perror("setsockopt()");
        }

        val = 1000 + i;
        if (setsockopt(listenfd, SOL_TCP, TCP_MAXSEG, &amp;val, sizeof(val))&lt;0) {
            perror("setsockopt()");
        }

        int ret = bind(listenfd, (struct sockaddr*)&amp;address, sizeof(address));
        assert(ret != -1);

        ret = listen(listenfd, 5);
        assert(ret != -1);
        while (1) {
            cc ++;
            if (cc % 100 == 0)
                printf("thread=%d cc=%d\n", i, cc);
    //      printf("I am worker %d, begin to accept connection.\n", i);
            struct sockaddr_in client_addr;
            socklen_t client_addrlen = sizeof( client_addr );
            int connfd = accept(listenfd, (struct sockaddr*)&amp;client_addr, &amp;client_addrlen);
            if (connfd != -1) {
    //          printf("worker %d accept a connection success. ip:%s, prot:%d\n", i, inet_ntoa(client_addr.sin_addr), client_addr.sin_port);
            } else {
    //          printf("worker %d accept a connection failed,error:%s", i, strerror(errno));
            }
            char buffer[MAXLINE];
    //      int nbytes = read(connfd, buffer, MAXLINE);
    //      printf("read from client is:%s\n", buffer);
    //      write(connfd, buffer, nbytes);
            close(connfd);
        }
        return 0;
    }

    int main()
    {
        int i = 0;
        for (i = 0; i &lt; WORKER; i++) {
            printf("Create worker %d\n", i);
            pid_t pid = fork();
            /*child  process */
            if (pid == 0) {
                worker(i);
            }
            if (pid &lt; 0) {
                printf("fork error");
            }
        }
        /*wait child process*/
        while (wait(NULL) != 0)
            ;
        if (errno == ECHILD) {
            fprintf(stderr, "wait error:%s\n", strerror(errno));
        }
        return 0;
    }
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[veth虚拟网络设备的qdisc]]></title>
    <link href="http://abcdxyzk.github.io/blog/2020/05/22/kernel-qdisc-veth/"/>
    <updated>2020-05-22T11:24:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2020/05/22/kernel-qdisc-veth</id>
    <content type="html"><![CDATA[<p><a href="http://hustcat.github.io/veth/">http://hustcat.github.io/veth/</a></p>

<p><a href="http://hustcat.github.io/vlan-performance-problem/">http://hustcat.github.io/vlan-performance-problem/</a></p>

<p>veth设备qdisc队列，而环回设备/桥接设备是没qdisc队列的，参考br_dev_setup函数。</p>

<h4>内核实现</h4>

<p>在注册（创建）设备时，qdisc设置为noop_qdisc， register_netdevice -> dev_init_scheduler</p>

<pre><code>    void dev_init_scheduler(struct net_device *dev)
    {
        dev-&gt;qdisc = &amp;noop_qdisc;
        netdev_for_each_tx_queue(dev, dev_init_scheduler_queue, &amp;noop_qdisc);
        dev_init_scheduler_queue(dev, &amp;dev-&gt;rx_queue, &amp;noop_qdisc);

        setup_timer(&amp;dev-&gt;watchdog_timer, dev_watchdog, (unsigned long)dev);
    }
</code></pre>

<p>打开设备时，如果没有配置qdisc时，就指定为默认的pfifo_fast队列： dev_open -> dev_activate，</p>

<pre><code>    void dev_activate(struct net_device *dev)
    {
        int need_watchdog;

        /* No queueing discipline is attached to device;
           create default one i.e. pfifo_fast for devices,
           which need queueing and noqueue_qdisc for
           virtual interfaces
         */

        if (dev-&gt;qdisc == &amp;noop_qdisc)
            attach_default_qdiscs(dev);
    ...
    }

    static void attach_default_qdiscs(struct net_device *dev)
    {
        struct netdev_queue *txq;
        struct Qdisc *qdisc;

        txq = netdev_get_tx_queue(dev, 0);

        if (!netif_is_multiqueue(dev) || dev-&gt;tx_queue_len == 0) {
            netdev_for_each_tx_queue(dev, attach_one_default_qdisc, NULL);
            dev-&gt;qdisc = txq-&gt;qdisc_sleeping;
            atomic_inc(&amp;dev-&gt;qdisc-&gt;refcnt);
        } else {///multi queue
            qdisc = qdisc_create_dflt(dev, txq, &amp;mq_qdisc_ops, TC_H_ROOT);
            if (qdisc) {
                qdisc-&gt;ops-&gt;attach(qdisc);
                dev-&gt;qdisc = qdisc;
            }
        }
    }

    static void attach_one_default_qdisc(struct net_device *dev,
                         struct netdev_queue *dev_queue,
                         void *_unused)
    {
        struct Qdisc *qdisc;

        if (dev-&gt;tx_queue_len) {
            qdisc = qdisc_create_dflt(dev, dev_queue,
                          &amp;pfifo_fast_ops, TC_H_ROOT);
            if (!qdisc) {
                printk(KERN_INFO "%s: activation failed\n", dev-&gt;name);
                return;
            }

            /* Can by-pass the queue discipline for default qdisc */
            qdisc-&gt;flags |= TCQ_F_CAN_BYPASS;
        } else {
            qdisc =  &amp;noqueue_qdisc;
        }
        dev_queue-&gt;qdisc_sleeping = qdisc;
    }
</code></pre>

<h4>创建noqueue</h4>

<p>开始尝试直接删除设备默认的pfifo_fast队列，发现会出错：</p>

<pre><code>    # tc qdisc del dev vethd4ea root
    RTNETLINK answers: No such file or directory
    # tc  -s qdisc ls dev vethd4ea
    qdisc pfifo_fast 0: root refcnt 2 bands 3 priomap  1 2 2 2 1 2 0 0 1 1 1 1 1 1 1 1
     Sent 29705382 bytes 441562 pkt (dropped 0, overlimits 0 requeues 0) 
     backlog 0b 0p requeues 0 
</code></pre>

<p>后来看到Jesper Brouer给出一个替换默认队列的方式，尝试了一下，成功完成。</p>

<p>替换默认的qdisc队列</p>

<pre><code>    # tc qdisc replace dev vethd4ea root pfifo limit 100
    # tc  -s qdisc ls dev vethd4ea                      
    qdisc pfifo 8001: root refcnt 2 limit 100p
     Sent 264 bytes 4 pkt (dropped 0, overlimits 0 requeues 0) 
     backlog 0b 0p requeues 0 
    # ip link show vethd4ea
    9: vethd4ea: &lt;BROADCAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo master docker0 state UP mode DEFAULT qlen 1000
    link/ether 3a:15:3b:e1:d7:6d brd ff:ff:ff:ff:ff:ff
</code></pre>

<p>修改队列长度</p>

<pre><code>    # ifconfig vethd4ea txqueuelen 0
</code></pre>

<p>删除qdisc</p>

<pre><code>    # tc qdisc del dev vethd4ea root                    
    # ip link show vethd4ea                
    9: vethd4ea: &lt;BROADCAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master docker0 state UP mode DEFAULT 
    link/ether 3a:15:3b:e1:d7:6d brd ff:ff:ff:ff:ff:ff
</code></pre>

<p>可以看到，UP的veth设备成功修改成noqueue。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[qdisc 的创建过程]]></title>
    <link href="http://abcdxyzk.github.io/blog/2020/05/22/kernel-qdisc-init/"/>
    <updated>2020-05-22T11:12:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2020/05/22/kernel-qdisc-init</id>
    <content type="html"><![CDATA[<p><a href="http://blog.chinaunix.net/uid-26902809-id-4106161.html">http://blog.chinaunix.net/uid-26902809-id-4106161.html</a></p>

<p>register_netdevice会初始化netdev的Tx调度discipline, 缺省使用noop_qdisc</p>

<pre><code>    register_netdevice
    ---&gt;dev_init_scheduler


    void dev_init_scheduler(struct net_device *dev)
    {
        dev-&gt;qdisc = &amp;noop_qdisc; //缺省为设备配置noop_qdisc
        netdev_for_each_tx_queue(dev, dev_init_scheduler_queue, &amp;noop_qdisc); //缺省为每个队列配置noop_qdisc
        if (dev_ingress_queue(dev))
            dev_init_scheduler_queue(dev, dev_ingress_queue(dev), &amp;noop_qdisc);

        setup_timer(&amp;dev-&gt;watchdog_timer, dev_watchdog, (unsigned long)dev);
    }
</code></pre>

<pre><code>    dev_open
    ---&gt;__dev_open
    ----&gt;dev_activate
    ----&gt;attach_default_qdiscs
    ----&gt;attach_one_default_qdisc
    为单队列的设备创建pfifo_fast的qdisc

    static void attach_one_default_qdisc(struct net_device *dev,
                    struct netdev_queue *dev_queue,
                    void *_unused)
    {
        struct Qdisc *qdisc = &amp;noqueue_qdisc;

        if (dev-&gt;tx_queue_len) {
            qdisc = qdisc_create_dflt(dev_queue,
                    &amp;pfifo_fast_ops, TC_H_ROOT);
            if (!qdisc) {
                netdev_info(dev, "activation failed\n");
                return;
            }
        }
        dev_queue-&gt;qdisc_sleeping = qdisc;
    }
</code></pre>

<pre><code>    dev_open
    ---&gt;__dev_open
    ----&gt;dev_activate
    ----&gt;attach_default_qdiscs
    ----&gt;qdisc_create_dflt
    为多队列的设备创建mq_qdisc, 创建完mq_qdisc， 接着调用mq_qdisc_ops-&gt;mq_init函数为每个队列创建pfifo_fast_ops的qdisc

    struct Qdisc *qdisc_create_dflt(struct netdev_queue *dev_queue,
                struct Qdisc_ops *ops, unsigned int parentid)
    {
        struct Qdisc *sch;

        sch = qdisc_alloc(dev_queue, ops);
        if (IS_ERR(sch))
            goto errout;
        sch-&gt;parent = parentid;

        if (!ops-&gt;init || ops-&gt;init(sch, NULL) == 0)
            return sch;

        qdisc_destroy(sch);
    errout:
        return NULL;
    }
    EXPORT_SYMBOL(qdisc_create_dflt);
</code></pre>

<pre><code>    dev_open
    ---&gt;__dev_open
    ----&gt;dev_activate
    ----&gt;attach_default_qdiscs
    static void attach_default_qdiscs(struct net_device *dev)
    {
        struct netdev_queue *txq;
        struct Qdisc *qdisc;

        txq = netdev_get_tx_queue(dev, 0);

        if (!netif_is_multiqueue(dev) || dev-&gt;tx_queue_len == 0) {
            netdev_for_each_tx_queue(dev, attach_one_default_qdisc, NULL);
            dev-&gt;qdisc = txq-&gt;qdisc_sleeping;
            atomic_inc(&amp;dev-&gt;qdisc-&gt;refcnt);
        } else {
            qdisc = qdisc_create_dflt(txq, &amp;mq_qdisc_ops, TC_H_ROOT);
            if (qdisc) {
                qdisc-&gt;ops-&gt;attach(qdisc);
                dev-&gt;qdisc = qdisc;
            }
        }
    }
</code></pre>

<pre><code>    dev_open函数会调用dev_activate：
    a. 为单队列的设备创建pfifo_fast的qdisc
    b. 为多队列的设备创建mq_qdisc, 创建完mq_qdisc， 接着调用mq_qdisc_ops-&gt;mq_init函数为每个队列创建pfifo_fast_ops的qdisc
    dev_open
    ---&gt;__dev_open
    ----&gt;dev_activate

    void dev_activate(struct net_device *dev)
    {
        int need_watchdog;

        /* No queueing discipline is attached to device;
           create default one i.e. pfifo_fast for devices,
           which need queueing and noqueue_qdisc for
           virtual interfaces
        */

        if (dev-&gt;qdisc == &amp;noop_qdisc)
            attach_default_qdiscs(dev);

        if (!netif_carrier_ok(dev))
        /* Delay activation until next carrier-on event */
            return;

        need_watchdog = 0;
        netdev_for_each_tx_queue(dev, transition_one_qdisc, &amp;need_watchdog);
        if (dev_ingress_queue(dev))
            transition_one_qdisc(dev, dev_ingress_queue(dev), NULL);

        if (need_watchdog) {
            dev-&gt;trans_start = jiffies;
            dev_watchdog_up(dev);
        }
    }
</code></pre>
]]></content>
  </entry>
  
</feed>
