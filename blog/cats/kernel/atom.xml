<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: kernel | kk Blog —— 通用基础]]></title>
  <link href="http://abcdxyzk.github.io/blog/cats/kernel/atom.xml" rel="self"/>
  <link href="http://abcdxyzk.github.io/"/>
  <updated>2024-12-31T15:33:01+08:00</updated>
  <id>http://abcdxyzk.github.io/</id>
  <author>
    <name><![CDATA[kk]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[ubuntu 编译strongswan]]></title>
    <link href="http://abcdxyzk.github.io/blog/2024/04/29/strongswan-make/"/>
    <updated>2024-04-29T14:45:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2024/04/29/strongswan-make</id>
    <content type="html"><![CDATA[<pre><code>    lsb_release -a

    wget https://download2.strongswan.org/old/5.x/strongswan-5.7.2.tar.gz

    sudo apt-get install -y build-essential libpcre3-dev libssl-dev libprotobuf-dev libsqlite3-dev libreadline-dev
    sudo apt-get install -y libgmp-dev
    sudo apt-get install -y libldap-dev
    sudo apt-get install -y libcurl4-openssl-dev 
    sudo apt-get install -y libldns-dev
    sudo apt-get install -y libunbound-dev
    sudo apt-get install -y  pkg-config
    sudo apt-get install -y  libsoup2.4-dev 
    sudo apt-get install -y libsystemd-dev
    sudo apt-get install -y libtspi-dev 
    sudo apt-get install libjson-c-dev 
    sudo apt-get install libneo27-dev 
    sudo apt-get install libneon27-gnutls-dev 
    sudo apt-get install -y libpam-dev

    ./configure --disable-tss-tss --enable-eap-identity --enable-eap-md5 --enable-eap-mschapv2 --enable-eap-tls --enable-eap-ttls --enable-eap-peap --enable-eap-tnc --enable-eap-dynamic --enable-eap-radius --enable-xauth-eap --enable-xauth-pam --enable-dhcp --enable-openssl --enable-addrblock --enable-unity --enable-certexpire --enable-radattr --enable-tools --enable-openssl --disable-gmp --enable-kernel-libipsec
    make -j4
    make install

    vim /usr/local/etc/swanctl/swanctl.conf 

    ipsec stop
    ipsec start
    ipsec statusall

    swanctl --load-all
    tail -f /var/log/syslog 
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MPTCP mptcp_v6_mapped bug]]></title>
    <link href="http://abcdxyzk.github.io/blog/2023/07/09/kernel-mptcp-bug/"/>
    <updated>2023-07-09T15:38:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2023/07/09/kernel-mptcp-bug</id>
    <content type="html"><![CDATA[<p><a href="https://github.com/multipath-tcp/mptcp/issues/501">https://github.com/multipath-tcp/mptcp/issues/501</a></p>

<p>git log -p -1 6baa3e5d7fd3e8efa6e9ced5f1ee22547c0889d1</p>

<pre><code>    commit 6baa3e5d7fd3e8efa6e9ced5f1ee22547c0889d1
    Author: Matthieu Baerts &lt;matthieu.baerts@tessares.net&gt;
    Date:   Thu Dec 29 15:35:24 2022 +0100

        mptcp: set icsk_af_ops to mptcp_v6 one if mptcp

        abcdxyzk recently opened a very detailed bug report on Github where it
        is noticed that surprisingly, AF_INET{4,6} operations were set to MPTCP
        specific functions if the attached connection was *not* an MPTCP one.

        This is a typo of course.

        As explained by Christoph, there was no consequences: mptcp_v6_mapped
        only changes conn_request to mptcp_conn_request(): that's only important
        with listening sockets, not used here then.

        Anyway, it is still better to properly fix that just in case other
        functions from mptcp_v6_mapped are modified later.

        Closes: https://github.com/multipath-tcp/mptcp/issues/501
        Fixes: db745ef1568d ("mptcp: Only access the sysctl once to detect mptcp-enabled")
        Suggested-by: Christoph Paasch &lt;cpaasch@apple.com&gt;
        Reviewed-by: Timothée Boucher-Lambert &lt;timothee.boucher-lambert@tessares.net&gt;
        Acked-by: Christoph Paasch &lt;cpaasch@apple.com&gt;
        Signed-off-by: Matthieu Baerts &lt;matthieu.baerts@tessares.net&gt;
        (cherry picked from commit edd63c2dab7ecb48b1613d112d2779c222d2bec1)
        Signed-off-by: Matthieu Baerts &lt;matthieu.baerts@tessares.net&gt;
        (cherry picked from commit 772b4de2997d6bcda92ca37afa9f69ed853c6b7a)
        Signed-off-by: Matthieu Baerts &lt;matthieu.baerts@tessares.net&gt;

    diff --git a/net/ipv6/tcp_ipv6.c b/net/ipv6/tcp_ipv6.c
    index 2e262aa2674a..bbdd53191c35 100644
    --- a/net/ipv6/tcp_ipv6.c
    +++ b/net/ipv6/tcp_ipv6.c
    @@ -1161,7 +1161,7 @@ struct sock *tcp_v6_syn_recv_sock(const struct sock *sk, struct sk_buff *skb,
            /* We must check on the request-socket because the listener
             * socket's flag may have been changed halfway through.
             */
    -       if (!inet_rsk(req)-&gt;saw_mpc)
    +       if (inet_rsk(req)-&gt;saw_mpc)
                inet_csk(newsk)-&gt;icsk_af_ops = &amp;mptcp_v6_mapped;
            else
     #endif
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Linux内核之GRE处理分析]]></title>
    <link href="http://abcdxyzk.github.io/blog/2022/11/27/kernel-gre/"/>
    <updated>2022-11-27T20:47:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2022/11/27/kernel-gre</id>
    <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/s2603898260/article/details/115773153">https://blog.csdn.net/s2603898260/article/details/115773153</a></p>

<p><a href="https://blog.csdn.net/u014044624/article/details/106596000">https://blog.csdn.net/u014044624/article/details/106596000</a></p>

<p><img src="/images/kernel/20221127-31.png" alt="" /></p>

<hr />

<h2>GRE</h2>

<p>GRE（Generic Routing Encapsulation，通用路由封装）协议是对某些网络层协议（如IP 和IPX）的数据报文进行封装，使这些被封装的数据报文能够在另一个网络层协议（如IP）中传输。</p>

<p>在大多数常规情况下，系统拥有一个有效载荷（或负载）包，需要将它封装并发送至某个目的地。首先将有效载荷封装在一个 GRE 包中，然后将此 GRE 包封装在其它某协议中并进行转发。此外发协议即为发送协议。当 IPv4 被作为 GRE 有效载荷传输时，协议类型字段必须被设置为 0x800 。当一个隧道终点拆封此含有 IPv4 包作为有效载荷的 GRE 包时， IPv4 包头中的目的地址必须用来转发包，并且需要减少有效载荷包的 TTL 。值得注意的是，在转发这样一个包时，如果有效载荷包的目的地址就是包的封装器（也就是隧道另一端），就会出现回路现象。在此情形下，必须丢弃该包。当 GRE 包被封装在 IPv4 中时，需要使用 IPv4 协议 47 。</p>

<p><img src="/images/kernel/20221127-32.png" alt="" /></p>

<p>GRE采用了Tunnel（隧道）技术，是VPN（Virtual Private Network）的第三层隧道协议。Tunnel 是一个虚拟的点对点的连接，提供了一条通路使封装的数据报文能够在这个通路上传输，并且在一个Tunnel 的两端分别对数据报进行封装及解封装。</p>

<p><img src="/images/kernel/20221127-33.png" alt="" /></p>

<h3>GRE包发送过程：</h3>

<p>发送过程是很简单的，因为 router A 上配置了一条路由规则，凡是发往 10.0.2.0 网络的包都要经过 netb 这个 tunnel 设备，在内核中经过 forward 之后就最终到达这个 GRE tunnel 设备的 ndo_start_xmit()，也就是 ipgre_tunnel_xmit() 函数。这个函数所做的事情无非就是通过 tunnel 的 header_ops 构造一个新的头，并把对应的外部 IP 地址填进去，最后发送出去。</p>

<h4>Linux kernel函数调用分析：</h4>

<p><img src="/images/kernel/20221127-34.png" alt="" /></p>

<h3>GRE包接收过程：</h3>

<p>接收过程，即 router B 上面进行的操作。这里需要指出的一点是，GRE tunnel 自己定义了一个新的 IP proto，也就是 IPPROTO_GRE。当 router B 收到从 router A 过来的这个包时，它暂时还不知道这个是 GRE 的包，它首先会把它当作普通的 IP 包处理。因为外部的 IP 头的目的地址是该路由器的地址，所以它自己会接收这个包，把它交给上层，到了 IP 层之后才发现这个包不是 TCP，UDP，而是 GRE，这时内核会转交给 GRE 模块处理。</p>

<p>ipgre_rcv() 所做的工作是：通过外层IP 头找到对应的 tunnel，然后剥去外层 IP 头，把这个“新的”包重新交给 IP 栈去处理，像接收到普通 IP 包一样。到了这里，“新的”包处理和其它普通的 IP 包已经没有什么两样了：根据 IP 头中目的地址转发给相应的 host。</p>

<p>注：在这里可以把gre当做L4层协议。</p>

<h4>Linux kernel函数调用分析：</h4>

<p><img src="/images/kernel/20221127-35.png" alt="" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Linux nf_conntrack连接跟踪的实现]]></title>
    <link href="http://abcdxyzk.github.io/blog/2022/11/27/nf-conntrack/"/>
    <updated>2022-11-27T20:09:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2022/11/27/nf-conntrack</id>
    <content type="html"><![CDATA[<p><a href="http://bbs.chinaunix.net/thread-4082396-1-1.html">http://bbs.chinaunix.net/thread-4082396-1-1.html</a></p>

<p>连接跟踪，顾名思义，就是识别一个连接上双方向的数据包，同时记录状态。下面看一下它的数据结构：</p>

<pre><code>    struct nf_conn {
        /* Usage count in here is 1 for hash table/destruct timer, 1 per skb, plus 1 for any connection(s) we are `master' for */
        struct  nf_conntrack  ct_general;       /* 连接跟踪的引用计数 */
        spinlock_t  lock;

        /* Connection tracking(链接跟踪)用来跟踪、记录每个链接的信息(目前仅支持IP协议的连接跟踪)。
            每个链接由“tuple”来唯一标识，这里的“tuple”对不同的协议会有不同的含义，例如对tcp,udp
             来说就是五元组: (源IP，源端口，目的IP, 目的端口，协议号)，对ICMP协议来说是: (源IP, 目
            的IP, id, type, code), 其中id,type与code都是icmp协议的信息。链接跟踪是防火墙实现状态检
            测的基础，很多功能都需要借助链接跟踪才能实现，例如NAT、快速转发、等等。*/
        struct  nf_conntrack_tuple_hash  tuplehash[IP_CT_DIR_MAX];

        unsigned long  status;                 /* 可以设置由enum ip_conntrack_status中描述的状态 */

        struct  nf_conn  *master;           /* 如果该连接是某个连接的子连接，则master指向它的主连接 */
        /* Timer function; drops refcnt when it goes off. */
        struct  timer_list  timeout;

        union nf_conntrack_proto proto;     /* 用于保存不同协议的私有数据 */
        /* Extensions */
        struct nf_ct_ext *ext;          /* 用于扩展结构 */
    };
</code></pre>

<p>这个结构非常简单，其中最主要的就是tuplehash（跟踪连接双方向数据）和status（记录连接状态），这也连接跟踪最主要的功能。</p>

<p>在status中可以设置的标志，由下面的enum ip_conntrack_status描述，它们可以共存。这些标志设置后就不会再被清除。</p>

<pre><code>    enum ip_conntrack_status {
        IPS_EXPECTED_BIT = 0,       /* 表示该连接是个子连接 */
        IPS_SEEN_REPLY_BIT = 1,     /* 表示该连接上双方向上都有数据包了 */
        IPS_ASSURED_BIT = 2,        /* TCP：在三次握手建立完连接后即设定该标志。UDP：如果在该连接上的两个方向都有数据包通过，
                                    则再有数据包在该连接上通过时，就设定该标志。ICMP：不设置该标志 */
        IPS_CONFIRMED_BIT = 3,      /* 表示该连接已被添加到net-&gt;ct.hash表中 */
        IPS_SRC_NAT_BIT = 4,        /*在POSTROUTING处，当替换reply tuple完成时, 设置该标记 */
        IPS_DST_NAT_BIT = 5,        /* 在PREROUTING处，当替换reply tuple完成时, 设置该标记 */
        /* Both together. */
        IPS_NAT_MASK = (IPS_DST_NAT | IPS_SRC_NAT),
        /* Connection needs TCP sequence adjusted. */
        IPS_SEQ_ADJUST_BIT = 6,
        IPS_SRC_NAT_DONE_BIT = 7,   /* 在POSTROUTING处，已被SNAT处理，并被加入到bysource链中，设置该标记 */
        IPS_DST_NAT_DONE_BIT = 8,   /* 在PREROUTING处，已被DNAT处理，并被加入到bysource链中，设置该标记 */
        /* Both together */
        IPS_NAT_DONE_MASK = (IPS_DST_NAT_DONE | IPS_SRC_NAT_DONE),
        IPS_DYING_BIT = 9,      /* 表示该连接正在被释放，内核通过该标志保证正在被释放的ct不会被其它地方再次引用。有了这个标志，当某个连接要被删除时，即使它还在net-&gt;ct.hash中，也不会再次被引用。*/
        IPS_FIXED_TIMEOUT_BIT = 10, /* 固定连接超时时间，这将不根据状态修改连接超时时间。通过函数nf_ct_refresh_acct()修改超时时间时检查该标志。 */
        IPS_TEMPLATE_BIT = 11,      /* 由CT target进行设置（这个target只能用在raw表中，用于为数据包构建指定ct，并打上该标志），用于表明这个ct是由CT target创建的 */
    };
</code></pre>

<p>连接跟踪对该连接上的每个数据包表现为以下几种状态之一，由enum ip_conntrack_info表示，被设置在skb->nfctinfo中。
<code>
    enum ip_conntrack_info {
        IP_CT_ESTABLISHED（0）,    /* 表示这个数据包对应的连接在两个方向都有数据包通过，并且这是ORIGINAL初始方向数据包（无论是TCP、UDP、ICMP数据包，只要在该连接的两个方向上已有数据包通过，就会将该连接设置为IP_CT_ESTABLISHED状态。不会根据协议中的标志位进行判断，例如TCP的SYN等）。但它表示不了这是第几个数据包，也说明不了这个CT是否是子连接。*/
        IP_CT_RELATED（1）,       /* 表示这个数据包对应的连接还没有REPLY方向数据包，当前数据包是ORIGINAL方向数据包。并且这个连接关联一个已有的连接，是该已有连接的子连接，（即status标志中已经设置了IPS_EXPECTED标志，该标志在init_conntrack()函数中设置）。但无法判断是第几个数据包（不一定是第一个）*/
        IP_CT_NEW（2）,           /* 表示这个数据包对应的连接还没有REPLY方向数据包，当前数据包是ORIGINAL方向数据包，该连接不是子连接。但无法判断是第几个数据包（不一定是第一个）*/
        IP_CT_IS_REPLY（3）,      /* 这个状态一般不单独使用，通常以下面两种方式使用 */
        IP_CT_ESTABLISHED + IP_CT_IS_REPLY（3）,  /* 表示这个数据包对应的连接在两个方向都有数据包通过，并且这是REPLY应答方向数据包。但它表示不了这是第几个数据包，也说明不了这个CT是否是子连接。*/
        IP_CT_RELATED + IP_CT_IS_REPLY（4）,      /* 这个状态仅在nf_conntrack_attach()函数中设置，用于本机返回REJECT，例如返回一个ICMP目的不可达报文， 或返回一个reset报文。它表示不了这是第几个数据包。*/
        IP_CT_NUMBER = IP_CT_IS_REPLY * 2 - 1（5）    /* 可表示状态的总数 */
    };
</code></p>

<p>以上就是连接跟踪里最重要的数据结构了，用于跟踪连接、记录状态、并对该连接的每个数据包设置一种状态。</p>

<p>除了上面的主要数据结构外，还有一些辅助数据结构，用于处理不同协议的私有信息、处理子连接、对conntrack进行扩展等。</p>

<h4>三层协议（IPv4/IPv6）</h4>

<p>利用nf_conntrack_proto.c文件中的
<code>
    nf_conntrack_l3proto_register(struct nf_conntrack_l3proto *proto)
    和
    nf_conntrack_l3proto_unregister(struct nf_conntrack_l3proto *proto)
</code>
在nf_ct_l3protos[]数组中注册自己的三层协议处理函数。</p>

<p><img src="/images/kernel/20221127-11.png" alt="" /></p>

<h4>四层协议（TCP/UDP）</h4>

<p>利用nf_conntrack_proto.c文件中的
<code>
    nf_conntrack_l4proto_register(struct nf_conntrack_l4proto *l4proto)
    和
    nf_conntrack_l4proto_unregister(struct nf_conntrack_l4proto *l4proto)
</code>
在nf_ct_protos[]数组中注册自己的四层协议处理函数。</p>

<p><img src="/images/kernel/20221127-12.png" alt="" /></p>

<h4>处理一个连接的子连接协议</h4>

<p>利用nf_conntrack_helper.c文件中的
<code>
    nf_conntrack_helper_register(struct nf_conntrack_helper *me)
</code>
来注册nf_conntrack_helper结构，</p>

<p>和nf_conntrack_expect.c文件中的
<code>
    nf_ct_expect_related_report(struct nf_conntrack_expect *expect, u32 pid, int report)
</code>
来注册nf_conntrack_expect结构。</p>

<p><img src="/images/kernel/20221127-13.png" alt="" /></p>

<h4>扩展连接跟踪结构（nf_conn）</h4>

<p>利用nf_conntrack_extend.c文件中的
<code>
    nf_ct_extend_register(struct nf_ct_ext_type *type)
    和
    nf_ct_extend_unregister(struct nf_ct_ext_type *type)
</code>
进行扩展，并修改连接跟踪相应代码来利用这部分扩展功能。</p>

<p><img src="/images/kernel/20221127-14.png" alt="" /></p>

<p>了解了上面的数据结构，我们下面来看一下nf_conntrack的执行流程以及如何利用这些数据结构的。首先来看一下nf_conntrack模块加载时的初始化流程。</p>

<p><img src="/images/kernel/20221127-15.png" alt="" /></p>

<h4>nf_conntrack的初始化</h4>

<p>就是初始化上面提到的那些数据结构，它在内核启动时调用nf_conntrack_standalone_init()函数进行初始化的。初始化完成后，构建出如下图所示的结构图，只是不包含下图中与连接有关的信息（nf_conn和nf_conntrack_expect结构）。</p>

<p><img src="/images/kernel/20221127-16.png" alt="" /></p>

<p>上图中有三个HASH桶，ct_hash、expect_hash、helper_hash这三个HASH桶大小在初始化时就已确定，后面不能再更改。其中ct_hash、expect_hash可在加载nf_conntrack.ko模块时通过参数hashsize和expect_hashsize进行设定，而helper_hash不能通过参数修改，它的默认值是page/sizeof(helper_hash)。</p>

<p>下面再来看一个当创建子连接时，各个数据结构之间的关系。</p>

<p><img src="/images/kernel/20221127-17.png" alt="" /></p>

<p>nf_conn和nf_conntrack_expect都有最大个数限制。nf_conn通过全局变量nf_conntrack_max限制，可通过 /proc/sys/net/netfilter/nf_conntrack_max 文件在运行时修改。nf_conntrack_expect通过全局变量nf_ct_expect_max限制，可通过 /proc/sys/net/netfilter/nf_conntrack_expect_max 文件在运行时修改。nf_conntrack_helper没有最大数限制，因为这个是通过注册不同协议的模块添加的，大小取决于动态协议跟踪模块的多少，一般不会很大。</p>

<p>上面两幅数据结构图中，大部分都已介绍过，下面介绍一下netns_ct数据结构，该结构主要用于linux的网络命名空间，表示nf_conntrack在不同的命名空间中都有一套独立的数据信息（这是另一个话题，这里就不再深入讨论了）。</p>

<pre><code>    struct netns_ct {
        atomic_t            count;              /* 当前连接表中连接的个数 */
        unsigned int        expect_count;           /* nf_conntrack_helper创建的期待子连接nf_conntrack_expect项的个数 */
        unsigned int        htable_size;            /* 存储连接（nf_conn）的HASH桶的大小 */
        struct kmem_cache   *nf_conntrack_cachep;       /* 指向用于分配nf_conn结构而建立的高速缓存（slab）对象 */
        struct hlist_nulls_head *hash;              /* 指向存储连接（nf_conn）的HASH桶 */
        struct hlist_head       *expect_hash;           /* 指向存储期待子连接nf_conntrack_expect项的HASH桶 */
        struct hlist_nulls_head unconfirmed;            /* 对于一个链接的第一个包，在init_conntrack()函数中会将该包original方向的tuple结构挂入该链，这是因为在此时还不确定该链接会不会被后续的规则过滤掉，如果被过滤掉就没有必要挂入正式的链接跟踪表。在ipv4_confirm()函数中，会将unconfirmed链中的tuple拆掉，然后再将original方向和reply方向的tuple挂入到正式的链接跟踪表中，即init_net.ct.hash中，这是因为到达ipv4_confirm()函数时，应经在钩子NF_IP_POST_ROUTING处了，已经通过了前面的filter表。 通过cat  /proc/net/nf_conntrack显示连接，是不会显示该链中的连接的。但总的连接个数（net-&gt;ct.count）包含该链中的连接。当注销l3proto、l4proto、helper、nat等资源或在应用层删除所有连接（conntrack -F）时，除了释放confirmed连接（在net-&gt;ct.hash中的连接）的资源，还要释放unconfirmed连接（即在该链中的连接）的资源。*/
        struct hlist_nulls_head dying;              /* 释放连接时，通告DESTROY事件失败的ct被放入该链中，并设置定时器，等待下次通告。 通过cat  /proc/net/nf_conntrack显示连接，是不会显示该链中的连接的。但总的连接个数（net-&gt;ct.count）包含该链中的连接。当注销连接跟踪模块时，同时要清除正再等待被释放的连接（即该链中的连接）*/
        struct ip_conntrack_stat    __percpu *stat;         /* 连接跟踪过程中的一些状态统计，每个CPU一项，目的是为了减少锁 */
        int         sysctl_events;          /* 是否开启连接事件通告功能 */
        unsigned int        sysctl_events_retry_timeout;    /* 通告失败后，重试通告的间隔时间，单位是秒 */
        int         sysctl_acct;            /* 是否开启每个连接数据包统计功能 */
        int         sysctl_checksum;
        unsigned int        sysctl_log_invalid;      /* Log invalid packets */
    #ifdef CONFIG_SYSCTL
        struct ctl_table_header *sysctl_header;
        struct ctl_table_header *acct_sysctl_header;
        struct ctl_table_header *event_sysctl_header;
    #endif
        int         hash_vmalloc;           /* 存储连接（nf_conn）的HASH桶是否是使用vmalloc()进行分配的 */
        int         expect_vmalloc;         /* 存储期待子连接nf_conntrack_expect项的HASH桶是否是使用vmalloc()进行分配的 */
        char            *slabname;          /* 用于分配nf_conn结构而建立的高速缓存（slab）对象的名字 */
    };
</code></pre>

<p>从nf_conntrack的框架来看，它可用于跟踪任何三层和四协议的连接，但目前在三层协议只实现了IPv4和IPv6的连接跟踪，下面我们以IPv4为例，介绍一下该协议是如何利用nf_conntrack框架和netfilter实现连接跟踪的。有关netfilter框架，可参考我的另一个帖子</p>

<p><a href="http://bbs.chinaunix.net/forum.php?mod=viewthread&amp;tid=3749208&amp;fromuid=20171559">linux-2.6.35.6内核netfilter框架</a></p>

<p>首先介绍一下IPv4协议连接跟踪模块的初始化。</p>

<p>Ipv4连接跟踪模块注册了自己的3层协议，和IPv4相关的三个4层协议TCP、UDP、ICMP。注册后的结构图如下图所示：</p>

<p><img src="/images/kernel/20221127-18.png" alt="" /></p>

<p>在netfilter框架中利用
<code>
    nf_register_hook(struct nf_hook_ops *reg)、nf_unregister_hook(struct nf_hook_ops *reg)
</code>
函数注册自己的钩子项，调用nf_conntrack_in()函数来建立相应连接。</p>

<p><img src="/images/kernel/20221127-19.png" alt="" /></p>

<p>这样数据包就会经过ipv4注册的钩子项，并调用nf_conntrack_in()函数建立连接表项，连接表项中的tuple由ipv4注册的3/4层协议处理函数构建。</p>

<p>ipv4_conntrack_in() 挂载在NF_IP_PRE_ROUTING点上。该函数主要功能是创建链接，即创建struct nf_conn结构，同时填充struct nf_conn中的一些必要的信息，例如链接状态、引用计数、helper结构等。</p>

<p>ipv4_confirm() 挂载在NF_IP_POST_ROUTING和NF_IP_LOCAL_IN点上。该函数主要功能是确认一个链接。对于一个新链接，在ipv4_conntrack_in()函数中只是创建了struct nf_conn结构，但并没有将该结构挂载到链接跟踪的Hash表中，因为此时还不能确定该链接是否会被NF_IP_FORWARD点上的钩子函数过滤掉，所以将挂载到Hash表的工作放到了ipv4_confirm()函数中。同时，子链接的helper功能也是在该函数中实现的。</p>

<p>ipv4_conntrack_local() 挂载在NF_IP_LOCAL_OUT点上。该函数功能与ipv4_conntrack_in()函数基本相同，但其用来处理本机主动向外发起的链接。</p>

<p>nf_conntrack_ipv4_compat_init() &ndash;> register_pernet_subsys() &ndash;> ip_conntrack_net_init() 创建/proc文件ip_conntrack和ip_conntrack_expect</p>

<p>如上面所示，IPv4连接跟踪模块已初始化完成，下面我们来看一下它创建连接的流程图。上图中连接的建立主要由三个函数来完成，即ipv4_conntrack_in()，ipv4_confirm()与ipv4_conntrack_local()。其中ipv4_conntrack_in()与ipv4_conntrack_local()都是通过调用函数nf_conntrack_in()来实现的，所以下面我们主要关注nf_conntrack_in()与ipv4_confirm()这两个函数。nf_conntrack_in()函数主要完成创建链接、添加链接的扩展结构(例如helper, acct结构)、设置链接状态等。ipv4_confirm()函数主要负责确认链接(即将链接挂入到正式的链接表中)、执行helper函数、启动链接超时定时器等。另外还有一个定时器函数death_by_timeout(), 该函数负责链接到期时删除该链接。</p>

<p>nf_conntrack_in()函数流程图</p>

<p><img src="/images/kernel/20221127-20.png" alt="" /></p>

<p>ipv4_confirm()函数流程图</p>

<p><img src="/images/kernel/20221127-21.png" alt="" /></p>

<p>death_by_timeout()函数流程图</p>

<p><img src="/images/kernel/20221127-22.png" alt="" /></p>

<p>上图中有一点需要说明，由于skb会引用nf_conn，同时会增加它的引用计数，所以当skb被释放时，也要释放nf_conn的引用计数，并且在nf_conn引用计数为0时，要释放全部资源。</p>

<p>当数据包经过nf_conntrack_in()和ipv4_confirm()函数处理流程后，就会建立起3楼第二幅结构图所示的连接nf_conn。同时这两个函数已经包含了子连接的处理流程，即流程图中help和exp的处理。子连接建立后的结构图如3楼第三幅结构图，主链接与子连接通过helper和expect关联起来。</p>

<p>连接跟踪到此就介绍完了，下面介绍IPv4基于nf_conntrack框架适合实现NAT转换的。先介绍IPv4-NAT初始化的资源，然后处理流程。</p>

<h2>IPv4-NAT连接跟踪相关部分通过函数nf_nat_init()初始化</h2>

<p>调用nf_ct_extend_register() 注册一个连接跟踪的扩展功能。</p>

<p><img src="/images/kernel/20221127-23.png" alt="" /></p>

<p>调用register_pernet_subsys() &ndash;> nf_nat_net_init() 创建net->ipv4.nat_bysource的HASH表，大小等于net->ct.htable_size。</p>

<p>初始化nf_nat_protos[]数组，为TCP、UDP、ICMP协议指定专用处理结构，其它协议都指向默认处理结构。</p>

<p><img src="/images/kernel/20221127-24.png" alt="" /></p>

<p>为nf_conntrack_untracked连接设置IPS_NAT_DONE_MASK标志。</p>

<p>将NAT模块的全局变量l3proto指向IPV4协议的nf_conntrack_l3proto结构。</p>

<p>设置全局指针nf_nat_seq_adjust_hook指向nf_nat_seq_adjust()函数。</p>

<p>设置全局指针nfnetlink_parse_nat_setup_hook指向nfnetlink_parse_nat_setup()函数。</p>

<p>设置全局指针nf_ct_nat_offset指向nf_nat_get_offset()函数。</p>

<h2>IPv4-NAT功能的iptables部分通过函数nf_nat_standalone_init()初始化</h2>

<p>调用nf_nat_rule_init() &ndash;> nf_nat_rule_net_init()在iptables中注册一个NAT表（通过ipt_register_table()函数，参考另一个帖子iptables）</p>

<p>调用 nf_nat_rule_init() 注册SNAT target和DNAT target（通过xt_register_target()函数）</p>

<p><img src="/images/kernel/20221127-25.png" alt="" /></p>

<p>调用nf_register_hooks() 挂载NAT的HOOK函数，橙色部分为NAT挂载的HOOK函数（参考另一个帖子netfilter）</p>

<p><img src="/images/kernel/20221127-26.png" alt="" /></p>

<p>根据上面介绍，可以看到IPv4-NAT的主要是通过nf_nat_fn()钩子函数处理的，下面我就来看看nf_nat_fn()函数的处理流程。</p>

<p><img src="/images/kernel/20221127-27.png" alt="" /></p>

<p>针对上图中的nf_nat_setup_info()函数进一步描述</p>

<p><img src="/images/kernel/20221127-28.png" alt="" /></p>

<h2>下面对NAT转换算法中重要部分做一些文字说明</h2>

<p>每个ct在第一个包就会做好snat与dnat, nat的信息全放在reply tuple中，orig tuple不会被改变。一旦第一个包建立好nat信息后，后续再也不会修改tuple内容了。</p>

<p>orig tuple中的地址信息与reply tuple中的地址信息就是原始数据包的信息。例如对A->B数据包同时做snat与dnat，PREROUTING处B被dnat到D，POSTROUTING处A被snat到C。则ct的内容是:  A->B | D->C,  A->B说明了orig方向上数据包刚到达墙时的地址内容，D->C说明reply方向上数据包刚到达墙时的地址内容。</p>

<p>在代码中有很多!dir操作，原理是: 当为了反向的数据包做事情的时候就取反向tuple的数据，这样才能保证NAT后的tuple信息被正确使用。</p>

<p>bysource链中链接了所有CT（做过NAT和未做过NAT），通过ct->nat->bysource，HASH值的计算使用的是CT的orig tuple。其作用是，当为一个新连接做SNAT，需要得到地址映射时，首先对该链进行查找，查找此源IP、协议和端口号是否已经做过了映射。如果做过的话，就需要在SNAT转换时，映射为相同的源IP和端口号。为什么要这么做呢？因为对于UDP来说，有些协议可能会用相同端口和同一主机不同的端口（或不同的主机）进行通信。此时，由于目的地不同，原来已有的映射不可使用，需要一个新的连接。但为了保证通信的的正确性，此时，就要映射为相同的源IP和端口号。其实就是为NAT的打洞服务的。所以bysource就是以源IP、协议和端口号为hash值的一个表，这样在做snat时保证相同的ip+port影射到相同的ip+port。</p>

<p>IP_NAT_RANGE_PROTO_RANDOM指的是做nat时，当计算端口时，如果没有此random标志，则会先使用原始得tuple中的端口试一下看是否可用，如果可用就使用该原始端口作为nat后的端口， 即尽量保证转换后的端口与转换前的端口保持一致。如果不可用，再根据nat的端口算法计算出一个端口。 如果有此标记，则直接根据端口算法计算出端口。</p>

<p>第一个包之后，ct的两个方向的tuple内容就固定了，所有的nat操作都必须在第一个包就完成。所以会有daddr = &amp;ct->tuplehash[!dir].tuple.dst.u3;这样的操作。</p>

<p>IPS_SRC_NAT与IPS_DST_NAT，如果被设置，表示经过了NAT，并且ct中的tuple被做过SNAT或DNAT。</p>

<p>数据包永远都是在PREROUTING链做目的地址和目的端口转换，在POSTROUTING链做原地址和原端口转换。是否要做NAT转换则要根数据包方向（dir）和NAT标志（IPS_SRC_NAT或IPS_DST_NAT）来判断。</p>

<p>在PREROUTING链上&mdash;>数据包是original方向、并且连接上设置IPS_DST_NAT标志，或数据包是reply方向、并且连接上设置IPS_SRC_NAT标志，则做DNAT转换。</p>

<p>在POSTROUTING链上&mdash;>数据包是original方向、并且连接上设置IPS_SRC_NAT标志，或数据包是reply方向、并且连接上设置IPS_DST_NAT标志，则做SNAT转换。</p>

<p>IPS_DST_NAT_DONE_BIT与IPS_SRC_NAT_DONE_BIT，表示该ct进入过NAT模块，已经进行了源或者目的NAT判断，但并不表示ct中的tuple被修改过。</p>

<p>源目的nat都是在第一个包就判断完成的，假设先添加了snat策略，第一个包通过，这时又添加了dnat策略, 第二个包到来时是不会匹配dnat策略的 。</p>

<p>对于一个ct，nf_nat_setup_info函数最多只能进入2次，第一次DNAT，第二次SNAT。在nf_nat_follow_master函数中，第一次SNAT，第二次DNAT。</p>

<p>下面介绍有子连接的NAT实现。有两个关键点：1.主链接能正确的构建出NAT后的expect来识别子连接。2.能够修改主链接数据通道的信息为NAT后的信息。这两点都在动态协议的help中完成，下面我们来看一下它的流程图：</p>

<p><img src="/images/kernel/20221127-29.png" alt="" /></p>

<h2>下面针对有无子连接的NAT做一下对比</h2>

<h4>无子连接的NAT</h4>

<p>一个ct用于跟踪一个连接的双方向数据，ct->orig_tuple用于跟踪初始方向数据，ct->reply_tuple用于跟踪应答方向数据。当根据初始方向数据构建ct->orig_tuple时，同时要构建出ct->reply_tuple，用于识别同一连接上应答方向数据。</p>

<p>如果初始方向的数据在通过防火墙后被做了NAT转换，为识别出NAT数据的应答数据包，则对ct->reply_tuple也要做NAT转换。同时ct上做好相应NAT标记。</p>

<p>因此，上面的信息在初始方向第一个数据包通过后，就要求全部建立好，并且不再改变。</p>

<p>一个连接上不同方向的数据，都有相对应的tuple（orig_tuple和reply_tuple），所以该连接后续数据都将被识别出来。如果ct上有NAT标记，则根据要去往方向（即另一个方向）的tuple对数据做NAT转换。所以会有ct->tuplehash[!dir].tuple这样的操作。</p>

<h4>有子连接的NAT</h4>

<p>子连接是由主连接构建的expect项识别出来的。</p>

<p>help用于构建expect项，它期待哪个方向的连接，则用那个方向的tuple和数据包中数据通道信息构建expect项。例如期待和当前数据包相反方向的连接，则用相反方向的tuple中的信息（ct->tuplehash[!dir].tuple）。调用help时，NAT转换都已完成（tuple中都包含有正确的识别各自方向的信息），所以这时所使用的信息都是正确和所期望的信息。</p>

<p>如果子连接还可能有子连接，则构建expect项时，初始化一个helper结构，并赋值给expect->helper指针。</p>

<p>如果该连接已被做了NAT转换，则对数据包中数据通道信息也要做NAT转换</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[strongswan ipsec 配置 -- win]]></title>
    <link href="http://abcdxyzk.github.io/blog/2022/09/24/strongswan-conf-win/"/>
    <updated>2022-09-24T23:01:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2022/09/24/strongswan-conf-win</id>
    <content type="html"><![CDATA[<h2>RunAsDate 修改时间运行，达到永久试用</h2>

<p><a href="/download/tools/RunAsDate.zip">RunAsDate.zip</a></p>

<p>运行32位版本</p>

<p><img src="/images/kernel/vpn-win/20221129-40.png" alt="" /></p>

<p>需要运行的程序 C:\Program Files (x86)\TheGreenBow\TheGreenBow VPN\vpnconf.exe</p>

<p>时间/日期 调到安装之前</p>

<p>可以运行 或者 创建桌面快捷方式</p>

<hr />

<h2>server</h2>

<pre><code>    setenforce 0

    vim /etc/sysconfig/selinux
    SELINUX=enforcing       =&gt;      SELINUX=disabled
</code></pre>

<p>vim /etc/strongswan/strongswan.d/charon.conf</p>

<pre><code>    port_nat_t = 4501
</code></pre>

<p>vim /etc/strongswan/swanctl/swanctl.conf</p>

<pre><code>    # Include config snippets
    include conf.d/*.conf

    connections {
        # cp www.abcxyzkk.xyz_apache/root_bundle.crt /etc/strongswan/swanctl/x509ca/
        # cp www.abcxyzkk.xyz_apache/www.abcxyzkk.xyz.crt /etc/strongswan/swanctl/x509/
        # cp www.abcxyzkk.xyz_apache/www.abcxyzkk.xyz.key /etc/strongswan/swanctl/private/
        # EAP android 客户端 https://download.strongswan.org/Android/
        # EAP android 客户端 https://raw.githubusercontent.com/abcdxyzk/abcdxyzk.github.io_files/master/tools/vpn/strongSwan-2.3.3.apk
        # EAP 服务端转发上网 iptables -t nat -A POSTROUTING -s 100.64.0.0/24 -o eth0 -j MASQUERADE
        # echo 1 &gt; /proc/sys/net/ipv4/ip_forward
            testEAP {
                    version = 2
                    proposals = default
                    local_addrs = 192.168.100.178
                    pools = pool1
                    rekey_time = 24h

                    local {
                            certs = www.npcable.cn.crt
                            id = www.npcable.cn
                    }
                    remote {
                            auth = eap-mschapv2
                            id = %any
                    }
                    children {
                            testEAP_child {
                                    # local_ts = 0.0.0.0/0
                                    local_ts = 192.168.100.0/24
                                    #remote_ts = 100.64.0.0/24
                                    esp_proposals = default

                                    rekey_time = 24h
                            }
                    }
            }
    }

    secrets {
            private-www {
                    file = www.npcable.cn.key
            }
            eap-user {
                    id = abc
                    secret = abc123
            }
            eap-user1 {
                    id = abc1
                    secret = abc123
            }
    }

    pools {
        pool1 {
            addrs = 100.64.0.0/24
            # hk的时候必须要填
            dns = 8.8.8.8
        }
    }
</code></pre>

<pre><code>    service strongswan restart

    swanctl --load-all
    swanctl --list-sas
</code></pre>

<h2>exe</h2>

<p><a href="https://raw.githubusercontent.com/abcdxyzk/abcdxyzk.github.io_files/master/tools/vpn/TheGreenBow_VPN_Client_6.64.3.2.exe">https://raw.githubusercontent.com/abcdxyzk/abcdxyzk.github.io_files/master/tools/vpn/TheGreenBow_VPN_Client_6.64.3.2.exe</a></p>

<p><a href="https://raw.githubusercontent.com/abcdxyzk/abcdxyzk.github.io_files/master/tools/vpn/tgbvpnvirtm.inf_amd64_6.1.zip">https://raw.githubusercontent.com/abcdxyzk/abcdxyzk.github.io_files/master/tools/vpn/tgbvpnvirtm.inf_amd64_6.1.zip</a></p>

<p><a href="https://raw.githubusercontent.com/abcdxyzk/abcdxyzk.github.io_files/master/tools/vpn/tgbmpenum.inf_amd64_6.1.zip">https://raw.githubusercontent.com/abcdxyzk/abcdxyzk.github.io_files/master/tools/vpn/tgbmpenum.inf_amd64_6.1.zip</a></p>

<h2>先调系统时间</h2>

<p>改到2035年左右，这样可以一直试用。时间改太大也不行？</p>

<h2>win10</h2>

<p>安装 6.64.3.2 就 OK</p>

<h2>win7</h2>

<p>先安装 6.64.3.2 , 再调整两个驱动: 网络适配器、系统设备</p>

<h3>调整驱动 TheGreenBow Virtual Miniport Adapter</h3>

<h4>计算机管理 &ndash;> 设备管理器 &ndash;> 点击"网络适配器", 再点击"菜单"上的"操作"， 再点击"添加过时硬件(L)"</h4>

<p><img src="/images/kernel/vpn-win/20220924-2.png" alt="" /></p>

<p><img src="/images/kernel/vpn-win/20220924-3.png" alt="" /></p>

<p><img src="/images/kernel/vpn-win/20220924-4.png" alt="" /></p>

<p><img src="/images/kernel/vpn-win/20220924-5.png" alt="" /></p>

<p><img src="/images/kernel/vpn-win/20220924-6.png" alt="" /></p>

<p><img src="/images/kernel/vpn-win/20220924-7.png" alt="" /></p>

<h4>再卸载这个设备，同时勾选"删除此设备的驱动程序软件"</h4>

<p><img src="/images/kernel/vpn-win/20220924-9.png" alt="" /></p>

<p><img src="/images/kernel/vpn-win/20220924-10.png" alt="" /></p>

<h4>再添加从 6.10.14.4 那里copy来的驱动</h4>

<p>点击"网络适配器", 再点击"菜单"上的"操作"， 再点击"添加过时硬件(L)"</p>

<p>tgbvpnvirtm.inf_amd64_6.1.zip</p>

<p><img src="/images/kernel/vpn-win/20220924-11.png" alt="" /></p>

<p><img src="/images/kernel/vpn-win/20220924-12.png" alt="" /></p>

<p><img src="/images/kernel/vpn-win/20220924-13.png" alt="" /></p>

<h3>调整驱动 TheGreenBow VPN Miniport Enumerator</h3>

<h4>不用先添加，可以直接卸载。同时勾选"删除此设备的驱动程序软件"</h4>

<p><img src="/images/kernel/vpn-win/20220924-1.jpg" alt="" /></p>

<h4>再添加从 6.10.14.4 那里copy来的驱动</h4>

<p>点击"系统设备", 再点击"菜单"上的"操作"， 再点击"添加过时硬件(L)"</p>

<p>tgbmpenum.inf_amd64_6.1.zip</p>

<p><img src="/images/kernel/vpn-win/20220924-14.png" alt="" /></p>

<p><img src="/images/kernel/vpn-win/20220924-15.png" alt="" /></p>

<p><img src="/images/kernel/vpn-win/20220924-16.png" alt="" /></p>

<p><img src="/images/kernel/vpn-win/20220924-17.png" alt="" /></p>

<h2>配置</h2>

<p>rekey可能失败，改长 ike_sa, ipsec_sa 的存活时间</p>

<p><img src="/images/kernel/vpn-win/20220924-20.png" alt="" /></p>

<p><img src="/images/kernel/vpn-win/20220924-21.png" alt="" /></p>

<p><img src="/images/kernel/vpn-win/20220924-22.png" alt="" /></p>

<p><img src="/images/kernel/vpn-win/20220924-23.png" alt="" /></p>

<h2>win7 在连接的时候有一定的失败概率</h2>

<h4>多网卡可能会失败，禁用掉其他网卡</h4>

<h4>点两下 “从网关请求设置” 好像能恢复。</h4>

<p><img src="/images/kernel/vpn-win/20220924-30.png" alt="" /></p>

<h2>破解</h2>

<p>似乎是这个字段，但基本无法破解</p>

<p>HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\App Paths\TgbIke.exe</p>

<p>sBoot32</p>
]]></content>
  </entry>
  
</feed>
