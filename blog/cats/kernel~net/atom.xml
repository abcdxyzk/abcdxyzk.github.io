<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: kernel~net | kk Blog —— 通用基础]]></title>
  <link href="http://abcdxyzk.github.io/blog/cats/kernel~net/atom.xml" rel="self"/>
  <link href="http://abcdxyzk.github.io/"/>
  <updated>2024-12-12T15:09:44+08:00</updated>
  <id>http://abcdxyzk.github.io/</id>
  <author>
    <name><![CDATA[kk]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Linux内核之GRE处理分析]]></title>
    <link href="http://abcdxyzk.github.io/blog/2022/11/27/kernel-gre/"/>
    <updated>2022-11-27T20:47:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2022/11/27/kernel-gre</id>
    <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/s2603898260/article/details/115773153">https://blog.csdn.net/s2603898260/article/details/115773153</a></p>

<p><a href="https://blog.csdn.net/u014044624/article/details/106596000">https://blog.csdn.net/u014044624/article/details/106596000</a></p>

<p><img src="/images/kernel/20221127-31.png" alt="" /></p>

<hr />

<h2>GRE</h2>

<p>GRE（Generic Routing Encapsulation，通用路由封装）协议是对某些网络层协议（如IP 和IPX）的数据报文进行封装，使这些被封装的数据报文能够在另一个网络层协议（如IP）中传输。</p>

<p>在大多数常规情况下，系统拥有一个有效载荷（或负载）包，需要将它封装并发送至某个目的地。首先将有效载荷封装在一个 GRE 包中，然后将此 GRE 包封装在其它某协议中并进行转发。此外发协议即为发送协议。当 IPv4 被作为 GRE 有效载荷传输时，协议类型字段必须被设置为 0x800 。当一个隧道终点拆封此含有 IPv4 包作为有效载荷的 GRE 包时， IPv4 包头中的目的地址必须用来转发包，并且需要减少有效载荷包的 TTL 。值得注意的是，在转发这样一个包时，如果有效载荷包的目的地址就是包的封装器（也就是隧道另一端），就会出现回路现象。在此情形下，必须丢弃该包。当 GRE 包被封装在 IPv4 中时，需要使用 IPv4 协议 47 。</p>

<p><img src="/images/kernel/20221127-32.png" alt="" /></p>

<p>GRE采用了Tunnel（隧道）技术，是VPN（Virtual Private Network）的第三层隧道协议。Tunnel 是一个虚拟的点对点的连接，提供了一条通路使封装的数据报文能够在这个通路上传输，并且在一个Tunnel 的两端分别对数据报进行封装及解封装。</p>

<p><img src="/images/kernel/20221127-33.png" alt="" /></p>

<h3>GRE包发送过程：</h3>

<p>发送过程是很简单的，因为 router A 上配置了一条路由规则，凡是发往 10.0.2.0 网络的包都要经过 netb 这个 tunnel 设备，在内核中经过 forward 之后就最终到达这个 GRE tunnel 设备的 ndo_start_xmit()，也就是 ipgre_tunnel_xmit() 函数。这个函数所做的事情无非就是通过 tunnel 的 header_ops 构造一个新的头，并把对应的外部 IP 地址填进去，最后发送出去。</p>

<h4>Linux kernel函数调用分析：</h4>

<p><img src="/images/kernel/20221127-34.png" alt="" /></p>

<h3>GRE包接收过程：</h3>

<p>接收过程，即 router B 上面进行的操作。这里需要指出的一点是，GRE tunnel 自己定义了一个新的 IP proto，也就是 IPPROTO_GRE。当 router B 收到从 router A 过来的这个包时，它暂时还不知道这个是 GRE 的包，它首先会把它当作普通的 IP 包处理。因为外部的 IP 头的目的地址是该路由器的地址，所以它自己会接收这个包，把它交给上层，到了 IP 层之后才发现这个包不是 TCP，UDP，而是 GRE，这时内核会转交给 GRE 模块处理。</p>

<p>ipgre_rcv() 所做的工作是：通过外层IP 头找到对应的 tunnel，然后剥去外层 IP 头，把这个“新的”包重新交给 IP 栈去处理，像接收到普通 IP 包一样。到了这里，“新的”包处理和其它普通的 IP 包已经没有什么两样了：根据 IP 头中目的地址转发给相应的 host。</p>

<p>注：在这里可以把gre当做L4层协议。</p>

<h4>Linux kernel函数调用分析：</h4>

<p><img src="/images/kernel/20221127-35.png" alt="" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Linux nf_conntrack连接跟踪的实现]]></title>
    <link href="http://abcdxyzk.github.io/blog/2022/11/27/nf-conntrack/"/>
    <updated>2022-11-27T20:09:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2022/11/27/nf-conntrack</id>
    <content type="html"><![CDATA[<p><a href="http://bbs.chinaunix.net/thread-4082396-1-1.html">http://bbs.chinaunix.net/thread-4082396-1-1.html</a></p>

<p>连接跟踪，顾名思义，就是识别一个连接上双方向的数据包，同时记录状态。下面看一下它的数据结构：</p>

<pre><code>    struct nf_conn {
        /* Usage count in here is 1 for hash table/destruct timer, 1 per skb, plus 1 for any connection(s) we are `master' for */
        struct  nf_conntrack  ct_general;       /* 连接跟踪的引用计数 */
        spinlock_t  lock;

        /* Connection tracking(链接跟踪)用来跟踪、记录每个链接的信息(目前仅支持IP协议的连接跟踪)。
            每个链接由“tuple”来唯一标识，这里的“tuple”对不同的协议会有不同的含义，例如对tcp,udp
             来说就是五元组: (源IP，源端口，目的IP, 目的端口，协议号)，对ICMP协议来说是: (源IP, 目
            的IP, id, type, code), 其中id,type与code都是icmp协议的信息。链接跟踪是防火墙实现状态检
            测的基础，很多功能都需要借助链接跟踪才能实现，例如NAT、快速转发、等等。*/
        struct  nf_conntrack_tuple_hash  tuplehash[IP_CT_DIR_MAX];

        unsigned long  status;                 /* 可以设置由enum ip_conntrack_status中描述的状态 */

        struct  nf_conn  *master;           /* 如果该连接是某个连接的子连接，则master指向它的主连接 */
        /* Timer function; drops refcnt when it goes off. */
        struct  timer_list  timeout;

        union nf_conntrack_proto proto;     /* 用于保存不同协议的私有数据 */
        /* Extensions */
        struct nf_ct_ext *ext;          /* 用于扩展结构 */
    };
</code></pre>

<p>这个结构非常简单，其中最主要的就是tuplehash（跟踪连接双方向数据）和status（记录连接状态），这也连接跟踪最主要的功能。</p>

<p>在status中可以设置的标志，由下面的enum ip_conntrack_status描述，它们可以共存。这些标志设置后就不会再被清除。</p>

<pre><code>    enum ip_conntrack_status {
        IPS_EXPECTED_BIT = 0,       /* 表示该连接是个子连接 */
        IPS_SEEN_REPLY_BIT = 1,     /* 表示该连接上双方向上都有数据包了 */
        IPS_ASSURED_BIT = 2,        /* TCP：在三次握手建立完连接后即设定该标志。UDP：如果在该连接上的两个方向都有数据包通过，
                                    则再有数据包在该连接上通过时，就设定该标志。ICMP：不设置该标志 */
        IPS_CONFIRMED_BIT = 3,      /* 表示该连接已被添加到net-&gt;ct.hash表中 */
        IPS_SRC_NAT_BIT = 4,        /*在POSTROUTING处，当替换reply tuple完成时, 设置该标记 */
        IPS_DST_NAT_BIT = 5,        /* 在PREROUTING处，当替换reply tuple完成时, 设置该标记 */
        /* Both together. */
        IPS_NAT_MASK = (IPS_DST_NAT | IPS_SRC_NAT),
        /* Connection needs TCP sequence adjusted. */
        IPS_SEQ_ADJUST_BIT = 6,
        IPS_SRC_NAT_DONE_BIT = 7,   /* 在POSTROUTING处，已被SNAT处理，并被加入到bysource链中，设置该标记 */
        IPS_DST_NAT_DONE_BIT = 8,   /* 在PREROUTING处，已被DNAT处理，并被加入到bysource链中，设置该标记 */
        /* Both together */
        IPS_NAT_DONE_MASK = (IPS_DST_NAT_DONE | IPS_SRC_NAT_DONE),
        IPS_DYING_BIT = 9,      /* 表示该连接正在被释放，内核通过该标志保证正在被释放的ct不会被其它地方再次引用。有了这个标志，当某个连接要被删除时，即使它还在net-&gt;ct.hash中，也不会再次被引用。*/
        IPS_FIXED_TIMEOUT_BIT = 10, /* 固定连接超时时间，这将不根据状态修改连接超时时间。通过函数nf_ct_refresh_acct()修改超时时间时检查该标志。 */
        IPS_TEMPLATE_BIT = 11,      /* 由CT target进行设置（这个target只能用在raw表中，用于为数据包构建指定ct，并打上该标志），用于表明这个ct是由CT target创建的 */
    };
</code></pre>

<p>连接跟踪对该连接上的每个数据包表现为以下几种状态之一，由enum ip_conntrack_info表示，被设置在skb->nfctinfo中。
<code>
    enum ip_conntrack_info {
        IP_CT_ESTABLISHED（0）,    /* 表示这个数据包对应的连接在两个方向都有数据包通过，并且这是ORIGINAL初始方向数据包（无论是TCP、UDP、ICMP数据包，只要在该连接的两个方向上已有数据包通过，就会将该连接设置为IP_CT_ESTABLISHED状态。不会根据协议中的标志位进行判断，例如TCP的SYN等）。但它表示不了这是第几个数据包，也说明不了这个CT是否是子连接。*/
        IP_CT_RELATED（1）,       /* 表示这个数据包对应的连接还没有REPLY方向数据包，当前数据包是ORIGINAL方向数据包。并且这个连接关联一个已有的连接，是该已有连接的子连接，（即status标志中已经设置了IPS_EXPECTED标志，该标志在init_conntrack()函数中设置）。但无法判断是第几个数据包（不一定是第一个）*/
        IP_CT_NEW（2）,           /* 表示这个数据包对应的连接还没有REPLY方向数据包，当前数据包是ORIGINAL方向数据包，该连接不是子连接。但无法判断是第几个数据包（不一定是第一个）*/
        IP_CT_IS_REPLY（3）,      /* 这个状态一般不单独使用，通常以下面两种方式使用 */
        IP_CT_ESTABLISHED + IP_CT_IS_REPLY（3）,  /* 表示这个数据包对应的连接在两个方向都有数据包通过，并且这是REPLY应答方向数据包。但它表示不了这是第几个数据包，也说明不了这个CT是否是子连接。*/
        IP_CT_RELATED + IP_CT_IS_REPLY（4）,      /* 这个状态仅在nf_conntrack_attach()函数中设置，用于本机返回REJECT，例如返回一个ICMP目的不可达报文， 或返回一个reset报文。它表示不了这是第几个数据包。*/
        IP_CT_NUMBER = IP_CT_IS_REPLY * 2 - 1（5）    /* 可表示状态的总数 */
    };
</code></p>

<p>以上就是连接跟踪里最重要的数据结构了，用于跟踪连接、记录状态、并对该连接的每个数据包设置一种状态。</p>

<p>除了上面的主要数据结构外，还有一些辅助数据结构，用于处理不同协议的私有信息、处理子连接、对conntrack进行扩展等。</p>

<h4>三层协议（IPv4/IPv6）</h4>

<p>利用nf_conntrack_proto.c文件中的
<code>
    nf_conntrack_l3proto_register(struct nf_conntrack_l3proto *proto)
    和
    nf_conntrack_l3proto_unregister(struct nf_conntrack_l3proto *proto)
</code>
在nf_ct_l3protos[]数组中注册自己的三层协议处理函数。</p>

<p><img src="/images/kernel/20221127-11.png" alt="" /></p>

<h4>四层协议（TCP/UDP）</h4>

<p>利用nf_conntrack_proto.c文件中的
<code>
    nf_conntrack_l4proto_register(struct nf_conntrack_l4proto *l4proto)
    和
    nf_conntrack_l4proto_unregister(struct nf_conntrack_l4proto *l4proto)
</code>
在nf_ct_protos[]数组中注册自己的四层协议处理函数。</p>

<p><img src="/images/kernel/20221127-12.png" alt="" /></p>

<h4>处理一个连接的子连接协议</h4>

<p>利用nf_conntrack_helper.c文件中的
<code>
    nf_conntrack_helper_register(struct nf_conntrack_helper *me)
</code>
来注册nf_conntrack_helper结构，</p>

<p>和nf_conntrack_expect.c文件中的
<code>
    nf_ct_expect_related_report(struct nf_conntrack_expect *expect, u32 pid, int report)
</code>
来注册nf_conntrack_expect结构。</p>

<p><img src="/images/kernel/20221127-13.png" alt="" /></p>

<h4>扩展连接跟踪结构（nf_conn）</h4>

<p>利用nf_conntrack_extend.c文件中的
<code>
    nf_ct_extend_register(struct nf_ct_ext_type *type)
    和
    nf_ct_extend_unregister(struct nf_ct_ext_type *type)
</code>
进行扩展，并修改连接跟踪相应代码来利用这部分扩展功能。</p>

<p><img src="/images/kernel/20221127-14.png" alt="" /></p>

<p>了解了上面的数据结构，我们下面来看一下nf_conntrack的执行流程以及如何利用这些数据结构的。首先来看一下nf_conntrack模块加载时的初始化流程。</p>

<p><img src="/images/kernel/20221127-15.png" alt="" /></p>

<h4>nf_conntrack的初始化</h4>

<p>就是初始化上面提到的那些数据结构，它在内核启动时调用nf_conntrack_standalone_init()函数进行初始化的。初始化完成后，构建出如下图所示的结构图，只是不包含下图中与连接有关的信息（nf_conn和nf_conntrack_expect结构）。</p>

<p><img src="/images/kernel/20221127-16.png" alt="" /></p>

<p>上图中有三个HASH桶，ct_hash、expect_hash、helper_hash这三个HASH桶大小在初始化时就已确定，后面不能再更改。其中ct_hash、expect_hash可在加载nf_conntrack.ko模块时通过参数hashsize和expect_hashsize进行设定，而helper_hash不能通过参数修改，它的默认值是page/sizeof(helper_hash)。</p>

<p>下面再来看一个当创建子连接时，各个数据结构之间的关系。</p>

<p><img src="/images/kernel/20221127-17.png" alt="" /></p>

<p>nf_conn和nf_conntrack_expect都有最大个数限制。nf_conn通过全局变量nf_conntrack_max限制，可通过 /proc/sys/net/netfilter/nf_conntrack_max 文件在运行时修改。nf_conntrack_expect通过全局变量nf_ct_expect_max限制，可通过 /proc/sys/net/netfilter/nf_conntrack_expect_max 文件在运行时修改。nf_conntrack_helper没有最大数限制，因为这个是通过注册不同协议的模块添加的，大小取决于动态协议跟踪模块的多少，一般不会很大。</p>

<p>上面两幅数据结构图中，大部分都已介绍过，下面介绍一下netns_ct数据结构，该结构主要用于linux的网络命名空间，表示nf_conntrack在不同的命名空间中都有一套独立的数据信息（这是另一个话题，这里就不再深入讨论了）。</p>

<pre><code>    struct netns_ct {
        atomic_t            count;              /* 当前连接表中连接的个数 */
        unsigned int        expect_count;           /* nf_conntrack_helper创建的期待子连接nf_conntrack_expect项的个数 */
        unsigned int        htable_size;            /* 存储连接（nf_conn）的HASH桶的大小 */
        struct kmem_cache   *nf_conntrack_cachep;       /* 指向用于分配nf_conn结构而建立的高速缓存（slab）对象 */
        struct hlist_nulls_head *hash;              /* 指向存储连接（nf_conn）的HASH桶 */
        struct hlist_head       *expect_hash;           /* 指向存储期待子连接nf_conntrack_expect项的HASH桶 */
        struct hlist_nulls_head unconfirmed;            /* 对于一个链接的第一个包，在init_conntrack()函数中会将该包original方向的tuple结构挂入该链，这是因为在此时还不确定该链接会不会被后续的规则过滤掉，如果被过滤掉就没有必要挂入正式的链接跟踪表。在ipv4_confirm()函数中，会将unconfirmed链中的tuple拆掉，然后再将original方向和reply方向的tuple挂入到正式的链接跟踪表中，即init_net.ct.hash中，这是因为到达ipv4_confirm()函数时，应经在钩子NF_IP_POST_ROUTING处了，已经通过了前面的filter表。 通过cat  /proc/net/nf_conntrack显示连接，是不会显示该链中的连接的。但总的连接个数（net-&gt;ct.count）包含该链中的连接。当注销l3proto、l4proto、helper、nat等资源或在应用层删除所有连接（conntrack -F）时，除了释放confirmed连接（在net-&gt;ct.hash中的连接）的资源，还要释放unconfirmed连接（即在该链中的连接）的资源。*/
        struct hlist_nulls_head dying;              /* 释放连接时，通告DESTROY事件失败的ct被放入该链中，并设置定时器，等待下次通告。 通过cat  /proc/net/nf_conntrack显示连接，是不会显示该链中的连接的。但总的连接个数（net-&gt;ct.count）包含该链中的连接。当注销连接跟踪模块时，同时要清除正再等待被释放的连接（即该链中的连接）*/
        struct ip_conntrack_stat    __percpu *stat;         /* 连接跟踪过程中的一些状态统计，每个CPU一项，目的是为了减少锁 */
        int         sysctl_events;          /* 是否开启连接事件通告功能 */
        unsigned int        sysctl_events_retry_timeout;    /* 通告失败后，重试通告的间隔时间，单位是秒 */
        int         sysctl_acct;            /* 是否开启每个连接数据包统计功能 */
        int         sysctl_checksum;
        unsigned int        sysctl_log_invalid;      /* Log invalid packets */
    #ifdef CONFIG_SYSCTL
        struct ctl_table_header *sysctl_header;
        struct ctl_table_header *acct_sysctl_header;
        struct ctl_table_header *event_sysctl_header;
    #endif
        int         hash_vmalloc;           /* 存储连接（nf_conn）的HASH桶是否是使用vmalloc()进行分配的 */
        int         expect_vmalloc;         /* 存储期待子连接nf_conntrack_expect项的HASH桶是否是使用vmalloc()进行分配的 */
        char            *slabname;          /* 用于分配nf_conn结构而建立的高速缓存（slab）对象的名字 */
    };
</code></pre>

<p>从nf_conntrack的框架来看，它可用于跟踪任何三层和四协议的连接，但目前在三层协议只实现了IPv4和IPv6的连接跟踪，下面我们以IPv4为例，介绍一下该协议是如何利用nf_conntrack框架和netfilter实现连接跟踪的。有关netfilter框架，可参考我的另一个帖子</p>

<p><a href="http://bbs.chinaunix.net/forum.php?mod=viewthread&amp;tid=3749208&amp;fromuid=20171559">linux-2.6.35.6内核netfilter框架</a></p>

<p>首先介绍一下IPv4协议连接跟踪模块的初始化。</p>

<p>Ipv4连接跟踪模块注册了自己的3层协议，和IPv4相关的三个4层协议TCP、UDP、ICMP。注册后的结构图如下图所示：</p>

<p><img src="/images/kernel/20221127-18.png" alt="" /></p>

<p>在netfilter框架中利用
<code>
    nf_register_hook(struct nf_hook_ops *reg)、nf_unregister_hook(struct nf_hook_ops *reg)
</code>
函数注册自己的钩子项，调用nf_conntrack_in()函数来建立相应连接。</p>

<p><img src="/images/kernel/20221127-19.png" alt="" /></p>

<p>这样数据包就会经过ipv4注册的钩子项，并调用nf_conntrack_in()函数建立连接表项，连接表项中的tuple由ipv4注册的3/4层协议处理函数构建。</p>

<p>ipv4_conntrack_in() 挂载在NF_IP_PRE_ROUTING点上。该函数主要功能是创建链接，即创建struct nf_conn结构，同时填充struct nf_conn中的一些必要的信息，例如链接状态、引用计数、helper结构等。</p>

<p>ipv4_confirm() 挂载在NF_IP_POST_ROUTING和NF_IP_LOCAL_IN点上。该函数主要功能是确认一个链接。对于一个新链接，在ipv4_conntrack_in()函数中只是创建了struct nf_conn结构，但并没有将该结构挂载到链接跟踪的Hash表中，因为此时还不能确定该链接是否会被NF_IP_FORWARD点上的钩子函数过滤掉，所以将挂载到Hash表的工作放到了ipv4_confirm()函数中。同时，子链接的helper功能也是在该函数中实现的。</p>

<p>ipv4_conntrack_local() 挂载在NF_IP_LOCAL_OUT点上。该函数功能与ipv4_conntrack_in()函数基本相同，但其用来处理本机主动向外发起的链接。</p>

<p>nf_conntrack_ipv4_compat_init() &ndash;> register_pernet_subsys() &ndash;> ip_conntrack_net_init() 创建/proc文件ip_conntrack和ip_conntrack_expect</p>

<p>如上面所示，IPv4连接跟踪模块已初始化完成，下面我们来看一下它创建连接的流程图。上图中连接的建立主要由三个函数来完成，即ipv4_conntrack_in()，ipv4_confirm()与ipv4_conntrack_local()。其中ipv4_conntrack_in()与ipv4_conntrack_local()都是通过调用函数nf_conntrack_in()来实现的，所以下面我们主要关注nf_conntrack_in()与ipv4_confirm()这两个函数。nf_conntrack_in()函数主要完成创建链接、添加链接的扩展结构(例如helper, acct结构)、设置链接状态等。ipv4_confirm()函数主要负责确认链接(即将链接挂入到正式的链接表中)、执行helper函数、启动链接超时定时器等。另外还有一个定时器函数death_by_timeout(), 该函数负责链接到期时删除该链接。</p>

<p>nf_conntrack_in()函数流程图</p>

<p><img src="/images/kernel/20221127-20.png" alt="" /></p>

<p>ipv4_confirm()函数流程图</p>

<p><img src="/images/kernel/20221127-21.png" alt="" /></p>

<p>death_by_timeout()函数流程图</p>

<p><img src="/images/kernel/20221127-22.png" alt="" /></p>

<p>上图中有一点需要说明，由于skb会引用nf_conn，同时会增加它的引用计数，所以当skb被释放时，也要释放nf_conn的引用计数，并且在nf_conn引用计数为0时，要释放全部资源。</p>

<p>当数据包经过nf_conntrack_in()和ipv4_confirm()函数处理流程后，就会建立起3楼第二幅结构图所示的连接nf_conn。同时这两个函数已经包含了子连接的处理流程，即流程图中help和exp的处理。子连接建立后的结构图如3楼第三幅结构图，主链接与子连接通过helper和expect关联起来。</p>

<p>连接跟踪到此就介绍完了，下面介绍IPv4基于nf_conntrack框架适合实现NAT转换的。先介绍IPv4-NAT初始化的资源，然后处理流程。</p>

<h2>IPv4-NAT连接跟踪相关部分通过函数nf_nat_init()初始化</h2>

<p>调用nf_ct_extend_register() 注册一个连接跟踪的扩展功能。</p>

<p><img src="/images/kernel/20221127-23.png" alt="" /></p>

<p>调用register_pernet_subsys() &ndash;> nf_nat_net_init() 创建net->ipv4.nat_bysource的HASH表，大小等于net->ct.htable_size。</p>

<p>初始化nf_nat_protos[]数组，为TCP、UDP、ICMP协议指定专用处理结构，其它协议都指向默认处理结构。</p>

<p><img src="/images/kernel/20221127-24.png" alt="" /></p>

<p>为nf_conntrack_untracked连接设置IPS_NAT_DONE_MASK标志。</p>

<p>将NAT模块的全局变量l3proto指向IPV4协议的nf_conntrack_l3proto结构。</p>

<p>设置全局指针nf_nat_seq_adjust_hook指向nf_nat_seq_adjust()函数。</p>

<p>设置全局指针nfnetlink_parse_nat_setup_hook指向nfnetlink_parse_nat_setup()函数。</p>

<p>设置全局指针nf_ct_nat_offset指向nf_nat_get_offset()函数。</p>

<h2>IPv4-NAT功能的iptables部分通过函数nf_nat_standalone_init()初始化</h2>

<p>调用nf_nat_rule_init() &ndash;> nf_nat_rule_net_init()在iptables中注册一个NAT表（通过ipt_register_table()函数，参考另一个帖子iptables）</p>

<p>调用 nf_nat_rule_init() 注册SNAT target和DNAT target（通过xt_register_target()函数）</p>

<p><img src="/images/kernel/20221127-25.png" alt="" /></p>

<p>调用nf_register_hooks() 挂载NAT的HOOK函数，橙色部分为NAT挂载的HOOK函数（参考另一个帖子netfilter）</p>

<p><img src="/images/kernel/20221127-26.png" alt="" /></p>

<p>根据上面介绍，可以看到IPv4-NAT的主要是通过nf_nat_fn()钩子函数处理的，下面我就来看看nf_nat_fn()函数的处理流程。</p>

<p><img src="/images/kernel/20221127-27.png" alt="" /></p>

<p>针对上图中的nf_nat_setup_info()函数进一步描述</p>

<p><img src="/images/kernel/20221127-28.png" alt="" /></p>

<h2>下面对NAT转换算法中重要部分做一些文字说明</h2>

<p>每个ct在第一个包就会做好snat与dnat, nat的信息全放在reply tuple中，orig tuple不会被改变。一旦第一个包建立好nat信息后，后续再也不会修改tuple内容了。</p>

<p>orig tuple中的地址信息与reply tuple中的地址信息就是原始数据包的信息。例如对A->B数据包同时做snat与dnat，PREROUTING处B被dnat到D，POSTROUTING处A被snat到C。则ct的内容是:  A->B | D->C,  A->B说明了orig方向上数据包刚到达墙时的地址内容，D->C说明reply方向上数据包刚到达墙时的地址内容。</p>

<p>在代码中有很多!dir操作，原理是: 当为了反向的数据包做事情的时候就取反向tuple的数据，这样才能保证NAT后的tuple信息被正确使用。</p>

<p>bysource链中链接了所有CT（做过NAT和未做过NAT），通过ct->nat->bysource，HASH值的计算使用的是CT的orig tuple。其作用是，当为一个新连接做SNAT，需要得到地址映射时，首先对该链进行查找，查找此源IP、协议和端口号是否已经做过了映射。如果做过的话，就需要在SNAT转换时，映射为相同的源IP和端口号。为什么要这么做呢？因为对于UDP来说，有些协议可能会用相同端口和同一主机不同的端口（或不同的主机）进行通信。此时，由于目的地不同，原来已有的映射不可使用，需要一个新的连接。但为了保证通信的的正确性，此时，就要映射为相同的源IP和端口号。其实就是为NAT的打洞服务的。所以bysource就是以源IP、协议和端口号为hash值的一个表，这样在做snat时保证相同的ip+port影射到相同的ip+port。</p>

<p>IP_NAT_RANGE_PROTO_RANDOM指的是做nat时，当计算端口时，如果没有此random标志，则会先使用原始得tuple中的端口试一下看是否可用，如果可用就使用该原始端口作为nat后的端口， 即尽量保证转换后的端口与转换前的端口保持一致。如果不可用，再根据nat的端口算法计算出一个端口。 如果有此标记，则直接根据端口算法计算出端口。</p>

<p>第一个包之后，ct的两个方向的tuple内容就固定了，所有的nat操作都必须在第一个包就完成。所以会有daddr = &amp;ct->tuplehash[!dir].tuple.dst.u3;这样的操作。</p>

<p>IPS_SRC_NAT与IPS_DST_NAT，如果被设置，表示经过了NAT，并且ct中的tuple被做过SNAT或DNAT。</p>

<p>数据包永远都是在PREROUTING链做目的地址和目的端口转换，在POSTROUTING链做原地址和原端口转换。是否要做NAT转换则要根数据包方向（dir）和NAT标志（IPS_SRC_NAT或IPS_DST_NAT）来判断。</p>

<p>在PREROUTING链上&mdash;>数据包是original方向、并且连接上设置IPS_DST_NAT标志，或数据包是reply方向、并且连接上设置IPS_SRC_NAT标志，则做DNAT转换。</p>

<p>在POSTROUTING链上&mdash;>数据包是original方向、并且连接上设置IPS_SRC_NAT标志，或数据包是reply方向、并且连接上设置IPS_DST_NAT标志，则做SNAT转换。</p>

<p>IPS_DST_NAT_DONE_BIT与IPS_SRC_NAT_DONE_BIT，表示该ct进入过NAT模块，已经进行了源或者目的NAT判断，但并不表示ct中的tuple被修改过。</p>

<p>源目的nat都是在第一个包就判断完成的，假设先添加了snat策略，第一个包通过，这时又添加了dnat策略, 第二个包到来时是不会匹配dnat策略的 。</p>

<p>对于一个ct，nf_nat_setup_info函数最多只能进入2次，第一次DNAT，第二次SNAT。在nf_nat_follow_master函数中，第一次SNAT，第二次DNAT。</p>

<p>下面介绍有子连接的NAT实现。有两个关键点：1.主链接能正确的构建出NAT后的expect来识别子连接。2.能够修改主链接数据通道的信息为NAT后的信息。这两点都在动态协议的help中完成，下面我们来看一下它的流程图：</p>

<p><img src="/images/kernel/20221127-29.png" alt="" /></p>

<h2>下面针对有无子连接的NAT做一下对比</h2>

<h4>无子连接的NAT</h4>

<p>一个ct用于跟踪一个连接的双方向数据，ct->orig_tuple用于跟踪初始方向数据，ct->reply_tuple用于跟踪应答方向数据。当根据初始方向数据构建ct->orig_tuple时，同时要构建出ct->reply_tuple，用于识别同一连接上应答方向数据。</p>

<p>如果初始方向的数据在通过防火墙后被做了NAT转换，为识别出NAT数据的应答数据包，则对ct->reply_tuple也要做NAT转换。同时ct上做好相应NAT标记。</p>

<p>因此，上面的信息在初始方向第一个数据包通过后，就要求全部建立好，并且不再改变。</p>

<p>一个连接上不同方向的数据，都有相对应的tuple（orig_tuple和reply_tuple），所以该连接后续数据都将被识别出来。如果ct上有NAT标记，则根据要去往方向（即另一个方向）的tuple对数据做NAT转换。所以会有ct->tuplehash[!dir].tuple这样的操作。</p>

<h4>有子连接的NAT</h4>

<p>子连接是由主连接构建的expect项识别出来的。</p>

<p>help用于构建expect项，它期待哪个方向的连接，则用那个方向的tuple和数据包中数据通道信息构建expect项。例如期待和当前数据包相反方向的连接，则用相反方向的tuple中的信息（ct->tuplehash[!dir].tuple）。调用help时，NAT转换都已完成（tuple中都包含有正确的识别各自方向的信息），所以这时所使用的信息都是正确和所期望的信息。</p>

<p>如果子连接还可能有子连接，则构建expect项时，初始化一个helper结构，并赋值给expect->helper指针。</p>

<p>如果该连接已被做了NAT转换，则对数据包中数据通道信息也要做NAT转换</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SKB路由缓存与SOCK路由缓存]]></title>
    <link href="http://abcdxyzk.github.io/blog/2021/06/08/net-dst/"/>
    <updated>2021-06-08T22:04:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2021/06/08/net-dst</id>
    <content type="html"><![CDATA[<p><a href="https://www.2cto.com/kf/201805/745174.html">https://www.2cto.com/kf/201805/745174.html</a></p>

<p>skb结构体中的成员 <code>_skb_refdst</code> 与sock结构体中成员sk_rx_dst（缓存入口路由）和sk_dst_cache（缓存出口路由）成员之间的交互操作。</p>

<h3>SOCK入口路由与SKB路由缓存</h3>

<p>内核在接收流程中，调用early_demux函数提前在IP层做established状态的sock查找，并负责将sock结构体成员sk_rx_dst的路由缓存赋值给skb成员<code>_skb_refdst</code>，对于UDP协议，先判断DST_NOCACHE标志，如果成立，增加dst引用计数，设置skb的dst；否则，调用skb_dst_set_noref直接进行设置。
<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
<span class='line-number'>75</span>
<span class='line-number'>76</span>
<span class='line-number'>77</span>
<span class='line-number'>78</span>
<span class='line-number'>79</span>
<span class='line-number'>80</span>
<span class='line-number'>81</span>
<span class='line-number'>82</span>
<span class='line-number'>83</span>
<span class='line-number'>84</span>
<span class='line-number'>85</span>
<span class='line-number'>86</span>
<span class='line-number'>87</span>
<span class='line-number'>88</span>
<span class='line-number'>89</span>
<span class='line-number'>90</span>
<span class='line-number'>91</span>
<span class='line-number'>92</span>
<span class='line-number'>93</span>
<span class='line-number'>94</span>
<span class='line-number'>95</span>
<span class='line-number'>96</span>
<span class='line-number'>97</span>
<span class='line-number'>98</span>
<span class='line-number'>99</span>
<span class='line-number'>100</span>
<span class='line-number'>101</span>
<span class='line-number'>102</span>
<span class='line-number'>103</span>
<span class='line-number'>104</span>
<span class='line-number'>105</span>
<span class='line-number'>106</span>
<span class='line-number'>107</span>
<span class='line-number'>108</span>
<span class='line-number'>109</span>
<span class='line-number'>110</span>
<span class='line-number'>111</span>
<span class='line-number'>112</span>
<span class='line-number'>113</span>
<span class='line-number'>114</span>
<span class='line-number'>115</span>
<span class='line-number'>116</span>
<span class='line-number'>117</span>
<span class='line-number'>118</span>
<span class='line-number'>119</span>
<span class='line-number'>120</span>
<span class='line-number'>121</span>
<span class='line-number'>122</span>
<span class='line-number'>123</span>
<span class='line-number'>124</span>
<span class='line-number'>125</span>
<span class='line-number'>126</span>
<span class='line-number'>127</span>
<span class='line-number'>128</span>
<span class='line-number'>129</span>
<span class='line-number'>130</span>
<span class='line-number'>131</span>
<span class='line-number'>132</span>
<span class='line-number'>133</span>
<span class='line-number'>134</span>
<span class='line-number'>135</span>
<span class='line-number'>136</span>
<span class='line-number'>137</span>
<span class='line-number'>138</span>
<span class='line-number'>139</span>
<span class='line-number'>140</span>
<span class='line-number'>141</span>
<span class='line-number'>142</span>
<span class='line-number'>143</span>
<span class='line-number'>144</span>
<span class='line-number'>145</span>
<span class='line-number'>146</span>
<span class='line-number'>147</span>
<span class='line-number'>148</span>
<span class='line-number'>149</span>
<span class='line-number'>150</span>
<span class='line-number'>151</span>
<span class='line-number'>152</span>
<span class='line-number'>153</span>
<span class='line-number'>154</span>
<span class='line-number'>155</span>
<span class='line-number'>156</span>
<span class='line-number'>157</span>
<span class='line-number'>158</span>
<span class='line-number'>159</span>
<span class='line-number'>160</span>
<span class='line-number'>161</span>
<span class='line-number'>162</span>
<span class='line-number'>163</span>
<span class='line-number'>164</span>
<span class='line-number'>165</span>
<span class='line-number'>166</span>
<span class='line-number'>167</span>
<span class='line-number'>168</span>
<span class='line-number'>169</span>
<span class='line-number'>170</span>
<span class='line-number'>171</span>
<span class='line-number'>172</span>
<span class='line-number'>173</span>
<span class='line-number'>174</span>
<span class='line-number'>175</span>
<span class='line-number'>176</span>
<span class='line-number'>177</span>
<span class='line-number'>178</span>
<span class='line-number'>179</span>
<span class='line-number'>180</span>
<span class='line-number'>181</span>
<span class='line-number'>182</span>
<span class='line-number'>183</span>
<span class='line-number'>184</span>
<span class='line-number'>185</span>
<span class='line-number'>186</span>
<span class='line-number'>187</span>
<span class='line-number'>188</span>
<span class='line-number'>189</span>
<span class='line-number'>190</span>
<span class='line-number'>191</span>
<span class='line-number'>192</span>
<span class='line-number'>193</span>
<span class='line-number'>194</span>
<span class='line-number'>195</span>
<span class='line-number'>196</span>
<span class='line-number'>197</span>
<span class='line-number'>198</span>
<span class='line-number'>199</span>
<span class='line-number'>200</span>
<span class='line-number'>201</span>
<span class='line-number'>202</span>
<span class='line-number'>203</span>
<span class='line-number'>204</span>
<span class='line-number'>205</span>
<span class='line-number'>206</span>
<span class='line-number'>207</span>
<span class='line-number'>208</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>void udp_v4_early_demux(struct sk_buff *skb)
</span><span class='line'>{
</span><span class='line'>    dst = READ_ONCE(sk-&gt;sk_rx_dst);&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;    if (dst)
</span><span class='line'>    dst = dst_check(dst, 0);
</span><span class='line'>if (dst) {
</span><span class='line'>    /* DST_NOCACHE can not be used without taking a reference */
</span><span class='line'>    if (dst-&gt;flags &amp; DST_NOCACHE) {
</span><span class='line'>        if (likely(atomic_inc_not_zero(&amp;dst-&gt;__refcnt)))
</span><span class='line'>            skb_dst_set(skb, dst);
</span><span class='line'>    } else {
</span><span class='line'>        skb_dst_set_noref(skb, dst);
</span><span class='line'>    }
</span><span class='line'>}
</span><span class='line'>}
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;
</span><span class='line'>对于TCP协议，直接调用skb_dst_set_noref函数，将sock结构体成员sk_rx_dst缓存到skb结构体中。
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;void tcp_v4_early_demux(struct sk_buff *skb)
</span><span class='line'>{
</span><span class='line'>if (sk-&gt;sk_state != TCP_TIME_WAIT) {
</span><span class='line'>    struct dst_entry *dst = sk-&gt;sk_rx_dst;
</span><span class='line'>
</span><span class='line'>    if (dst)
</span><span class='line'>        dst = dst_check(dst, 0);
</span><span class='line'>    if (dst &amp;&amp;
</span><span class='line'>        inet_sk(sk)-&gt;rx_dst_ifindex == skb-&gt;skb_iif)
</span><span class='line'>        skb_dst_set_noref(skb, dst);
</span><span class='line'>}
</span><span class='line'>}
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;
</span><span class='line'>同样都为early_demux函数，都是从sk-&gt;sk_rx_dst获取路由缓存，tcp和udp的存在明显差别。TCP直接赋值，UDP需要先判断DST_NOCACHE标志。此情况是由UDP与TCP在sock中缓存dst时的处理不同造成的，TCP预先调用了dst_hold_safe函数，进行了DST_NOCACHE标志的判断处理，如未缓存则增加了引用计数。然而，UDP在缓存路由dst时，使用xchg函数，未判断也未增加引用计数，所以需要在后续判断处理。
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;static inline bool dst_hold_safe(struct dst_entry *dst)
</span><span class='line'>{
</span><span class='line'>if (dst-&gt;flags &amp; DST_NOCACHE)
</span><span class='line'>    return atomic_inc_not_zero(&amp;dst-&gt;__refcnt);
</span><span class='line'>dst_hold(dst);
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>void inet_sk_rx_dst_set(struct sock *sk, const struct sk_buff *skb)
</span><span class='line'>{
</span><span class='line'>struct dst_entry *dst = skb_dst(skb);
</span><span class='line'>
</span><span class='line'>if (dst &amp;&amp; dst_hold_safe(dst)) {
</span><span class='line'>    sk-&gt;sk_rx_dst = dst;
</span><span class='line'>    inet_sk(sk)-&gt;rx_dst_ifindex = skb-&gt;skb_iif;
</span><span class='line'>}
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>static void udp_sk_rx_dst_set(struct sock *sk, struct dst_entry *dst)
</span><span class='line'>{
</span><span class='line'>struct dst_entry *old;
</span><span class='line'>
</span><span class='line'>dst_hold(dst);
</span><span class='line'>old = xchg(&amp;sk-&gt;sk_rx_dst, dst);
</span><span class='line'>dst_release(old);
</span><span class='line'>}
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;
</span><span class='line'>### SOCK出口路由与SKB路由缓存
</span><span class='line'>
</span><span class='line'>对于UDP协议客户端，其在connect时（UDP客户端connect不同于TCP，仅绑定通信端地址），查询路由，缓存到sock结构体的sk_dst_cache中。
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;int ip4_datagram_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len)
</span><span class='line'>{
</span><span class='line'>rt = ip_route_connect(...);
</span><span class='line'>sk_dst_set(sk, &amp;rt-&gt;dst);
</span><span class='line'>}
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;
</span><span class='line'>之后，发送UDP数据包时，检查sock结构体中的出口路由是否有效，有效的话可不用再次查询路由表，在函数ip_make_skb中直接使用rt，并且调用skb_dst_set赋值给skb的`_skb_refdst`结构体，以便在发送过程中使用。
</span><span class='line'>
</span><span class='line'>对于UDP服务端，在首次发包检测到rt为空时，查询路由表得到出口路由，缓存在sock结构中，之后发包时rt有效，省去再次查询。
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;struct sk_buff *__ip_make_skb(...)
</span><span class='line'>{
</span><span class='line'>skb_dst_set(skb, &amp;rt-&gt;dst);
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>int udp_sendmsg(...)
</span><span class='line'>{
</span><span class='line'>if (connected)
</span><span class='line'>    rt = (struct rtable *)sk_dst_check(sk, 0);
</span><span class='line'>if (rt == NULL) {
</span><span class='line'>    rt = ip_route_output_flow(net, fl4, sk);
</span><span class='line'>    if (connected)
</span><span class='line'>        sk_dst_set(sk, dst_clone(&amp;rt-&gt;dst));
</span><span class='line'>}
</span><span class='line'>
</span><span class='line'>skb = ip_make_skb(sk, fl4, getfrag, msg-&gt;msg_iov, ulen,
</span><span class='line'>        sizeof(struct udphdr), &amp;ipc, &amp;rt,
</span><span class='line'>        msg-&gt;msg_flags);
</span><span class='line'>}
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;
</span><span class='line'>IP层发送数据包时(调用ip_queue_xmit），检测sock结构中出口路由缓存，如果有效，设置到skb结构体中。否则重新进行出口路由查找。
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;int ip_queue_xmit(struct sk_buff *skb, struct flowi *fl)
</span><span class='line'>{
</span><span class='line'>rt = (struct rtable *)__sk_dst_check(sk, 0);
</span><span class='line'>if (rt == NULL) {
</span><span class='line'>    rt = ip_route_output_ports(...);
</span><span class='line'>    sk_setup_caps(sk, &amp;rt-&gt;dst);
</span><span class='line'>}
</span><span class='line'>skb_dst_set_noref(skb, &amp;rt-&gt;dst);
</span><span class='line'>}
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;```&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;内核版本&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;linux-3.10.0&lt;/p&gt;
</span><span class='line'>]]&gt;&lt;/content&gt;
</span><span class='line'>  &lt;/entry&gt;
</span><span class='line'>  
</span><span class='line'>  &lt;entry&gt;
</span><span class='line'>&lt;title type="html"&gt;&lt;![CDATA[Linux网络栈之队列]]&gt;&lt;/title&gt;
</span><span class='line'>&lt;link href="http://abcdxyzk.github.io/blog/2021/06/08/net-queue/"/&gt;
</span><span class='line'>&lt;updated&gt;2021-06-08T16:55:00+08:00&lt;/updated&gt;
</span><span class='line'>&lt;id&gt;http://abcdxyzk.github.io/blog/2021/06/08/net-queue&lt;/id&gt;
</span><span class='line'>&lt;content type="html"&gt;&lt;![CDATA[&lt;p&gt;&lt;a href="https://zhensheng.im/2017/08/11/%e7%bf%bb%e8%af%91linux%e7%bd%91%e7%bb%9c%e6%a0%88%e4%b9%8b%e9%98%9f%e5%88%97.meow"&gt;https://zhensheng.im/2017/08/11/%e7%bf%bb%e8%af%91linux%e7%bd%91%e7%bb%9c%e6%a0%88%e4%b9%8b%e9%98%9f%e5%88%97.meow&lt;/a&gt;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;数据包队列是任何一个网络栈的核心组件，数据包队列实现了异步模块之间的通讯，提升了网络性能，并且拥有影响延迟的副作用。本文的目标，是解释Linux的网络栈中IP数据包在何处排队，新的延迟降低技术如BQL是多么的有趣，以及如何控制缓冲区以降低延迟。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;下面这张图片将会贯穿全文，其多个修改版本将会用来解释一些特别的概念。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;&lt;img src="/images/kernel/20210608-5.png" alt="" /&gt;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;图片一 – Simplified high level overview of the queues on the transmit path of the Linux 网络栈&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h2&gt;驱动队列(Driver Queue，又名环形缓冲）&lt;/h2&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;在内核的IP stack和网络接口控制器（NIC）之间，存在一个驱动队列。这个队列典型地以一个先进先出的环形缓冲区实现 —— 即一个固定大小的缓冲区。驱动队列并不附带数据包数据，而是持有指向内核中名为socket kernel buffers(SKBs)的结构体的描述符，SKBs持有数据包的数据并且在整个内核中使用。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;&lt;img src="/images/kernel/20210608-6.png" alt="" /&gt;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;图片 2 – Partially full driver queue with descriptors pointing to SKBs&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;驱动队列的输入来源是一个为所有数据包排队的IP stack，这些数据包可能是本地生成，或者在一个路由器上，由一个NIC接收然后选路从另一个NIC发出。数据包从IP stack入队到驱动队列后，将会被驱动程序执行出队操作，然后通过数据总线进行传输。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;驱动队列之所以存在，是为了保证系统无论在任何需要传输数据， NIC都能立即传输。换言之，驱动队列从硬件上给予了IP stack一个异步数据排队的地方。一个可选的方式是当NIC可以传输数据时，主动向IP stack索取数据，但这种设计模式下，无法实时对NIC响应，浪费了珍贵的传输机会，损失了网络吞吐量。另一个与此相反的方法是IP stack创建一个数据包后，需要同步等待NIC，直到NIC可以发送数据包，这也不是一个好的设计模式，因为在同步等待的过程中IP stack无法执行其它工作。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h2&gt;巨型数据包&lt;/h2&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;绝大多数的NIC都拥有一个固定的最大传输单元（MTU），意思是物理媒介可以传输的最大帧。以太网默认的MTU是1,500字节，但一些以太网络支持上限9,000字节的巨型帧(Jumbo Frames)。在IP 网络栈中，MTU描述了一个可被传输的数据包大小上限。例如，一个应用程序通过TCP socket发送了2,000字节的数据，IP stack就需要把这份数据拆分成数个数据包，以保持单个数据包的小于或等于MTU(1,500)。传输大量数据时，小的MTU将会产生更多分包。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;为了避免大量数据包排队，Linux内核实现了数个优化：TCP segmentation offload (TSO), UDP fragmentation offload (UFO) 和 generic segmentation offload (GSO)，这些优化机制允许IP stack创建大于出口NIC MTU的数据包。以IPv4为例，可以创建上限为65,536字节的数据包，并且可以入队到驱动队列。在TSO和UFO中，NIC在硬件上实现并负责拆分大数据包，以适合在物理链路上传输。对于没有TSO和UFO支持的NIC，GSO则在软件上实现同样的功能。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;前文提到，驱动队列只有固定容量，只能存放固定数量的描述符，由于TSO，UFO和GSO的特性，使得大型的数据包可以加入到驱动队列当中，从而间接地增加了队列的容量。图三与图二的比较，解释了这个概念。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;&lt;img src="/images/kernel/20210608-7.png" alt="" /&gt;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;图片 3 – Large packets can be sent to the NIC when TSO, UFO or GSO are enabled. This can greatly increase the number of bytes in the driver queue.&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;虽然本文的其余部分重点介绍传输路径，但值得注意的是Linux也有工作方式像TSO，UFO和GSO的接收端优化。这些优化的目标也是减少每一个数据包的开销。特别地，generic receive offload (GRO)允许NIC驱动把接收到的数据包组合成一个大型数据包，然后加入IP stack。在转发数据包的时候，为了维护端对端IP数据包的特性，GRO会重新组合接收到的数据包。然而，这只是单端效果，当大型数据包在转发方处拆分时，将会出现多个数据包一次性入队的情况，这种数据包"微型突发"会给网络延迟带来负面影响。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h2&gt;饥饿与延迟&lt;/h2&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;先不讨论必要性与优点，在IP stack和硬件之间的队列描述了两个问题：饥饿与延迟。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;如果NIC驱动程序要处理队列，此时队列为空，NIC将会失去一个传输数据的机会，导致系统的生产量降低。这种情况定义为饥饿。需要注意：当操作系统没有任何数据需要传输时，队列为空的话，并不归类为饥饿，而是正常。为了避免饥饿，IP stack在填充驱动队列的同时，NIC驱动程序也要进行出队操作。糟糕的是，队列填满或为空的事件持续的时间会随着系统和外部的情况而变化。例如，在一个繁忙的操作系统上，IP stack很少有机会往驱动队列中入队数据包，这样有很大的几率出现驱动队列为空的情况。拥有一个大容量的驱动队列缓冲区，有利于减少饥饿的几率，提高网络吞吐量。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;虽然一个大的队列有利于增加吞吐量，但缺点也很明显：提高了延迟。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;&lt;img src="/images/kernel/20210608-8.png" alt="" /&gt;&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;图片 4 – Interactive packet (yellow) behind bulk flow packets (blue)&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;图片4展示了驱动队列几乎被单个高流量（蓝色）的TCP段填满。队列中最后一个数据包来自VoIP或者游戏（黄色）。交互式应用，例如VoIP或游戏会在固定的间隔发送小数据包，占用大量带宽的数据传输会使用高数据包传输速率传输大量数据包，高速率的数据包传输将会在交互式数据包之间插入大量数据包，从而导致这些交互式数据包延迟传输。为了进一步解释这种情况，假设有如下场景：&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  一个网络接口拥有5 Mbit/sec(或5,000,000 bit/sec)的传输能力&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  每一个大流量的数据包都是1,500 bytes或12,000 bits。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  每一个交互式数据包都是500 bytes。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  驱动队列的长度为128。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  有127个大流量数据包，还有1个交互式数据包排在队列末尾。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;在上述情况下，发送127个大流量的数据包，需要(127 * 12,000) / 5,000,000 = 0.304 秒(以ping的方式来看，延迟值为304毫秒)。如此高的延迟，对于交互式程序来说是无法接受的，然而这还没计算往返时间。前文提到，通过TSO，UFO，GSO技术，大型数据包还可以在队列中排队，这将导致延迟问题更严重。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;大的延迟，一般由过大、疏于管理的缓冲区造成，如Bufferbloat。更多关于此现象的细节，可以查阅控制队列延迟（Controlling Queue Delay），以及Bufferbloat项目。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;如上所述，为驱动队列选择一个合适的容量是一个Goldilocks问题 – 这个值不能太小，否则损失吞吐量，也不能太大，否则过增延迟。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h2&gt;字节级队列限制（Byte Queue Limits (BQL)）&lt;/h2&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;Byte Queue Limits (BQL)是一个在Linux Kernel 3.3.0加入的新特性，以自动解决驱动队列容量问题。BQL通过添加一个协议，计算出的当前情况下避免饥饿的最小数据包缓冲区大小，以决定是否允许继续向驱动队列中入队数据包。根据前文，排队的数据包越少，数据包排队的最大发送延迟就越低。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;需要注意，驱动队列的容量并不能被BQL修改，BQL做的只是计算出一个限制值，表示当时有多少的数据可以被排队。任何超过此限制的数据，是等待还是被丢弃，会根据协议而定。&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;BQL机制在以下两种事件发生时将会触发：数据包入队，数据包传输完成。一个简化的BQL算法版本概括如下IMIT为BQL根据当前情况计算出的限制值。</span></code></pre></td></tr></table></div></figure>
    <em><em><strong>
    </strong> 数据包入驱动队列后
    </em></em>**</p>

<pre><code>如果队列排队数据包的总数据量超过当前限制值
禁止数据包入驱动队列
</code></pre>

<pre><code>
这里要清楚，被排队的数据量可以超过LIMIT，因为在TSO，UFO或GSO启用的情况下，一个大型的数据包可以通过单个操作入队，因此LIMIT检查在入队之后才发生，如果你很注重延迟，那么可能需要考虑关闭这些功能，本文后面将会提到如何实现这个需求。

BQL的第二个阶段在硬件完成数据传输后触发（pseudo-code简化版）：
</code></pre>

<pre><code>****
** 当硬件已经完成一批次数据包的发送
** (一个周期结束)
****

如果硬件在一个周期内处于饥饿状态
提高LIMIT

否则，如果硬件在一个周期内都没有进入饥饿状态，并且仍然有数据需要发送
使LIMIT减少"本周期内留下未发送的数据量"

如果驱动队列中排队的数据量小于LIMIT
允许数据包入驱动队列
</code></pre>

<pre><code>
如你所见，BQL是以测试设备是否被饥饿为基础实现的。如果设备被饥饿，LIMIT值将会增加，允许更多的数据排队，以减少饥饿，如果设备整个周期内都处于忙碌状态并且队列中仍然有数据需要传输，表明队列容量大于当前系统所需，LIMIT值将会降低，以避免延迟的提升。

BQL对数据排队的影响效果如何？一个真实世界的案例也许可以给你一个感觉。我的一个服务器的驱动队列大小为256个描述符，MTU 1,500字节，意味着最多能有256 * 1,500 = 384,000字节同时排队（TSO，GSO之类的已被关闭，否则这个值将会更高）。然而，由BQL计算的限制值是3,012字节。如你所见，BQL大大地限制了排队数据量。

BQL的一个有趣方面可以从它名字的第一个词思议——byte（字节）。不像驱动队列和大多数的队列容量，BQL直接操作字节，这是因为字节数与数据包数量相比，能更有效地影响数据传输的延迟。

BQL通过限制排队的数据量为避免饥饿的最小需求值以降低网络延迟。对于移动大量在入口NIC的驱动队列处排队的数据包到queueing discipline(QDisc)层，BQL起到了非常重要的影响。QDisc层实现了更复杂的排队策略，下一节介绍Linux QDisc层。

## 排队规则(Queuing Disciplines (QDisc))

驱动队列是一个很简单的先进先出（FIFO）队列，它平等对待所有数据包，没有区分不同流量数据包的功能。这样的设计优点是保持了驱动程序的简单以及高效。要注意更多高级的以太网络适配器以及绝大多数的无线网络适配器支持多种独立的传输队列，但同样的都是典型的FIFO。较高层的负责选择需要使用的传输队列。

在IP stack和驱动队列之间的是排队规则（queueing discipline(QDisc)）层（见图1）。这一层实现了内核的流量管理能力，如流量分类，优先级和速率调整。QDisc层通过一些不透明的tc命令进行配置。QDisc层有三个关键的概念需要理解：QDiscs，classes（类）和filters（过滤器）。

QDisc是Linux对流量队列的一个抽象化，比标准的FIFO队列要复杂得多。这个接口允许QDisc提供复杂的队列管理机制而无需修改IP stack或者NIC驱动。默认地，每一个网络接口都被分配了一个pfifo_fast QDisc，这是一个实现了简单的三频优先方案的队列，排序以数据包的TOS位为基础。尽管这是默认的，pfifo_fast QDisc离最佳选择还很远，因为它默认拥有一个很深的队列（见下文的txqueuelen）并且无法区分流量。

第二个与QDisc关系很密切的概念是类，独立的QDiscs为了以不同方式处理子集流量，可能实现类。例如，分层令牌桶（Hierarchical Token Bucket (HTB)）QDisc允许用户配置一个500 Kbps和300 Kbps的类，然后根据需要，把流量归为特定类。需要注意，并非所有QDiscs拥有对多个类的支持——那些被称为类的QDiscs。

过滤器（也被称为分类器），是一个用于流量分类到特定QDisc或类的机制。各种不同的过滤器复杂度不一，u32是一个最通用的也可能是一个最易用的流量过滤器。流量过滤器的文档比较缺乏，不过你可以在此找到使用例子：我的一个QoS脚本。

更多关于QDiscs，classes和filters的信息，可阅LARTC HOWTO，以及tc的man pages。

## 传输层与排队规则间的缓冲区

在前面的图片中，你可能会发现排队规则层并没有数据包队列。这意思是，网络栈直接放置数据包到排队规则中或者当队列已满时直接放回到更上层（例如socket缓冲区）。这很明显的一个问题是，如果接下来有大量数据需要发送，会发送什么？这种情况会在TCP链接发生大量堵塞或者甚至有些应用程序以其最快的速度发送UDP数据包时出现。对于一个持有单个队列的QDisc，与图4中驱动队列同样的问题将会发生，亦即单个大带宽或者高数据包传输速率流会把整个队列的空间消耗完毕，从而导致丢包，极大影响其它流的延迟。更糟糕的是，这产生了另一个缓冲点，其中可以形成standing queue，使得延迟增加并导致了TCP的RTT和拥塞窗口大小计算问题。Linux默认的pfifo_fast QDisc，由于大多数数据包TOS标记为0，因此基本可以视作单个队列，因此这种现象并不罕见。

Linux 3.6.0（2012-09-30），加入了一个新的特性，称为TCP小型队列，目标是解决上述问题。TCP小型队列限制了每个TCP流每次可在QDisc与驱动队列中排队的字节数。这有一个有趣的影响：内核会更早调度回应用程序，从而允许应用程序以更高效的优先级写入套接字。目前（2012-12-28），其它单个传输流仍然有可能淹没QDisc层。

另一个解决传输层洪水问题的方案是使用具有多个队列的QDisc，理想情况下每个网络流一个队列。随机公平队列（Stochastic Fairness Queueing (SFQ)）和延迟控制公平队列（Fair Queueing with Controlled Delay (fq_codel)）都有为每个网络流分配一个队列的机制，因此很适合解决这个洪水问题。

## 如何控制Linux的队列容量

#### 驱动队列

ethtool命令可用于控制以太网设备驱动队列容量。ethtool也提供了底层接口分析，可以启用或关闭IP stack和设备的一些特性。

-g参数可以输出驱动队列的信息：
</code></pre>

<pre><code>[root@alpha net-next]# ethtool -g eth0
Ring parameters for eth0:
Pre-set maximums:
RX:        16384
RX Mini:    0
RX Jumbo:    0
TX:        16384
Current hardware settings:
RX:        512
RX Mini:    0
RX Jumbo:    0
TX:        256
</code></pre>

<pre><code>
你可以从以上的输出看到本NIC的驱动程序默认拥有一个容量为256描述符的传输队列。早期，在Bufferbloat的探索中，这个队列的容量经常建议减少以降低延迟。随着BQL的使用（假设你的驱动程序支持它），再也没有任何必要去修改驱动队列的容量了（如何配置BQL见下文）。

Ethtool也允许你管理优化特性，例如TSO，UFO和GSO。-k参数输出当前的offload设置，-K修改它们。
</code></pre>

<pre><code>[dan@alpha ~]$ ethtool -k eth0
Offload parameters for eth0:
rx-checksumming: off
tx-checksumming: off
scatter-gather: off
tcp-segmentation-offload: off
udp-fragmentation-offload: off
generic-segmentation-offload: off
generic-receive-offload: on
large-receive-offload: off
rx-vlan-offload: off
tx-vlan-offload: off
ntuple-filters: off
receive-hashing: off
</code></pre>

<pre><code>
由于TSO，GSO，UFO和GRO极大的提高了驱动队列中可以排队的字节数，如果你想优化延迟而不是吞吐量，那么你应该关闭这些特性。如果禁用这些特性，除非系统正在处理非常高的数据速率，否则您将不会注意到任何CPU影响或吞吐量降低。

## Byte Queue Limits (BQL)

BQL是一个自适应算法，因此一般来说你不需要为此操心。然而，如果你想牺牲数据速率以换得最优延迟，你就需要修改LIMIT的上限值。BQL的状态和设置可以在/sys中NIC的目录找到，在我的服务器上，eth0的BQL目录是：
</code></pre>

<pre><code>/sys/devices/pci0000:00/0000:00:14.0/net/eth0/queues/tx-0/byte_queue_limits
</code></pre>

<pre><code>在该目录下的文件有：

  hold_time: 修改LIMIT值的时间间隔，单位为毫秒

  inflight: 还没发送且在排队的数据量

  limit: BQL计算的LIMIT值，如果NIC驱动不支持BQL，值为0

  limit_max: LIMIT的最大值，降低此值可以优化延迟

  limit_min: LIMIT的最小值，增高此值可以优化吞吐量

要修改LIMIT的上限值，把你需要的值写入limit_max文件即可，单位为字节：
</code></pre>

<pre><code>echo "3000" &gt; limit_max
</code></pre>

<pre><code>## 什么是txqueuelen？

在早期的Bufferbload讨论中，经常会提到静态地减少NIC传输队列长度。当前队列长度值可以通过ip和ifconfig命令取得。令人疑惑的是，这两个命令给了传输队列的长度不同的名字：
</code></pre>

<pre><code>[dan@alpha ~]$ ifconfig eth0
eth0      Link encap:Ethernet  HWaddr 00:18:F3:51:44:10
          inet addr:69.41.199.58  Bcast:69.41.199.63  Mask:255.255.255.248
          inet6 addr: fe80::218:f3ff:fe51:4410/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:435033 errors:0 dropped:0 overruns:0 frame:0
          TX packets:429919 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:65651219 (62.6 MiB)  TX bytes:132143593 (126.0 MiB)
          Interrupt:23
</code></pre>

<pre><code></code></pre>

<pre><code> [dan@alpha ~]$ ip link
1: lo:  mtu 16436 qdisc noqueue state UNKNOWN
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: eth0:  mtu 1500 qdisc pfifo_fast state UP qlen 1000
    link/ether 00:18:f3:51:44:10 brd ff:ff:ff:ff:ff:ff
</code></pre>

<pre><code>
Linux默认的传输队列长度为1,000个数据包，这是一个很大的缓冲区，尤其在低带宽的情况下。

有趣的问题是，这个变量实际上是控制什么？

我也不清楚，因此我花了点时间深入探索内核源码。我现在能说的，txqueuelen只是用来作为一些排队规则的默认队列长度。例如：
</code></pre>

<pre><code>pfifo_fast（Linux默认排队规则）
sch_fifo
sch_gred
sch_htb（只有默认队列）
sch_plug
sch_sfb
sch_teql
</code></pre>

<pre><code>
见图1，txqueuelen参数在排队规则中控制以上列出的队列类型的长度。绝大多数这些排队规则，tc的limit参数默认会覆盖掉txqueuelen。总的来说，如果你不是使用上述的排队规则，或者如果你用limit参数指定了队列长度，那么txqueuelen值就没有任何作用。

顺便一提，我发现一个令人疑惑的地方，ifconfig命令显示了网络接口的底层信息，例如MAC地址，但是txqueuelen却是来自高层的QDisc层，很自然的地，看起来ifconfig会输出驱动队列长度。

传输队列的长度可以使用ip或ifconfig命令修改：
</code></pre>

<pre><code>[root@alpha dan]# ip link set txqueuelen 500 dev eth0
</code></pre>

<pre><code>需要注意，ip命令使用"txqueuelen"但是输出时使用"qlen" —— 另一个不幸的不一致性。

## 排队规则

正如前文所描述，Linux内核拥有大量的排队规则（QDiscs），每一个都实现了自己的数据包排队方法。讨论如何配置每一个QDiscs已经超出了本文的范围。关于配置这些队列的信息，可以查阅tc的man page（man tc）。你可以使用"man tc qdisc-name"（例如："man tc htb"或"man tc fq_codel"）找到每一个队列的细节。LARTC也是一个很有用的资源，但是缺乏了一些新特性的信息。


以下是一些可能对你使用tc命令有用的建议和技巧：

  HTB QDisc实现了一个接收所有未分类数据包的默认队列。一些如DRR QDiscs会直接把未分类的数据包丢进黑洞。使用命令"tc qdisc show"，通过direct_packets_stat可以检查有多少数据包未被合适分类。

  HTB类分层只适用于分类，对于带宽分配无效。所有带宽分配通过检查Leaves和它们的优先级进行。

  QDisc中，使用一个major和一个minor数字作为QDiscs和classes的基本标识，major和minor之间使用英文冒号分隔。tc命令使用十六进制代表这些数字。由于很多字符串，例如10，在十进制和十六进制都是正确的，因此很多用户不知道tc使用十六进制。见我的tc脚本，可以查看我是如何处理这个问题的。

  如果你正在使用基于ATM的ADSL（绝大多数的DLS服务是基于ATM，新的变体例如VDSL2可能不是），你很可能需要考虑添加一个"linklayer adsl"的选项。这个统计把IP数据包分解成一组53字节的ATM单元所产生的开销。

  如果你正在使用PPPoE，你很可能需要考虑通过"overhead"参数统计PPPoE开销。

#### TCP小型队列

每个TCP Socket的队列限制可以通过/proc中的文件查看或修改：
</code></pre>

<pre><code>/proc/sys/net/ipv4/tcp_limit_output_bytes
</code></pre>

<p>```</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[网桥源码]]></title>
    <link href="http://abcdxyzk.github.io/blog/2020/11/09/kernel-bridge/"/>
    <updated>2020-11-09T17:52:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2020/11/09/kernel-bridge</id>
    <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/NW_NW_NW/article/details/75045966">https://blog.csdn.net/NW_NW_NW/article/details/75045966</a></p>

<p><a href="https://blog.csdn.net/NW_NW_NW/article/details/75090101">https://blog.csdn.net/NW_NW_NW/article/details/75090101</a></p>

<p><a href="https://blog.csdn.net/NW_NW_NW/article/details/75647220">https://blog.csdn.net/NW_NW_NW/article/details/75647220</a></p>

<p><a href="https://blog.csdn.net/NW_NW_NW/article/details/76022117">https://blog.csdn.net/NW_NW_NW/article/details/76022117</a></p>

<p><a href="https://blog.csdn.net/NW_NW_NW/article/details/76153027">https://blog.csdn.net/NW_NW_NW/article/details/76153027</a></p>

<p><a href="https://blog.csdn.net/NW_NW_NW/article/details/76204707">https://blog.csdn.net/NW_NW_NW/article/details/76204707</a></p>

<p><a href="https://blog.csdn.net/NW_NW_NW/article/details/76674232">https://blog.csdn.net/NW_NW_NW/article/details/76674232</a></p>

<p><a href="https://blog.csdn.net/NW_NW_NW/article/details/76710941">https://blog.csdn.net/NW_NW_NW/article/details/76710941</a></p>

<h2>桥初始化</h2>

<pre><code>    static int __init br_init(void)
    {
        int err;
        /*注册协议生成树收包函数*/
        err = stp_proto_register(&amp;br_stp_proto);
        if (err &lt; 0) {
            pr_err("bridge: can't register sap for STP\n");
            return err;
        }
        /*转发数据库初始化*/
        err = br_fdb_init();
        if (err)
            goto err_out;
        /*在/proc目录下生成任何与bridge相关的目录，如果我们想在/proc下生成bridge相关的子目录或子文件*/
        err = register_pernet_subsys(&amp;br_net_ops);
        if (err)
            goto err_out1;
        /*目前好像没有什么实际作用，在内核中所注册的函数为空*/
        err = br_nf_core_init();
        if (err)
            goto err_out2;
        /*注册相关网络设备的事件通知连*/
        err = register_netdevice_notifier(&amp;br_device_notifier);
        if (err)
            goto err_out3;

        /*注册通知连，主要针对桥转发表事件的相关信息*/
        err = register_switchdev_notifier(&amp;br_switchdev_notifier);
        if (err)
            goto err_out4;
        /*进行netlink的初始化*/
        err = br_netlink_init();
        if (err)
            goto err_out5;

        /*用来处理ioctl命令的函数，比如添加和删除网桥*/
        brioctl_set(br_ioctl_deviceless_stub);

    #if IS_ENABLED(CONFIG_ATM_LANE)
        br_fdb_test_addr_hook = br_fdb_test_addr;
    #endif
        return 0;
    }
</code></pre>

<p> 了解了桥初始化大致要做的事情后，我们再来看看这些初始化或者注册的事情到底干了些什么？</p>

<p>1.注册协议生成树收包函数stp_proto_register</p>

<p>在桥初始化的时候，注册了一个br_stp_proto参数，此参数的具体模样是这样子的</p>

<pre><code>    static const struct stp_proto br_stp_proto = {
        .rcv   = br_stp_rcv,
    };
</code></pre>

<p>br_stp_rcv函数在/net/bridge/br_stp_bpdu.c中主要针对网桥进行协议交换的帧（BPDU）进行配置操作。</p>

<p>2.桥转发数据库初始化br_fdb_init</p>

<p>此函数就是在内存中建立一块slab cache，以存放net_bridge_fdb_entry</p>

<p>其中：net_bridge_fdb_entry是一个结构体，用来转发数据库的记录项网桥所学到对的每个MAC地址都有这样一个记录</p>

<p>3.在proc目录下生成相关文件的注册函数register_pernet_subsys，初始化的时候给这个函数传递了一个参数br_net_ops，这个参数的模样是这样的</p>

<pre><code>    static struct pernet_operations br_net_ops = {
        .exit   = br_net_exit,
    };
</code></pre>

<p>但是在桥初始化的时候，仅仅注册了br_net_exit，这个函数会将桥下面的所有文件全部清空。</p>

<p>4.通知链的相关函数注册register_netdevice_notifier这个注册函数主要针对设备信息的变化，注册参数br_device_notifier，具体如下：</p>

<pre><code>    static struct notifier_block br_device_notifier = {
        .notifier_call = br_device_event
    };
</code></pre>

<p>br_device_event函数是用来当桥上的设备状态或者设备信息发生改变时做相应的处理，该函数在/net/bridge/br.c中</p>

<p>5.注册通知连，主要针对桥转发表事件的相关信息register_switchdev_notifier，传入的参数br_switchdev_notifier详细信息如下：</p>

<pre><code>    static struct notifier_block br_switchdev_notifier = {
        .notifier_call = br_switchdev_event,
    };
</code></pre>

<p>br_switchdev_event，主要针对桥转发表的事件做出相应的处理该函数在/net/bridge/br.c中</p>

<p>6.brioctl_set用来处理ioctl命令的函数，比如添加和删除网桥，br_ioctl_deviceless_stub给回调函数br_ioctl_hook,而br_ioctl_hook在sock_ioctl中</p>

<p>使用，这样通过在应用层调用socket的ioctl函数，就能够进行网桥的添加与删除了，函数用来处理添加和删除网桥的相关操作</p>

<p>以上就是网桥初始化的相关操作。</p>

<h2>添加一个桥设备——br_add_bridge</h2>

<p>我们先来看一个命令：brctl addbr br1</p>

<p>上节我们提到一个用来处理ioctl命令的函数br_ioctl_deviceless_stub通过调用brioctl_set，
将br_ioctl_deviceless_stub赋值给回调函数br_ioctl_hook，而br_ioctl_hook在sock_ioctl中使用。
这样通过在应用层调用socket的ioctl函数，就能够进行网桥的添加与删除了。</p>

<p>如果我们想增加新的ioctl，用于我们新开放的功能，就可以在该函数里增加新的case即可。</p>

<p>当我们输入上面命令时，就会触发br_ioctl_deviceless_stub函数来响应br_add_bridge函数，当命令执行完成以后，
使用brctl show命令就可以看见我们添加的br1这个网桥设备已经生成。</p>

<pre><code>    int br_ioctl_deviceless_stub(struct net *net, unsigned int cmd, void __user *uarg)
    {
        switch (cmd) {
        case SIOCGIFBR:
        case SIOCSIFBR:
            return old_deviceless(net, uarg);

        case SIOCBRADDBR:
        case SIOCBRDELBR:
        {
            char buf[IFNAMSIZ];

            if (!ns_capable(net-&gt;user_ns, CAP_NET_ADMIN))
                return -EPERM;

            if (copy_from_user(buf, uarg, IFNAMSIZ))
                return -EFAULT;

            buf[IFNAMSIZ-1] = 0;
            if (cmd == SIOCBRADDBR)
                return br_add_bridge(net, buf);/*添加网桥设备的操作*/

            return br_del_bridge(net, buf);
        }
        }
        return -EOPNOTSUPP;
    }
</code></pre>

<p>该函数注册在，桥设备添加时候dev->netdev_ops = &amp;br_netdev_ops;
 在br_netdev_ops有一个函数指针.ndo_do_ioctl= br_dev_ioctl</p>

<pre><code>    int br_add_if(struct net_bridge *br, struct net_device *dev)
    {
        struct net_bridge_port *p;
        int err = 0;
        unsigned br_hr, dev_hr;
        bool changed_addr;

        /* Don't allow bridging non-ethernet like devices, or DSA-enabled
         * master network devices since the bridge layer rx_handler prevents
         * the DSA fake ethertype handler to be invoked, so we do not strip off
         * the DSA switch tag protocol header and the bridge layer just return
         * RX_HANDLER_CONSUMED, stopping RX processing for these frames.
         */

        if ((dev-&gt;flags &amp; IFF_LOOPBACK) ||
            dev-&gt;type != ARPHRD_ETHER || dev-&gt;addr_len != ETH_ALEN ||
            !is_valid_ether_addr(dev-&gt;dev_addr) ||
            netdev_uses_dsa(dev))
            return -EINVAL;

        /* No bridging of bridges */
        if (dev-&gt;netdev_ops-&gt;ndo_start_xmit == br_dev_xmit)
            return -ELOOP;

        /* Device is already being bridged  */
        if (br_port_exists(dev))
            return -EBUSY;

        /* No bridging devices that dislike that (e.g. wireless) */
        if (dev-&gt;priv_flags &amp; IFF_DONT_BRIDGE)
            return -EOPNOTSUPP;

        /*分配一个新网桥端口并对其初始化*/
        p = new_nbp(br, dev);
        if (IS_ERR(p))
            return PTR_ERR(p);

        /*调用设备通知链，告诉网络有这样一个设备*/
        call_netdevice_notifiers(NETDEV_JOIN, dev);

        /**向设备添加或删除所有多播帧的接收。*/
        err = dev_set_allmulti(dev, 1);
        if (err)
            goto put_back;

        err = kobject_init_and_add(&amp;p-&gt;kobj, &amp;brport_ktype, &amp;(dev-&gt;dev.kobj),
                       SYSFS_BRIDGE_PORT_ATTR);
        if (err)
            goto err1;
        /*把链路添加到sysfs*/
        err = br_sysfs_addif(p);
        if (err)
            goto err2;

        err = br_netpoll_enable(p);
        if (err)
            goto err3;

        /*注册设备接收帧函数*/
        err = netdev_rx_handler_register(dev, br_handle_frame, p);
        if (err)
            goto err4;

        /*给该端口指派默认优先权*/
        dev-&gt;priv_flags |= IFF_BRIDGE_PORT;

        /*向上级设备添加主链路*/
        err = netdev_master_upper_dev_link(dev, br-&gt;dev, NULL, NULL);
        if (err)
            goto err5;

        /*禁用网络设备上的大型接收卸载（LRO）。
        必须在RTNL下调用。
        如果接收到的数据包可能转发到另一个接口，
        则需要这样做。*/
        dev_disable_lro(dev);

        list_add_rcu(&amp;p-&gt;list, &amp;br-&gt;port_list);
        /*更新桥上的端口数,如果有更新，再进一步将其设为混杂模式*/
        nbp_update_port_count(br);
        /* 重新计算dev-&gt;features并发送通知（如果已更改）。
        应该调用驱动程序或硬件依赖条件可能会改变影响功能。*/
        netdev_update_features(br-&gt;dev);

        br_hr = br-&gt;dev-&gt;needed_headroom;
        dev_hr = netdev_get_fwd_headroom(dev);
        if (br_hr &lt; dev_hr)
            update_headroom(br, dev_hr);
        else
            netdev_set_rx_headroom(dev, br_hr);

        /*把dev的mac添加到转发数据库中*/
        if (br_fdb_insert(br, p, dev-&gt;dev_addr, 0))
            netdev_err(dev, "failed insert local address bridge forwarding table\n");

        /*初始化该桥端口的vlan*/
        err = nbp_vlan_init(p);
        if (err) {
            netdev_err(dev, "failed to initialize vlan filtering on this port\n");
            goto err6;
        }

        spin_lock_bh(&amp;br-&gt;lock);
        /*更新网桥id*/
        changed_addr = br_stp_recalculate_bridge_id(br);
        /*设备是否启动，桥是否启动，设备上是否有载波信号(网桥没有载波状态，因为网桥是虚拟设备)*/
        if (netif_running(dev) &amp;&amp; netif_oper_up(dev) &amp;&amp;
            (br-&gt;dev-&gt;flags &amp; IFF_UP))
            /*启动网桥端口*/
            br_stp_enable_port(p);
        spin_unlock_bh(&amp;br-&gt;lock);

        br_ifinfo_notify(RTM_NEWLINK, p);
        /*如果网桥的地址改变，则调用通知连相关的函数*/
        if (changed_addr)
            call_netdevice_notifiers(NETDEV_CHANGEADDR, br-&gt;dev);
        /*更新网桥mtu*/
        dev_set_mtu(br-&gt;dev, br_min_mtu(br));
        br_set_gso_limits(br);
        /*添加一个内核对象*/
        kobject_uevent(&amp;p-&gt;kobj, KOBJ_ADD);

        return 0;

    err6:
        list_del_rcu(&amp;p-&gt;list);
        br_fdb_delete_by_port(br, p, 0, 1);
        nbp_update_port_count(br);
        netdev_upper_dev_unlink(dev, br-&gt;dev);

    err5:
        dev-&gt;priv_flags &amp;= ~IFF_BRIDGE_PORT;
        netdev_rx_handler_unregister(dev);
    err4:
        br_netpoll_disable(p);
    err3:
        sysfs_remove_link(br-&gt;ifobj, p-&gt;dev-&gt;name);
    err2:
        kobject_put(&amp;p-&gt;kobj);
        p = NULL; /* kobject_put frees */
    err1:
        dev_set_allmulti(dev, -1);
    put_back:
        dev_put(dev);
        kfree(p);
        return err;
    }
</code></pre>

<p>在桥上添加接口的基本步骤，如上，删除桥端口，主要是把建立接口时所做的事情撤销，如添加接口出错时的一些处理。</p>

<h2>桥设备及端口的开启和关闭</h2>

<p>关于设备的添加删除的基本动作，我们已经知道。
这节，我们看看关于网桥设备以及桥设备上的端口的启动和关闭。</p>

<p>我们说过，在初始化一个桥设备的时候有这样一个操作：
dev->netdev_ops = &amp;br_netdev_ops；</p>

<p>br_netdev_ops这个参数，注册了很多函数，其中包括网桥设备的启动和关闭函数</p>

<p>br_dev_open和br_dev_stop,这两个函数的工作主要是初始化桥设备的一些队列和
桥设备上端口的一些启动和关闭动作。</p>

<h4>启动和关闭网桥设备</h4>

<pre><code>    static int br_dev_open(struct net_device *dev)
    {
        struct net_bridge *br = netdev_priv(dev);
        /*重新更新网桥设备功能*/
        netdev_update_features(dev);
        /*函数启动进行设备传输*/
        netif_start_queue(dev);
        /*启动网桥设备*/
        br_stp_enable_bridge(br);
        /*初始化网桥本身的多播对列*/
        br_multicast_open(br);
        return 0;
    }
</code></pre>

<pre><code>    static int br_dev_stop(struct net_device *dev)
    {
        struct net_bridge *br = netdev_priv(dev);
        /*关闭网桥设备*/
        br_stp_disable_bridge(br);
        /*关闭网桥设备的多播队列*/
        br_multicast_stop(br);
        /*关闭设备的传输，任何企图在设备上传输信息的尝试都会被拒绝*/
        netif_stop_queue(dev);

        return 0;
    }
</code></pre>

<p>启动网桥设备，当启动网桥设备时，先前绑定在该设备上的端口也会跟着启动</p>

<pre><code>    void br_stp_enable_bridge(struct net_bridge *br)
    {
        struct net_bridge_port *p;
        /*锁定网桥*/
        spin_lock_bh(&amp;br-&gt;lock);
        if (br-&gt;stp_enabled == BR_KERNEL_STP)
            mod_timer(&amp;br-&gt;hello_timer, jiffies + br-&gt;hello_time);

        /* 当网桥启动时，设置次定时器，1/10秒到期一次 */
        mod_timer(&amp;br-&gt;gc_timer, jiffies + HZ/10);
        /*TX配置bpdu*/
        br_config_bpdu_generation(br);

        list_for_each_entry(p, &amp;br-&gt;port_list, list) {
            if (netif_running(p-&gt;dev) &amp;&amp; netif_oper_up(p-&gt;dev))
                br_stp_enable_port(p);/*启动网桥设备的每个端口*/
        }
        /*给网桥解锁*/
        spin_unlock_bh(&amp;br-&gt;lock);
    }
</code></pre>

<p>关闭网桥设备</p>

<pre><code>    void br_stp_disable_bridge(struct net_bridge *br)
    {
        struct net_bridge_port *p;

        spin_lock_bh(&amp;br-&gt;lock);
        list_for_each_entry(p, &amp;br-&gt;port_list, list) {
            if (p-&gt;state != BR_STATE_DISABLED)
                br_stp_disable_port(p);/*关闭网桥设备的每个端口*/
        }
        /*重新设置拓扑标识*/
        br-&gt;topology_change = 0;
        br-&gt;topology_change_detected = 0;
        spin_unlock_bh(&amp;br-&gt;lock);

        /*删除在初始化桥设备时的定时器*/
        del_timer_sync(&amp;br-&gt;hello_timer);
        del_timer_sync(&amp;br-&gt;topology_change_timer);
        del_timer_sync(&amp;br-&gt;tcn_timer);
        del_timer_sync(&amp;br-&gt;gc_timer);
    }
</code></pre>

<h3>启动和关闭网桥端口</h3>

<p>要启动网桥端口，必须满足下列几个条件</p>

<p>1.被管理的相关设备已用管理手段启动</p>

<p>2.被绑定的相关设备有载波状态</p>

<p>3.相关的网桥设备已用管理手段启动</p>

<p>注意：网桥设备上没有载波状态，因为网桥是虚拟设备。</p>

<p>当网桥是以用户空间命令建起来并且先前三个条件都满足时，该网桥端口就可以立即启用了</p>

<p>但是，假设当端口建立时，由于上述三项条件至少有一项不满足无法启动端口时，下面的条件是
每项条件最终满足时启用端口的场合：</p>

<p>1.当被关闭的网桥设备重新启动时，其所有关闭的端口就会启用</p>

<p>2.当被绑定的设备检测到载波状态时，桥程序会收到NETDE_CHANGE通知消息</p>

<p>3.当被关掉的版定设备重启时，桥程序会收到NETDEV_UP的通知消息</p>

<p>如若还不满足，网桥端口就会被关闭</p>

<h4>启动网桥上的端口</h4>

<pre><code>    void br_stp_enable_port(struct net_bridge_port *p)
    {
        /*初始化端口*/
        br_init_port(p);
        /*遍历所有端口，为端口指定合适的状态*/
        br_port_state_selection(p-&gt;br);
        /*捕捉一个端口变化信息的通知*/
        br_ifinfo_notify(RTM_NEWLINK, p);
    }
</code></pre>

<h4>关闭网桥上的端口</h4>

<pre><code>    void br_stp_disable_port(struct net_bridge_port *p)
    {
        struct net_bridge *br = p-&gt;br;
        int wasroot;
        /*判断是否是根网桥*/
        wasroot = br_is_root_bridge(br);
        /*分配制定角色*/
        br_become_designated_port(p);
        /*将关闭位置位*/
        br_set_state(p, BR_STATE_DISABLED);
        p-&gt;topology_change_ack = 0;
        p-&gt;config_pending = 0;

        br_ifinfo_notify(RTM_NEWLINK, p);
        /*删除定时器*/
        del_timer(&amp;p-&gt;message_age_timer);
        del_timer(&amp;p-&gt;forward_delay_timer);
        del_timer(&amp;p-&gt;hold_timer);
        /*更改转发表信息*/
        br_fdb_delete_by_port(br, p, 0, 0);
        br_multicast_disable_port(p);
        /*更改桥的bpdu信息*/
        br_configuration_update(br);
        /*更新所有桥上端口的状态*/
        br_port_state_selection(br);
        /*处理非根网桥到根网桥的转移*/
        if (br_is_root_bridge(br) &amp;&amp; !wasroot)
            br_become_root_bridge(br);
    }
</code></pre>

<p>注意，当网桥端口关闭时，非根网桥可能会变成根网桥</p>

<h2>skb桥转发蓝图</h2>

<p><img src="/images/kernel/20201109-10.png" alt="" /></p>

<p>需要说明的是：</p>

<p>1.我们先暂时忽略数据包从一开始是怎么从驱动进入到netif_receive_skb的，因为这个暂时不影响我们理解这幅图的流程。</p>

<p>2.由于桥转发的篇幅较大，图中没有标示出，数据包中途被丢弃的情况。约定数据包会发送成功。</p>

<p>现在数据包(skb)已经准备好了装备要闯关了</p>

<p>1.首先skb从驱动经过繁杂的路线走到了netif_receive_skb这个函数中经过一些小波折到达<code>__netif_receive_skb_core</code>中时遇到了第一个十字路口是看了看自己有没有skb->dev->rx_handler(注1)这个装备，如果有，则进入br_handle_frame(注2).如果没有则直接上协议栈。</p>

<p>注1：桥下的设备注册过rx_handler函数，所以数据包会进入桥，br_pass_frame_up函数将原先的设备换成了桥设备， 而桥设备没有注册过rx_handler函数，所以数据包不会二次进入桥。</p>

<p>注2：br_handle_frame我们在前几节提到过，是skb进入桥的入口函数，在br_add_if的时候会注册该函数。</p>

<p>2.skb注定要经历一番劫难，刚进入br_handle_frame又将陷入两难境地，此时有两个入口，这两个是netfilter设下的连个hook点，分别是，NF_BR_PRE_ROUTING，和NF_BR_LOCAL_IN，两种路径，如果数据包只想去本地，则会选择NF_BR_LOCAL_IN入口，然后发往本地，如果暂时还确定不了，则先进入NF_BR_PRE_ROUTING入口.</p>

<p>3.进入NF_BR_PRE_ROUTING入口后，会遇到br_handle_frame_finish函数，这个函数决定了skb的何去何从，(1)如果是转发，则在经过NF_BR_FORWARD钩子点进入转发阶段的障碍，最后在进入NF_BR_POST_ROUTING，以及最终的dev_queue_xmit，实现最终转发。(2)如果发往本地则重新进入NF_BR_LOCAL_IN最后在进入netif_receive_skb，进行转发。skb在经过目前口述的磨练最终得以释放。</p>

<p>4.如果是如果是本地发出的数据包，经过NF_BR_LOCAL_OUT钩子点然后进入最后阶段的NF_BR_POST_ROUTING，进行相应的转发。</p>

<h2>桥数据包接收&mdash;-br_handle_frame</h2>

<p>一个很重要的函数br_handle_frame这个函数的初始注册地点是在桥添加接口的时候，注册在桥某一个接口上</p>

<pre><code>    int br_add_if(struct net_bridge *br, struct net_device *dev)
    {
        ........
        /*注册设备接收帧函数*/
        err = netdev_rx_handler_register(dev, br_handle_frame, p);
        ........
    }
</code></pre>

<p>其次，那么<code>__netif_receive_skb_core</code>是怎样让数据包进入桥的呢？我们看看上面提到的netdev_rx_handler_register函数，具体做了什么
```
    int netdev_rx_handler_register(struct net_device <em>dev,
                       rx_handler_func_t </em>rx_handler,
                       void *rx_handler_data)
    {
        ASSERT_RTNL();</p>

<pre><code>    if (dev-&gt;rx_handler)
        return -EBUSY;
    /* Note: rx_handler_data must be set before rx_handler */
    /*将dev-&gt;rx_handler_data，指向rx_handler_data(上面的p是桥端口信息)*/
    rcu_assign_pointer(dev-&gt;rx_handler_data, rx_handler_data);
    /*将dev-&gt;rx_handle指针指向rx_handler*/
    rcu_assign_pointer(dev-&gt;rx_handler, rx_handler);

    return 0;
}
</code></pre>

<pre><code>
看完这个函数，我们就明白了为什么在`__netif_receive_skb_core`中可以用skb-&gt;dev-&gt;rx_handle将数据包传入br_handle_frame函数，也就是将数据包传入了桥。

值得注意的是：上面的dev是桥下的设备，不是桥设备，桥设备(比如br0)是没有注册rx_handle这个函数的


好，了解到，桥的注册函数和如何接收数据包后，然后一起来看看br_handle_frame是如何操作的
</code></pre>

<pre><code>/*
 * Return NULL if skb is handled
 * note: already called with rcu_read_lock
 */
rx_handler_result_t br_handle_frame(struct sk_buff **pskb)
{
    struct net_bridge_port *p;
    struct sk_buff *skb = *pskb;
    const unsigned char *dest = eth_hdr(skb)-&gt;h_dest;
    br_should_route_hook_t *rhook;

    if (unlikely(skb-&gt;pkt_type == PACKET_LOOPBACK))
        return RX_HANDLER_PASS;
    /*判断是否是有效的mac地址，即不是多播地址也不是全00地址*/
    if (!is_valid_ether_addr(eth_hdr(skb)-&gt;h_source))
        goto drop;
    /*判断是否是共享数据包，若果是则clone该数据包*/
    skb = skb_share_check(skb, GFP_ATOMIC);
    if (!skb)
        return RX_HANDLER_CONSUMED;
    /*获取数据包网桥端口的一些信息*/
    p = br_port_get_rcu(skb-&gt;dev);
    /*BPDU是网桥之间交流的报文，目标mac是 01:80:C2:00:00:00*/
    if (unlikely(is_link_local_ether_addr(dest))) {
        u16 fwd_mask = p-&gt;br-&gt;group_fwd_mask_required;

        /*
         * See IEEE 802.1D Table 7-10 Reserved addresses
         *
         * Assignment               Value
         * Bridge Group Address     01-80-C2-00-00-00
         * (MAC Control) 802.3      01-80-C2-00-00-01
         * (Link Aggregation) 802.3 01-80-C2-00-00-02
         * 802.1X PAE address       01-80-C2-00-00-03
         *
         * 802.1AB LLDP         01-80-C2-00-00-0E
         *
         * Others reserved for future standardization
         */
        switch (dest[5]) {
        case 0x00:  /* Bridge Group Address */
            /* If STP is turned off,
               then must forward to keep loop detection */
            if (p-&gt;br-&gt;stp_enabled == BR_NO_STP ||
                fwd_mask &amp; (1u &lt;&lt; dest[5]))
                goto forward;
            *pskb = skb;
            __br_handle_local_finish(skb);
            return RX_HANDLER_PASS;

        case 0x01:  /* IEEE MAC (Pause) */
            goto drop;

        case 0x0E:  /* 802.1AB LLDP */
            fwd_mask |= p-&gt;br-&gt;group_fwd_mask;
            if (fwd_mask &amp; (1u &lt;&lt; dest[5]))
                goto forward;
            *pskb = skb;
            __br_handle_local_finish(skb);
            return RX_HANDLER_PASS;

        default:
            /* Allow selective forwarding for most other protocols */
            fwd_mask |= p-&gt;br-&gt;group_fwd_mask;
            if (fwd_mask &amp; (1u &lt;&lt; dest[5]))
                goto forward;
        }

        /* Deliver packet to local host only */
        NF_HOOK(NFPROTO_BRIDGE, NF_BR_LOCAL_IN, dev_net(skb-&gt;dev),
            NULL, skb, skb-&gt;dev, NULL, br_handle_local_finish);
        return RX_HANDLER_CONSUMED;
    }

forward:
    switch (p-&gt;state) {
    case BR_STATE_FORWARDING:
        /*ebtables获取路由的hook点*/
        rhook = rcu_dereference(br_should_route_hook);
        if (rhook) {/*如果是转发状态，则转发数据包，然后返回*/
            if ((*rhook)(skb)) {
                *pskb = skb;
                return RX_HANDLER_PASS;
            }
            dest = eth_hdr(skb)-&gt;h_dest;
        }
        /* fall through */
    case BR_STATE_LEARNING:
        /*目的地址是否是设备链路层地址 */
        if (ether_addr_equal(p-&gt;br-&gt;dev-&gt;dev_addr, dest))
            skb-&gt;pkt_type = PACKET_HOST;
        /*将数据包送入数据帧处理函数br_handle_frame_finish*/
        NF_HOOK(NFPROTO_BRIDGE, NF_BR_PRE_ROUTING,
            dev_net(skb-&gt;dev), NULL, skb, skb-&gt;dev, NULL,
            br_handle_frame_finish);
        break;
    default:
drop:
        kfree_skb(skb);
    }
    return RX_HANDLER_CONSUMED;
}
</code></pre>

<pre><code>
在br_handle_frame主要做一件事，就是将数据包放进那个钩子点。


说明：br_handle_frame函数中有两个hook函数，br_handle_local_finish和br_handle_frame_finish这两个函数只有在netfilter因其他原因没有丢弃或者消化该帧时才会被调用，ebtables也能查看帧。ebtables是一个架构，能提供一些netfilter所没有的提供的额外功能，尤其是，ebtables可以过滤和修改任何类型的帧，而非仅限于那些携带ip封包的帧。

## 桥数据包处理函数——br_handle_frame_finish

br_handle_frame_finish.
作用：br_handle_frame_finish函数主要是决策将不同类别的数据包做不同的分发路径。

其函数处理的过程如下图所示：

![](/images/kernel/20201109-11.png)

首先判断该数据包是否符合桥转发的条件：

  (1)桥端口状态是否是开启状态，如果没有开启则丢掉数据包

  (2)是否允许从该桥上转发，如果不允许，则直接返回0

获得桥转发的条件以后，开始判断数据包的类型：

  (1)判断此时桥的标志为允许做哪些事情，学习还是扩展

  如果学习的标志位被至位，则更新数据转发表。否则继续向下走

  (2)根据多播或者广播报文的类型决定数据包的去留

  (3)判断此时端口的状态，如果是学习状态，则将数据包丢弃

  (要注意的是：桥的端口状态(和上面的flag不冲突，上面的flag表示网桥可以做的事情)state表示网桥端口所处于的状态)
在处理完一些需要预备的事情之后，就要为数据包的转发开始做准备了

  (1)网桥设备是否处于混杂模式，如果是则建立副本，为发往本地做个备份

  (注意的是，所有网桥端口绑定的设备都会处于混杂模式，因为 网桥运行必须此模式。但除非明确的对其进行配置，否则网桥自己是不会处于混杂模式的)

  (2)在次判断广播还是多播地址

   广播地址：仅仅设置副本，进行广播转发和发往本地

多播地址：先查多播地址转发表，如果存在，设置副本，进行多播转发，原始数据包指向NULL,如果已经传送至本地，则会释放副本，不进行本地转发，否则重新转发到本地

  (3)不是广播或者多播

  判断是否本地地址，如果是本地地址，则将原始数据包指向NULL，发往本地。

  否则进行数据包转发

## 桥数据包转发

无论是在发往本地还是转发，有一个函数的功能是不能忽略的，就是br_handle_vlan函数
</code></pre>

<pre><code>struct sk_buff *br_handle_vlan(struct net_bridge *br,
                   struct net_bridge_vlan_group *vg,
                   struct sk_buff *skb)
{
    struct br_vlan_stats *stats;
    struct net_bridge_vlan *v;
    u16 vid;

    /* If this packet was not filtered at input, let it pass */
    if (!BR_INPUT_SKB_CB(skb)-&gt;vlan_filtered)
        goto out;

    /* At this point, we know that the frame was filtered and contains
     * a valid vlan id.  If the vlan id has untagged flag set,
     * send untagged; otherwise, send tagged.
     */
    br_vlan_get_tag(skb, &amp;vid);
    /*find vid from vlan group*/
    v = br_vlan_find(vg, vid);
    /* Vlan entry must be configured at this point.  The
     * only exception is the bridge is set in promisc mode and the
     * packet is destined for the bridge device.  In this case
     * pass the packet as is.
     */
    if (!v || !br_vlan_should_use(v)) {
        if ((br-&gt;dev-&gt;flags &amp; IFF_PROMISC) &amp;&amp; skb-&gt;dev == br-&gt;dev) {
            goto out;
        } else {
            kfree_skb(skb);
            return NULL;
        }
    }
    /*statistacs the vlan if flow and if the vlan_stats_enabled is true */
    if (br-&gt;vlan_stats_enabled) {
        stats = this_cpu_ptr(v-&gt;stats);
        u64_stats_update_begin(&amp;stats-&gt;syncp);
        stats-&gt;tx_bytes += skb-&gt;len;
        stats-&gt;tx_packets++;
        u64_stats_update_end(&amp;stats-&gt;syncp);
    }

    if (v-&gt;flags &amp; BRIDGE_VLAN_INFO_UNTAGGED)
        skb-&gt;vlan_tci = 0;
out:
    return skb;
}
</code></pre>

<pre><code>
这个函数的作用很简单就是，数据包是否要带tag,

过程：

在传递进来的vlan group中查找自己所处的vlan

如果该vlan不存在则判断当前模式是否是混杂模式和数据包的设备是否是桥下的设备，选择发包或者丢弃。

如果存在，且vlan是开启的，则统计vlan接口上的数据流量，最后根据vlan出口的标记位进行位运算判断是否要带tag.

然后我们来看一下上节提到的发往本地数据包的处理函数
</code></pre>

<pre><code>static int br_pass_frame_up(struct sk_buff *skb)
{
    struct net_device *indev, *brdev = BR_INPUT_SKB_CB(skb)-&gt;brdev;
    struct net_bridge *br = netdev_priv(brdev);
    struct net_bridge_vlan_group *vg;
    struct pcpu_sw_netstats *brstats = this_cpu_ptr(br-&gt;stats);
    /*统计该桥上的流量*/
    u64_stats_update_begin(&amp;brstats-&gt;syncp);
    brstats-&gt;rx_packets++;
    brstats-&gt;rx_bytes += skb-&gt;len;
    u64_stats_update_end(&amp;brstats-&gt;syncp);

    /*获取该桥上的vlan组*/
    vg = br_vlan_group_rcu(br);
    /* Bridge is just like any other port.  Make sure the
     * packet is allowed except in promisc modue when someone
     * may be running packet capture.
     */
    if (!(brdev-&gt;flags &amp; IFF_PROMISC) &amp;&amp;
        !br_allowed_egress(vg, skb)) {
        kfree_skb(skb);
        return NET_RX_DROP;
    }
    /*替换掉数据包中的设备信息改为桥设备*/
    indev = skb-&gt;dev;
    skb-&gt;dev = brdev;
    /*配置数据包vlan的相关信息*/
    skb = br_handle_vlan(br, vg, skb);
    if (!skb)
        return NET_RX_DROP;
    /*进入NF_BR_LOCAL_IN*/
    return NF_HOOK(NFPROTO_BRIDGE, NF_BR_LOCAL_IN,
               dev_net(indev), NULL, skb, indev, NULL,
               br_netif_receive_skb);
}
</code></pre>

<pre><code>
这个函数所做的事情很简单，就是配置vlan的相关信息后，然后发往本地的netfilter钩子函数中
最后重新回到netif_recive_skb.如下函数：
</code></pre>

<pre><code>static int
br_netif_receive_skb(struct net *net, struct sock *sk, struct sk_buff *skb)
{
    return netif_receive_skb(skb);
}
</code></pre>

<pre><code>
再来看看数据包转发的函数
</code></pre>

<pre><code>static void __br_forward(const struct net_bridge_port *to, struct sk_buff *skb)
{
    struct net_bridge_vlan_group *vg;
    struct net_device *indev;

    if (skb_warn_if_lro(skb)) {
        kfree_skb(skb);
        return;
    }
    /*获取vlan组，这个组中有许多的vlanid，br_handle_vlan函数就是要在这个组中查找自己的vid*/
    vg = nbp_vlan_group_rcu(to);
    /*添加vlan的相关配置*/
    skb = br_handle_vlan(to-&gt;br, vg, skb);
    if (!skb)
        return;

    indev = skb-&gt;dev;
    skb-&gt;dev = to-&gt;dev;
    skb_forward_csum(skb);

    NF_HOOK(NFPROTO_BRIDGE, NF_BR_FORWARD,
        dev_net(indev), NULL, skb, indev, skb-&gt;dev,
        br_forward_finish);
}

int br_forward_finish(struct net *net, struct sock *sk, struct sk_buff *skb)
{
    return NF_HOOK(NFPROTO_BRIDGE, NF_BR_POST_ROUTING,
               net, sk, skb, NULL, skb-&gt;dev,
               br_dev_queue_push_xmit);

}
</code></pre>

<p>```</p>

<p>整个数据包转发的过程与转发到本地的过程类似，只不过所进入的netfilter钩子点不同.</p>

<p>整个分析中不包含数据包从本地发出的数据包</p>
]]></content>
  </entry>
  
</feed>
