<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: system~kvm | kk Blog —— 通用基础]]></title>
  <link href="http://abcdxyzk.github.io/blog/cats/system~kvm/atom.xml" rel="self"/>
  <link href="http://abcdxyzk.github.io/"/>
  <updated>2024-12-31T12:57:39+08:00</updated>
  <id>http://abcdxyzk.github.io/</id>
  <author>
    <name><![CDATA[kk]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Centos7下删除virbr0网卡信息的方法]]></title>
    <link href="http://abcdxyzk.github.io/blog/2022/10/22/kvm-virbr0/"/>
    <updated>2022-10-22T11:50:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2022/10/22/kvm-virbr0</id>
    <content type="html"><![CDATA[<p><a href="https://www.jb51.net/os/RedHat/543581.html">https://www.jb51.net/os/RedHat/543581.html</a></p>

<p>bridge模式改为普通模式后，查看网卡的时候还是可以看到很多垃圾信息，想彻底删除自己不想要的网卡配置信息，操作如下:</p>

<pre><code>    yum install libvirt
</code></pre>

<h4>查看网络列表：</h4>

<pre><code>    [root@linux-node1 ~]# virsh net-list
     Name                 State      Autostart     Persistent
    ----------------------------------------------------------
     default              active     yes           yes
</code></pre>

<h4>使用 virsh net-destroy default 删除</h4>

<pre><code>    [root@linux-node1 ~]# virsh net-destroy default
    Network default destroyed
</code></pre>

<h4>从配置文件中剔除</h4>

<pre><code>    [root@linux-node1 ~]# virsh net-undefine default
    Network default has been undefined
</code></pre>

<h4>重启libvirtd服务</h4>

<pre><code class="">    [root@linux-node1 ~]# systemctl restart libvirtd.service
    [root@linux-node1 ~]# virsh net-list
     Name                 State      Autostart     Persistent
    ----------------------------------------------------------
</code></pre>

<p>再次查看，发现不必要的信息已经清除，清爽多了</p>

<h4>dnsmasq不会自动停</h4>

<pre><code>    service libvirtd stop
    systemctl stop libvirtd.service

    service libvirtd status
</code></pre>

<p>还是有dnsmasq进程，手动kill</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[kvm虚拟化学习笔记]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/07/29/kvm-blog/"/>
    <updated>2015-07-29T15:32:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/07/29/kvm-blog</id>
    <content type="html"><![CDATA[<p><a href="http://koumm.blog.51cto.com/703525/1288795">http://koumm.blog.51cto.com/703525/1288795</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[编译qemu-kvm和安装qemu-kvm]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/07/29/kvm-qemu/"/>
    <updated>2015-07-29T15:22:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/07/29/kvm-qemu</id>
    <content type="html"><![CDATA[<p><a href="http://smilejay.com/2012/06/qemu-kvm_compilation_installation/">http://smilejay.com/2012/06/qemu-kvm_compilation_installation/</a></p>

<h4>3.4 编译和安装qemu-kvm</h4>

<p>除了在内核空间的KVM模块之外，在用户空间需要QEMU[注6]来模拟所需要CPU和设备模型以及用于启动客户机进程，这样才有了一个完整的KVM运行环境。而qemu-kvm是为了针对KVM专门做了修改和优化的QEMU分支[注7]，在本书写作的2012年，qemu-kvm分支里面的小部分特性还没有完全合并进入到qemu的主干代码之中，所以本书中采用qemu-kvm来讲解。</p>

<p>在编译和安装了KVM并且启动到编译的内核之后，下面来看一下qemu-kvm的编译和安装。</p>

<h5>3.4.1 下载qemu-kvm源代码</h5>

<p>目前的QEMU项目针对KVM的代码分支qemu-kvm也是由Redhat公司的Gleb Natapov和Marcelo Tosatti作维护者（Maintainer），代码也是托管在kernel.org上。</p>

<p>qemu-kvm开发代码仓库的网页连接为：<a href="http://git.kernel.org/?p=virt/kvm/qemu-kvm.git">http://git.kernel.org/?p=virt/kvm/qemu-kvm.git</a></p>

<p>其中，可以看到有如下3个URL连接可供下载开发中的最新qemu-kvm的代码仓库。
git://git.kernel.org/pub/scm/virt/kvm/qemu-kvm.git<br/>
<a href="http://git.kernel.org/pub/scm/virt/kvm/qemu-kvm.git">http://git.kernel.org/pub/scm/virt/kvm/qemu-kvm.git</a><br/>
<a href="https://git.kernel.org/pub/scm/virt/kvm/qemu-kvm.git">https://git.kernel.org/pub/scm/virt/kvm/qemu-kvm.git</a></p>

<p>可以根据自己实际需要选择3个中任一个用git clone命令下载即可，它们是完全一样的。</p>

<p>另外，可以到sourceforge.net的如下链接中根据需要下载qemu-kvm各个发布版本的代码压缩包（作者建议使用最新的正式发布版本，因为它的功能更多，同时也比较稳定）。</p>

<p><a href="http://sourceforge.net/projects/kvm/files/qemu-kvm/">http://sourceforge.net/projects/kvm/files/qemu-kvm/</a></p>

<p>在本节讲解编译时，以下载开发中的最新的qemu-kvm.git为例，获取其代码仓库过程如下：
<code>
    [root@jay-linux kvm_demo]# git clone\ git://git.kernel.org/pub/scm/virt/kvm/qemu-kvm.git qemu-kvm.git
    Initialized empty Git repository in /root/kvm_demo/qemu-kvm.git/.git/
    remote: Counting objects: 145222, done.
    remote: Compressing objects: 100% (35825/35825), done.
    remote: Total 145222 (delta 114656), reused 137663 (delta 107444)
    Receiving objects: 100% (145222/145222), 40.83 MiB | 10.33 MiB/s, done.
    Resolving deltas: 100% (114656/114656), done.
    [root@jay-linux kvm_demo]# cd qemu-kvm.git
    [root@jay-linux kvm.git]# pwd
    /root/kvm_demo/qemu-kvm.git
</code></p>

<h5>3.4.2 配置和编译qemu-kvm</h5>

<p>qemu-kvm的配置并不复杂，通常情况下，可以直接运行代码仓库中configure文件进行配置即可。当然，如果对其配置并不熟悉，可以运行“./configure –help”命令查看配置的一些选项及其帮助信息。</p>

<p>显示配置的帮助手册信息如下：
```
    [root@jay-linux qemu-kvm.git]# ./configure &ndash;help
    Usage: configure [options]
    Options: [defaults in brackets after descriptions]</p>

<pre><code>Standard options:
--help                   print this message
--prefix=PREFIX          install in PREFIX [/usr/local]
--interp-prefix=PREFIX   where to find shared libraries, etc.
use %M for cpu name [/usr/gnemul/qemu-%M]
--target-list=LIST       set target list (default: build everything)
Available targets: i386-softmmu x86_64-softmmu
&lt;!- 此处省略百余行帮助信息的输出 -&gt;
--disable-guest-agent    disable building of the QEMU Guest Agent
--enable-guest-agent     enable building of the QEMU Guest Agent
--with-coroutine=BACKEND coroutine backend. Supported options:
gthread, ucontext, sigaltstack, windows

NOTE: The object files are built at the place where configure is launched
</code></pre>

<pre><code>
执行configure文件进行配置的过程如下：
</code></pre>

<pre><code>[root@jay-linux qemu-kvm.git]# ./configure
Install prefix    /usr/local
BIOS directory    /usr/local/share/qemu
binary directory  /usr/local/bin
library directory /usr/local/lib
include directory /usr/local/include
config directory  /usr/local/etc
Manual directory  /usr/local/share/man
ELF interp prefix /usr/gnemul/qemu-%M
Source path       /root/kvm_demo/qemu-kvm.git
C compiler        gcc
Host C compiler   gcc
&lt;!– 此处省略数十行 –&gt;
VNC support       yes     #通常需要通过VNC连接到客户机中
&lt;!– 此处省略十余行 –&gt;
KVM support       yes     #这是对KVM的支持
TCG interpreter   no
KVM device assig. yes    #这是对KVM中VT-d功能的支持
&lt;!– 此处省略十余行 –&gt;
OpenGL support    yes
libiscsi support  no
build guest agent yes
coroutine backend ucontext
</code></pre>

<pre><code>需要注意的是，上面命令行输出的KVM相关的选项需要是配置为yes，另外，一般VNC的支持也是配置为yes的（因为通常需要用VNC连接到客户机中）。

【2013.05.13 updated】 在configure时，可能遇到“glib-2.12 required to compile QEMU”的错误，是需要安装glib2和glib2-dev软件包，在RHEL上的安装命令为“yum install glib2 glib2-devel”，在Ubuntu上安装的过程为“apt-get install libglib2.0 libglib2.0-dev”。

经过配置之后，进行编译就很简单了，直接执行make即可进行编译，如下所示：
</code></pre>

<pre><code>[root@jay-linux qemu-kvm.git]# make -j 20
GEN   config-host.h
GEN   trace.h
GEN   qemu-options.def
GEN   qmp-commands.h
GEN   qapi-types.h
GEN   qapi-visit.h
GEN   tests/test-qapi-types.h
GEN   tests/test-qapi-visit.h
GEN   tests/test-qmp-commands.h
CC    libcacard/cac.o
CC    libcacard/event.o
&lt;!– 此处省略数百行的编译时输出信息 –&gt;
CC    x86_64-softmmu/target-i386/cpu.o
CC    x86_64-softmmu/target-i386/machine.o
CC    x86_64-softmmu/target-i386/arch_memory_mapping.o
CC    x86_64-softmmu/target-i386/arch_dump.o
CC    x86_64-softmmu/target-i386/kvm.o
CC    x86_64-softmmu/target-i386/hyperv.o
LINK  x86_64-softmmu/qemu-system-x86_64
</code></pre>

<pre><code>
可以看到，最后有编译生成qemu-system-x86_64文件，它就是我们常用的qemu-kvm的命令行工具（在多数Linux发行版中自带的qemu-kvm软件包的命令行是qemu-kvm，只是名字不同而已）。

##### 3.4.2 安装qemu-kvm

编译完成之后，运行“make install”命令即可安装qemu-kvm，其过程如下：
</code></pre>

<pre><code>[root@jay-linux qemu-kvm.git]# make install | tee make-install.log
install -d -m 0755 “/usr/local/share/qemu”
install -d -m 0755 “/usr/local/etc/qemu”
install -c -m 0644 /root/kvm_demo/qemu-kvm.git/sysconfigs/target/target-x86_64.conf “/usr/local/etc/qemu”
install -c -m 0644 /root/kvm_demo/qemu-kvm.git/sysconfigs/target/cpus-x86_64.conf “/usr/local/share/qemu”
install -d -m 0755 “/usr/local/bin”
install -c -m 0755  vscclient qemu-ga qemu-nbd qemu-img qemu-io  “/usr/local/bin”
install -d -m 0755 “/usr/local/libexec”
&lt;!– 此处省略数行的安装时输出信息 –&gt;
make[1]: Entering directory `/root/kvm_demo/qemu-kvm.git/x86_64-softmmu’
install -m 755 qemu-system-x86_64 “/usr/local/bin”
strip “/usr/local/bin/qemu-system-x86_64″
make[1]: Leaving directory `/root/kvm_demo/qemu-kvm.git/x86_64-softmmu’
</code></pre>

<pre><code>
qemu-kvm的安装过程的主要任务有这几个：创建qemu的一些目录，拷贝一些配置文件到相应的目录下，拷贝一些firmware文件(如：sgabios.bin, kvmvapic.bin)到目录下以便qemu-kvm的命令行启动时可以找到对应的固件提供给客户机使用，拷贝keymaps到相应的目录下以便在客户机中支持各种所需键盘类型，拷贝qemu-system-x86_64、qemu-img等可执行程序到对应的目录下。下面的一些命令行检查了qemu-kvm被安装了之后的系统状态。
</code></pre>

<pre><code>[root@jay-linux qemu-kvm.git]# which qemu-system-x86_64
/usr/local/bin/qemu-system-x86_64
[root@jay-linux qemu-kvm.git]# which qemu-img
/usr/local/bin/qemu-img
[root@jay-linux qemu-kvm.git]# ls /usr/local/share/qemu/
bamboo.dtb        mpc8544ds.dtb     petalogix-ml605.dtb       pxe-pcnet.rom    slof.bin            vgabios-vmware.bin
bios.bin          multiboot.bin     petalogix-s3adsp1800.dtb  pxe-rtl8139.rom  spapr-rtas.bin
cpus-x86_64.conf  openbios-ppc      ppc_rom.bin               pxe-virtio.rom   vgabios.bin
keymaps           openbios-sparc32  pxe-e1000.rom             qemu-icon.bmp    vgabios-cirrus.bin
kvmvapic.bin      openbios-sparc64  pxe-eepro100.rom          s390-zipl.rom    vgabios-qxl.bin
linuxboot.bin     palcode-clipper   pxe-ne2k_pci.rom          sgabios.bin      vgabios-stdvga.bin
[root@jay-linux qemu-kvm.git]# ls /usr/local/share/qemu/keymaps/
ar    common  de     en-gb  es  fi  fr     fr-ca  hr  is  ja  lv  modifiers  nl-be  pl  pt-br  sl  th
bepo  da      de-ch  en-us  et  fo  fr-be  fr-ch  hu  it  lt  mk  nl         no     pt  ru     sv  tr
</code></pre>

<p>```</p>

<p>由于qemu-kvm是用户空间的程序，安装之后不用重启系统，直接用qemu-system-x86_64、qemu-img这样的命令行工具即可使用qemu-kvm了。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[KVM源代码分析4:内存虚拟化]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/07/29/kvm-src4/"/>
    <updated>2015-07-29T14:49:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/07/29/kvm-src4</id>
    <content type="html"><![CDATA[<p><a href="http://www.oenhan.com/kvm-src-4-mem">http://www.oenhan.com/kvm-src-4-mem</a></p>

<p>在虚拟机的创建与运行中pc_init_pci负责在qemu中初始化虚拟机，内存初始化也是在这里完成的，还是一步步从qemu说起，在vl.c的main函数中有ram_size参数，由qemu入参标识QEMU_OPTION_m设定，顾名思义就是虚拟机内存的大小，通过machine->init一步步传递给pc_init1函数。在这里分出了above_4g_mem_size和below_4g_mem_size，即高低端内存（也不一定是32bit机器..），然后开始初始化内存，即pc_memory_init，内存通过memory_region_init_ram下面的qemu_ram_alloc分配，使用qemu_ram_alloc_from_ptr。</p>

<p>插播qemu对内存条的模拟管理，是通过RAMBlock和ram_list管理的，RAMBlock就是每次申请的内存池，ram_list则是RAMBlock的链表，他们结构如下：</p>

<pre><code>    typedef struct RAMBlock {
        //对应宿主的内存地址
        uint8_t *host;
        //block在ramlist中的偏移
        ram_addr_t offset;
        //block长度
        ram_addr_t length;
        uint32_t flags;
        //block名字
        char idstr[256];
        QLIST_ENTRY(RAMBlock) next;
    #if defined(__linux__) &amp;&amp; !defined(TARGET_S390X)
        int fd;
    #endif
    } RAMBlock;

    typedef struct RAMList {
        //看代码理解就是list的head，但是不知道为啥叫dirty...
        uint8_t *phys_dirty;
        QLIST_HEAD(ram, RAMBlock) blocks;
    } RAMList;
</code></pre>

<p>下面再回到qemu_ram_alloc_from_ptr函数，使用find_ram_offset赋值给new block的offset，find_ram_offset具体工作模型已经在KVM源代码分析2:虚拟机的创建与运行中提到了，不赘述。然后是一串判断，在kvm_enabled的情况下使用new_block->host = kvm_vmalloc(size)，最终内存是qemu_vmalloc分配的，使用qemu_memalign干活。</p>

<pre><code>    void *qemu_memalign(size_t alignment, size_t size)
    {
        void *ptr;
        //使用posix进行内存针对页大小对齐
    #if defined(_POSIX_C_SOURCE) &amp;&amp; !defined(__sun__)
        int ret;
        ret = posix_memalign(&amp;ptr, alignment, size);
        if (ret != 0) {
            fprintf(stderr, "Failed to allocate %zu B: %s\n",
                    size, strerror(ret));
            abort();
        }
    #elif defined(CONFIG_BSD)
        ptr = qemu_oom_check(valloc(size));
    #else
        //所谓检查oom就是看memalign对应malloc申请内存是否成功
        ptr = qemu_oom_check(memalign(alignment, size));
    #endif
        trace_qemu_memalign(alignment, size, ptr);
        return ptr;
    }
</code></pre>

<p>以上qemu_vmalloc进行内存申请就结束了。在qemu_ram_alloc_from_ptr函数末尾则是将block添加到链表，realloc整个ramlist，用memset初始化整个ramblock，madvise对内存使用限定。
然后一层层的退回到pc_memory_init函数。</p>

<p>此时pc.ram已经分配完成，ram_addr已经拿到了分配的内存地址，MemoryRegion ram初始化完成。下面则是对已有的ram进行分段，即ram-below-4g和ram-above-4g，也就是高端内存和低端内存。用memory_region_init_alias初始化子MemoryRegion，然后将memory_region_add_subregion添加关联起来，memory_region_add_subregion具体细节“KVM源码分析2”中已经说了，参考对照着看吧，中间很多映射代码过程也只是qemu遗留的软件实现，没看到具体存在的意义，直接看到kvm_set_user_memory_region函数，内核真正需要kvm_vm_ioctl传递过去的参数是什么， struct kvm_userspace_memory_region mem而已，也就是</p>

<pre><code>    struct kvm_userspace_memory_region {
        __u32 slot;
        __u32 flags;
        __u64 guest_phys_addr;
        __u64 memory_size;    /* bytes */
        __u64 userspace_addr; /* start of the userspace allocated memory */
    };
</code></pre>

<p>kvm_vm_ioctl进入到内核是在KVM_SET_USER_MEMORY_REGION参数中，即执行kvm_vm_ioctl_set_memory_region，然后一直向下，到<code>__kvm_set_memory_region</code>函数，check_memory_region_flags检查mem->flags是否合法，而当前flag也就使用了两位，KVM_MEM_LOG_DIRTY_PAGES和KVM_MEM_READONLY，从qemu传递过来只能是KVM_MEM_LOG_DIRTY_PAGES,下面是对mem中各参数的合规检查，(mem->memory_size &amp; (PAGE_SIZE – 1))要求以页为单位，(mem->guest_phys_addr &amp; (PAGE_SIZE – 1))要求guest_phys_addr页对齐，而<code>((mem-&gt;userspace_addr &amp; (PAGE_SIZE – 1)) || !access_ok(VERIFY_WRITE,(void __user *)(unsigned long)mem-&gt;userspace_addr,mem-&gt;memory_size))</code>则保证host的线性地址页对齐而且该地址域有写权限。</p>

<p>id_to_memslot则是根据qemu的内存槽号得到kvm结构下的内存槽号，转换关系来自id_to_index数组，那映射关系怎么来的，映射关系是一一对应的，在kvm_create_vm虚拟机创建过程中，kvm_init_memslots_id初始化对应关系，即slots->id_to_index[i] = slots->memslots[i].id = i，当前映射是没有意义的，估计是为了后续扩展而存在的。</p>

<p>扩充了new的kvm_memory_slot，下面直接在代码中注释更方便：</p>

<pre><code>    //映射内存有大小，不是删除内存条
    if (npages) {
        //内存槽号没有虚拟内存条，意味内存新创建
        if (!old.npages)
            change = KVM_MR_CREATE;
        else { /* Modify an existing slot. */
            //修改已存在的内存修改标志或者平移映射地址
            //下面是不能处理的状态（内存条大小不能变，物理地址不能变，不能修改只读）
            if ((mem-&gt;userspace_addr != old.userspace_addr) ||
                (npages != old.npages) ||
                ((new.flags ^ old.flags) &amp; KVM_MEM_READONLY))
                goto out;
            //guest地址不同，内存条平移
            if (base_gfn != old.base_gfn)
                change = KVM_MR_MOVE;
            else if (new.flags != old.flags)
                //修改属性
                change = KVM_MR_FLAGS_ONLY;
            else { /* Nothing to change. */
                r = 0;
                goto out;
            }
        }
    } else if (old.npages) {
        //申请插入的内存为0，而内存槽上有内存，意味删除
        change = KVM_MR_DELETE;
    } else /* Modify a non-existent slot: disallowed. */
        goto out;
</code></pre>

<p>另外看kvm_mr_change就知道memslot的变动值了：</p>

<pre><code>    enum kvm_mr_change {
        KVM_MR_CREATE,
        KVM_MR_DELETE,
        KVM_MR_MOVE,
        KVM_MR_FLAGS_ONLY,
    };
</code></pre>

<p>在往下是一段检查</p>

<pre><code>    if ((change == KVM_MR_CREATE) || (change == KVM_MR_MOVE)) {
        /* Check for overlaps */
        r = -EEXIST;
        kvm_for_each_memslot(slot, kvm-&gt;memslots) {
            if ((slot-&gt;id &gt;= KVM_USER_MEM_SLOTS) ||
                //下面排除掉准备操作的内存条，在KVM_MR_MOVE中是有交集的
                (slot-&gt;id == mem-&gt;slot))
                continue;
            //下面就是当前已有的slot与new在guest线性区间上有交集
            if (!((base_gfn + npages &lt;= slot-&gt;base_gfn) ||
                  (base_gfn &gt;= slot-&gt;base_gfn + slot-&gt;npages)))
                goto out;
                //out错误码就是EEXIST
        }
    }
</code></pre>

<p>如果是新插入内存条，代码则走入kvm_arch_create_memslot函数，里面主要是一个循环，KVM_NR_PAGE_SIZES是分页的级数，此处是3，第一次循环，lpages = gfn_to_index(slot->base_gfn + npages – 1,slot->base_gfn, level) + 1，lpages就是一级页表所需要的page数，大致是npages>>0<em>9,然后为slot->arch.rmap[i]申请了内存空间，此处可以猜想，rmap就是一级页表了，继续看，lpages约为npages>>1</em>9,此处又多为lpage_info申请了同等空间，然后对lpage_info初始化赋值，现在看不到lpage_info的具体作用，看到后再补上。整体上看kvm_arch_create_memslot做了一个3级的软件页表。</p>

<p>如果有脏页,并且脏页位图为空,则分配脏页位图, kvm_create_dirty_bitmap实际就是”页数/8″.</p>

<pre><code>    if ((new.flags &amp; KVM_MEM_LOG_DIRTY_PAGES) &amp;&amp; !new.dirty_bitmap) {
        if (kvm_create_dirty_bitmap(&amp;new) &lt; 0)
            goto out_free;
    }
</code></pre>

<p>当内存条的改变是KVM_MR_DELETE或者KVM_MR_MOVE,先申请一个slots,把kvm->memslots暂存到这里,首先通过id_to_memslot获取准备插入的内存条对应到kvm的插槽是slot,无论删除还是移动,将其先标记为KVM_MEMSLOT_INVALID,然后是install_new_memslots,其实就是更新了一下slots->generation的值,</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[KVM源代码分析3:CPU虚拟化]]></title>
    <link href="http://abcdxyzk.github.io/blog/2015/07/29/kvm-src3/"/>
    <updated>2015-07-29T14:48:00+08:00</updated>
    <id>http://abcdxyzk.github.io/blog/2015/07/29/kvm-src3</id>
    <content type="html"><![CDATA[<p><a href="http://www.oenhan.com/kvm-src-3-cpu">http://www.oenhan.com/kvm-src-3-cpu</a></p>

<p>在虚拟机的创建与运行章节里面笼统的介绍了KVM在qemu中的创建和运行，基本的qemu代码流程已经梳理清楚，后续主要写一些硬件虚拟化的原理和代码流程，主要写原理和qemu控制KVM运行的的ioctl接口，后续对内核代码的梳理也从这些接口下手。</p>

<h4>1.VT-x 技术</h4>

<p>Intel处理器支持的虚拟化技术即是VT-x，之所以CPU支持硬件虚拟化是因为软件虚拟化的效率太低。</p>

<p>处理器虚拟化的本质是分时共享，主要体现在状态恢复和资源隔离，实际上每个VM对于VMM看就是一个task么，之前Intel处理器在虚拟化上没有提供默认的硬件支持，传统 x86 处理器有4个特权级，Linux使用了0,3级别，0即内核，3即用户态，（更多参考CPU的运行环、特权级与保护）而在虚拟化架构上，虚拟机监控器的运行级别需要内核态特权级，而CPU特权级被传统OS占用，所以Intel设计了VT-x，提出了VMX模式，即VMX root operation 和 VMX non-root operation，虚拟机监控器运行在VMX root operation，虚拟机运行在VMX non-root operation。每个模式下都有相对应的0~3特权级。</p>

<p>为什么引入这两种特殊模式，在传统x86的系统中，CPU有不同的特权级，是为了划分不同的权限指令，某些指令只能由系统软件操作，称为特权指令，这些指令只能在最高特权级上才能正确执行，反之则会触发异常，处理器会陷入到最高特权级，由系统软件处理。还有一种需要操作特权资源（如访问中断寄存器）的指令，称为敏感指令。OS运行在特权级上，屏蔽掉用户态直接执行的特权指令，达到控制所有的硬件资源目的；而在虚拟化环境中，VMM控制所有所有硬件资源，VM中的OS只能占用一部分资源，OS执行的很多特权指令是不能真正对硬件生效的，所以原特权级下有了root模式，OS指令不需要修改就可以正常执行在特权级上，但这个特权级的所有敏感指令都会传递到root模式处理，这样达到了VMM的目的。</p>

<p>在KVM源代码分析1:基本工作原理章节中也说了kvm分3个模式，对应到VT-x 中即是客户模式对应vmx非root模式，内核模式对应VMX root模式下的0特权级，用户模式对应vmx root模式下的3特权级。</p>

<p>如下图<br/>
<img src="/images/system/kvm/2015-07-29-6.jpg" alt="" /></p>

<p>在非根模式下敏感指令引发的陷入称为VM-Exit，VM-Exit发生后，CPU从非根模式切换到根模式；对应的，VM-Entry则是从根模式到非根模式，通常意味着调用VM进入运行态。VMLAUCH/VMRESUME命令则是用来发起VM-Entry。</p>

<h4>2.VMCS</h4>

<p>VMCS保存虚拟机的相关CPU状态，每个VCPU都有一个VMCS（内存的），每个物理CPU都有VMCS对应的寄存器（物理的），当CPU发生VM-Entry时，CPU则从VCPU指定的内存中读取VMCS加载到物理CPU上执行，当发生VM-Exit时，CPU则将当前的CPU状态保存到VCPU指定的内存中，即VMCS，以备下次VMRESUME。</p>

<p>VMLAUCH指VM的第一次VM-Entry，VMRESUME则是VMLAUCH之后后续的VM-Entry。VMCS下有一些控制域：</p>

<table>
<tr>
    <td>
        VM-execution controls
    </td>
    <td>
        Determines what operations cause VM exits
    </td>
    <td>
        CR0, CR3, CR4, Exceptions, IO Ports, Interrupts, Pin Events, etc
    </td>
</tr>
<tr>
    <td>
        Guest-state area
    </td>
    <td>
        Saved on VM exits，Reloaded on VM entry
    </td>
    <td>
        EIP, ESP, EFLAGS, IDTR, Segment Regs, Exit info, etc
    </td>
</tr>
<tr>
    <td>
        Host-state area
    </td>
    <td>
        Loaded on VM exits
    </td>
    <td>
        CR3, EIP set to monitor entry point, EFLAGS hardcoded, etc
    </td>
</tr>
<tr>
    <td>
        VM-exit controls
    </td>
    <td>
        Determines which state to save, load, how to transition
    </td>
    <td>
        Example: MSR save-load list  
    </td>
</tr>
<tr>
    <td>
        VM-entry controls
    </td>
    <td>
        Determines which state to load, how to transition
    </td>
    <td>
        Including injecting events (interrupts, exceptions) on entry
    </td>
</tr>
</table>


<p>关于具体控制域的细节，还是翻Intel手册吧。</p>

<h4>3.VM-Entry/VM-Exit</h4>

<p>VM-Entry是从根模式切换到非根模式，即VMM切换到guest上，这个状态由VMM发起，发起之前先保存VMM中的关键寄存器内容到VMCS中，然后进入到VM-Entry，VM-Entry附带参数主要有3个：1.guest是否处于64bit模式，2.MSR VM-Entry控制，3.注入事件。1应该只在VMLAUCH有意义，3更多是在VMRESUME，而VMM发起VM-Entry更多是因为3，2主要用来每次更新MSR。</p>

<p>VM-Exit是CPU从非根模式切换到根模式，从guest切换到VMM的操作，VM-Exit触发的原因就很多了，执行敏感指令，发生中断，模拟特权资源等。</p>

<p>运行在非根模式下的敏感指令一般分为3个方面：</p>

<p>1.行为没有变化的，也就是说该指令能够正确执行。</p>

<p>2.行为有变化的，直接产生VM-Exit。</p>

<p>3.行为有变化的，但是是否产生VM-Exit受到VM-Execution控制域控制。</p>

<p>主要说一下”受到VM-Execution控制域控制”的敏感指令，这个就是针对性的硬件优化了，一般是1.产生VM-Exit；2.不产生VM-Exit，同时调用优化函数完成功能。典型的有“RDTSC指令”。除了大部分是优化性能的，还有一小部分是直接VM-Exit执行指令结果是异常的，或者说在虚拟化场景下是不适用的，典型的就是TSC offset了。</p>

<p>VM-Exit发生时退出的相关信息，如退出原因、触发中断等，这些内容保存在VM-Exit信息域中。</p>

<h4>4.KVM_CREATE_VM</h4>

<p>创建VM就写这里吧，kvm_dev_ioctl_create_vm函数是主干，在kvm_create_vm中，主要有两个函数，kvm_arch_init_vm和hardware_enable_all，需要注意，但是更先一步的是KVM结构体，下面的struct是精简后的版本。</p>

<pre><code>    struct kvm {
        struct mm_struct *mm; /* userspace tied to this vm */
        struct kvm_memslots *memslots;  /*qemu模拟的内存条模型*/
        struct kvm_vcpu *vcpus[KVM_MAX_VCPUS]; /* 模拟的CPU */
        atomic_t online_vcpus;
        int last_boosted_vcpu;
        struct list_head vm_list;  //HOST上VM管理链表，
        struct kvm_io_bus *buses[KVM_NR_BUSES];
        struct kvm_vm_stat stat;
        struct kvm_arch arch; //这个是host的arch的一些参数
        atomic_t users_count;

        long tlbs_dirty;
        struct list_head devices;
    };
</code></pre>

<p>kvm_arch_init_vm基本没有特别动作，初始化了KVM->arch，以及更新了kvmclock函数，这个另外再说。</p>

<p>而hardware_enable_all，针对于每个CPU执行“on_each_cpu(hardware_enable_nolock, NULL, 1）”，在hardware_enable_nolock中先把cpus_hardware_enabled置位，进入到kvm_arch_hardware_enable中，有hardware_enable和TSC初始化规则，主要看hardware_enable，crash_enable_local_vmclear清理位图，判断MSR_IA32_FEATURE_CONTROL寄存器是否满足虚拟环境，不满足则将条件写入到寄存器内，CR4将X86_CR4_VMXE置位，另外还有kvm_cpu_vmxon打开VMX操作模式，外层包了vmm_exclusive的判断，它是kvm_intel.ko的外置参数，默认唯一，可以让用户强制不使用VMM硬件支持。</p>

<h4>5.KVM_CREATE_VCPU</h4>

<p>kvm_vm_ioctl_create_vcpu主要有三部分，kvm_arch_vcpu_create，kvm_arch_vcpu_setup和kvm_arch_vcpu_postcreate，重点自然是kvm_arch_vcpu_create。老样子，在这之前先看一下VCPU的结构体。</p>

<pre><code>    struct kvm_vcpu {
        struct kvm *kvm;  //归属的KVM
    #ifdef CONFIG_PREEMPT_NOTIFIERS
        struct preempt_notifier preempt_notifier;
    #endif
        int cpu;
        int vcpu_id;
        int srcu_idx;
        int mode;
        unsigned long requests;
        unsigned long guest_debug;

        struct mutex mutex;
        struct kvm_run *run;  //运行时的状态

        int fpu_active;
        int guest_fpu_loaded, guest_xcr0_loaded;
        wait_queue_head_t wq; //队列
        struct pid *pid;
        int sigset_active;
        sigset_t sigset;
        struct kvm_vcpu_stat stat; //一些数据

    #ifdef CONFIG_HAS_IOMEM
        int mmio_needed;
        int mmio_read_completed;
        int mmio_is_write;
        int mmio_cur_fragment;
        int mmio_nr_fragments;
        struct kvm_mmio_fragment mmio_fragments[KVM_MAX_MMIO_FRAGMENTS];
    #endif

    #ifdef CONFIG_KVM_ASYNC_PF
        struct {
            u32 queued;
            struct list_head queue;
            struct list_head done;
            spinlock_t lock;
        } async_pf;
    #endif

    #ifdef CONFIG_HAVE_KVM_CPU_RELAX_INTERCEPT
        /*
         * Cpu relax intercept or pause loop exit optimization
         * in_spin_loop: set when a vcpu does a pause loop exit
         *  or cpu relax intercepted.
         * dy_eligible: indicates whether vcpu is eligible for directed yield.
         */
        struct {
            bool in_spin_loop;
            bool dy_eligible;
        } spin_loop;
    #endif
        bool preempted;
        struct kvm_vcpu_arch arch;  //当前VCPU虚拟的架构，默认介绍X86
    };
</code></pre>

<p>借着看kvm_arch_vcpu_create，它借助kvm_x86_ops->vcpu_create即vmx_create_vcpu完成任务，vmx是X86硬件虚拟化层，从代码看，qemu用户态是一层，kernel 中KVM通用代码是一层，类似kvm_x86_ops是一层，针对各个不同硬件架构，而vcpu_vmx则是具体架构的虚拟化方案一层。首先是kvm_vcpu_init初始化，主要是填充结构体，可以注意的是vcpu->run分派了一页内存，下面有kvm_arch_vcpu_init负责填充x86 CPU结构体，下面就是kvm_vcpu_arch：</p>

<pre><code>    struct kvm_vcpu_arch {
        /*
         * rip and regs accesses must go through
         * kvm_{register,rip}_{read,write} functions.
         */
        unsigned long regs[NR_VCPU_REGS];
        u32 regs_avail;
        u32 regs_dirty;
        //类似这些寄存器就是就是用来缓存真正的CPU值的
        unsigned long cr0;
        unsigned long cr0_guest_owned_bits;
        unsigned long cr2;
        unsigned long cr3;
        unsigned long cr4;
        unsigned long cr4_guest_owned_bits;
        unsigned long cr8;
        u32 hflags;
        u64 efer;
        u64 apic_base;
        struct kvm_lapic *apic;    /* kernel irqchip context */
        unsigned long apic_attention;
        int32_t apic_arb_prio;
        int mp_state;
        u64 ia32_misc_enable_msr;
        bool tpr_access_reporting;
        u64 ia32_xss;

        /*
         * Paging state of the vcpu
         *
         * If the vcpu runs in guest mode with two level paging this still saves
         * the paging mode of the l1 guest. This context is always used to
         * handle faults.
         */
        struct kvm_mmu mmu; //内存管理，更多的是附带了直接操作函数

        /*
         * Paging state of an L2 guest (used for nested npt)
         *
         * This context will save all necessary information to walk page tables
         * of the an L2 guest. This context is only initialized for page table
         * walking and not for faulting since we never handle l2 page faults on
         * the host.
         */
        struct kvm_mmu nested_mmu;

        /*
         * Pointer to the mmu context currently used for
         * gva_to_gpa translations.
         */
        struct kvm_mmu *walk_mmu;

        struct kvm_mmu_memory_cache mmu_pte_list_desc_cache;
        struct kvm_mmu_memory_cache mmu_page_cache;
        struct kvm_mmu_memory_cache mmu_page_header_cache;

        struct fpu guest_fpu;
        u64 xcr0;
        u64 guest_supported_xcr0;
        u32 guest_xstate_size;

        struct kvm_pio_request pio;
        void *pio_data;

        u8 event_exit_inst_len;

        struct kvm_queued_exception {
            bool pending;
            bool has_error_code;
            bool reinject;
            u8 nr;
            u32 error_code;
        } exception;

        struct kvm_queued_interrupt {
            bool pending;
            bool soft;
            u8 nr;
        } interrupt;

        int halt_request; /* real mode on Intel only */

        int cpuid_nent;
        struct kvm_cpuid_entry2 cpuid_entries[KVM_MAX_CPUID_ENTRIES];

        int maxphyaddr;

        /* emulate context */
        //下面是KVM的软件模拟模式，也就是没有vmx的情况，估计也没人用这一套
        struct x86_emulate_ctxt emulate_ctxt;
        bool emulate_regs_need_sync_to_vcpu;
        bool emulate_regs_need_sync_from_vcpu;
        int (*complete_userspace_io)(struct kvm_vcpu *vcpu);

        gpa_t time;
        struct pvclock_vcpu_time_info hv_clock;
        unsigned int hw_tsc_khz;
        struct gfn_to_hva_cache pv_time;
        bool pv_time_enabled;
        /* set guest stopped flag in pvclock flags field */
        bool pvclock_set_guest_stopped_request;

        struct {
            u64 msr_val;
            u64 last_steal;
            u64 accum_steal;
            struct gfn_to_hva_cache stime;
            struct kvm_steal_time steal;
        } st;

        u64 last_guest_tsc;
        u64 last_host_tsc;
        u64 tsc_offset_adjustment;
        u64 this_tsc_nsec;
        u64 this_tsc_write;
        u64 this_tsc_generation;
        bool tsc_catchup;
        bool tsc_always_catchup;
        s8 virtual_tsc_shift;
        u32 virtual_tsc_mult;
        u32 virtual_tsc_khz;
        s64 ia32_tsc_adjust_msr;

        atomic_t nmi_queued;  /* unprocessed asynchronous NMIs */
        unsigned nmi_pending; /* NMI queued after currently running handler */
        bool nmi_injected;    /* Trying to inject an NMI this entry */

        struct mtrr_state_type mtrr_state;
        u64 pat;

        unsigned switch_db_regs;
        unsigned long db[KVM_NR_DB_REGS];
        unsigned long dr6;
        unsigned long dr7;
        unsigned long eff_db[KVM_NR_DB_REGS];
        unsigned long guest_debug_dr7;

        u64 mcg_cap;
        u64 mcg_status;
        u64 mcg_ctl;
        u64 *mce_banks;

        /* Cache MMIO info */
        u64 mmio_gva;
        unsigned access;
        gfn_t mmio_gfn;
        u64 mmio_gen;

        struct kvm_pmu pmu;

        /* used for guest single stepping over the given code position */
        unsigned long singlestep_rip;

        /* fields used by HYPER-V emulation */
        u64 hv_vapic;

        cpumask_var_t wbinvd_dirty_mask;

        unsigned long last_retry_eip;
        unsigned long last_retry_addr;

        struct {
            bool halted;
            gfn_t gfns[roundup_pow_of_two(ASYNC_PF_PER_VCPU)];
            struct gfn_to_hva_cache data;
            u64 msr_val;
            u32 id;
            bool send_user_only;
        } apf;

        /* OSVW MSRs (AMD only) */
        struct {
            u64 length;
            u64 status;
        } osvw;

        struct {
            u64 msr_val;
            struct gfn_to_hva_cache data;
        } pv_eoi;

        /*
         * Indicate whether the access faults on its page table in guest
         * which is set when fix page fault and used to detect unhandeable
         * instruction.
         */
        bool write_fault_to_shadow_pgtable;

        /* set at EPT violation at this point */
        unsigned long exit_qualification;

        /* pv related host specific info */
        struct {
            bool pv_unhalted;
        } pv;
    };
</code></pre>

<p>整个arch结构真是长，很适合凑篇幅，很多结构其他过程涉及到的再提吧，反正我也不知道。</p>

<p>kvm_arch_vcpu_init初始化了x86在虚拟化底层的实现函数，首先是pv和emulate_ctxt，这些不支持VMX下的模拟虚拟化，尤其是vcpu->arch.emulate_ctxt.ops = &amp;emulate_ops，emulate_ops初始化虚拟化模拟的对象函数。</p>

<pre><code class="">    static struct x86_emulate_ops emulate_ops = {
        .read_std            = kvm_read_guest_virt_system,
        .write_std           = kvm_write_guest_virt_system,
        .fetch               = kvm_fetch_guest_virt,
        .read_emulated       = emulator_read_emulated,
        .write_emulated      = emulator_write_emulated,
        .cmpxchg_emulated    = emulator_cmpxchg_emulated,
        .invlpg              = emulator_invlpg,
        .pio_in_emulated     = emulator_pio_in_emulated,
        .pio_out_emulated    = emulator_pio_out_emulated,
        .get_segment         = emulator_get_segment,
        .set_segment         = emulator_set_segment,
        .get_cached_segment_base = emulator_get_cached_segment_base,
        .get_gdt             = emulator_get_gdt,
        .get_idt         = emulator_get_idt,
        .set_gdt             = emulator_set_gdt,
        .set_idt         = emulator_set_idt,
        .get_cr              = emulator_get_cr,
        .set_cr              = emulator_set_cr,
        .cpl                 = emulator_get_cpl,
        .get_dr              = emulator_get_dr,
        .set_dr              = emulator_set_dr,
        .set_msr             = emulator_set_msr,
        .get_msr             = emulator_get_msr,
        .halt                = emulator_halt,
        .wbinvd              = emulator_wbinvd,
        .fix_hypercall       = emulator_fix_hypercall,
        .get_fpu             = emulator_get_fpu,
        .put_fpu             = emulator_put_fpu,
        .intercept           = emulator_intercept,
        .get_cpuid           = emulator_get_cpuid,
    };
</code></pre>

<p>x86_emulate_ops函数看看就好，实际上也很少有人放弃vmx直接软件模拟。后面又有mp_state，给pio_data分配了一个page，kvm_set_tsc_khz设置TSC，kvm_mmu_create则是初始化MMU的函数，里面的函数都是地址转换的重点，在内存虚拟化重点提到。kvm_create_lapic初始化lapic，初始化mce_banks结构，还有pv_time,xcr0,xstat,pmu等，类似x86硬件结构上需要存在的，OS底层需要看到的硬件名称都要有对应的软件结构。</p>

<p>回到vmx_create_vcpu，vmx的guest_msrs分配得到一个page，后面是vmcs的分配，vmx->loaded_vmcs->vmcs = alloc_vmcs()，alloc_vmcs为当前cpu执行alloc_vmcs_cpu，alloc_vmcs_cpu中alloc_pages_exact_node分配给vmcs，alloc_pages_exact_node调用<code>__alloc_pages</code>实现，原来以为vmcs占用了一个page，但此处从伙伴系统申请了2<sup>vmcs</sup>_config.order页，此处vmcs_config在setup_vmcs_config中初始化，vmcs_conf->order = get_order(vmcs_config.size)，而vmcs_conf->size = vmx_msr_high &amp; 0x1fff，又rdmsr(MSR_IA32_VMX_BASIC, vmx_msr_low, vmx_msr_high)，此处size由于与0x1fff与运算，大小必然小于4k，order则为0，然来绕去还是一个page大小。这么做估计是为了兼容vmcs_config中的size计算。</p>

<p>下面根据vmm_exclusive进行kvm_cpu_vmxon，进入vmx模式，初始化loaded_vmcs，然后用kvm_cpu_vmxoff退出vmx模式。</p>

<p>vmx_vcpu_load加载VCPU的信息，切换到指定cpu，进入到vmx模式，将loaded_vmcs的vmcs和当前cpu的vmcs绑定到一起。vmx_vcpu_setup则是初始化vmcs内容，主要是赋值计算，下面的vmx_vcpu_put则是vmx_vcpu_load的反运算。下面还有一些apic，nested，pml就不说了。</p>

<p>vmx_create_vcpu结束就直接回到kvm_vm_ioctl_create_vcpu函数，下面是kvm_arch_vcpu_setup，整个就一条线到kvm_arch_vcpu_load函数，主要有kvm_x86_ops->vcpu_load(vcpu, cpu)和tsc处理，vcpu_load就是vmx_vcpu_load，刚说了，就是进入vcpu模式下准备工作。</p>

<p>kvm_arch_vcpu_setup后面是create_vcpu_fd为proc创建控制fd，让qemu使用。kvm_arch_vcpu_postcreate则是马后炮般，重新vcpu_load，写msr，tsc。</p>

<p>如此整个vcpu就创建完成了。</p>

<h4>6.KVM_RUN</h4>

<p>KVM run涉及内容也不少，先写完内存虚拟化之后再开篇专门写RUN流程。</p>
]]></content>
  </entry>
  
</feed>
